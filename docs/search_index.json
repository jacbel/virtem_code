[
["index.html", "Structuring time: The hippocampus constructs sequence memories that generalize temporal relations across experiences Research Documentation for VIRTEM 1 Overview 1.1 Purpose 1.2 Contents 1.3 Contact 1.4 License Information", " Structuring time: The hippocampus constructs sequence memories that generalize temporal relations across experiences Research Documentation for VIRTEM Jacob L. S. Bellmund 03 February 2022 1 Overview 1.1 Purpose This website is intended to provide research documentation for our manuscript: Bellmund, J. L. S., Deuker, D., Montijn, N. D., &amp; Doeller, C. F. (2021). Structuring time: The hippocampus constructs sequence memories that generalize temporal relations across experiences. bioRxiv. https://doi.org/10.1101/2021.04.23.440002 The website documents the analysis of the virtual time project (virtem for short as it investigates the impact of virtual temporal distances on event representations in the hippocampus and entorhinal cortex). It is based on R markdown files that implement the individual analysis steps. The individual markdown files are merged using bookdown. On this website, beginning after the basic preprocessing of the MRI data, all preparatory analysis steps, statistical analyses and the generation of data figures are documented. Data to reproduce the statistical analyses will be made available upon publication. 1.2 Contents Overview: General information about the contents of this website. Analysis Setup: Here some variables and folder are defined Illustrations of Task Design and Analysis Logic: Illustrates features of the design &amp; analysis Prepare behavioral data: Builds a data frame from individual log files Behavioral Analysis: Statistical analysis of behavioral data Prepare (f)MRI Data: Quantifies representational change in regions of interest Run RSA Searchlight: Implements RSA searchlights on participant and group level RSA on Pattern Similarity Change: Statistical analysis of representational change Signal to noise ratio: tSNR in the regions of interest Credit: References and versions of used packages 1.3 Contact Dr. Jacob L. S. Bellmund is a postdoc in the Psychology Department of the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany. You can contact Jacob via e-mail, follow him on Twitter, find him on ResearchGate and LinkedIn, or visit his website to find out more about his work. 1.4 License Information Please cite our work! The manuscript can be found on bioRxiv soon, here is the reference: Bellmund, J. L. S., Deuker, D., Montijn, N. D., &amp; Doeller, C. F. (2021). Structuring time: The hippocampus constructs sequence memories that generalize temporal relations across experiences. bioRxiv. https://doi.org/10.1101/2021.04.23.440002 The analyses documented on this website build on third-party code and software (R packages, FSL etc.) as well as custom code. The custom code, written by Jacob Bellmund, is available under the MIT License, see license text below. Code/software from third-parties may have different licenses. Please respect and credit the work of others. If you are a licensor and think we did not credit your work appropriately, please reach out via e-mail. License for custom code MIT License Copyright (c) 2021 Jacob L. S. Bellmund Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. "],
["analysis-setup.html", "2 Analysis Setup 2.1 Defining Variables and Folders 2.2 Define Analysis Functions", " 2 Analysis Setup 2.1 Defining Variables and Folders First, we set up some global variables that will be used throughout the analyses. This includes the identifiers of the subjects to include in the analysis, specifics of the design such as the number of days and events per day and the regions of interest for representational similarity analysis. #-------- DEFINE GLOBAL VARIABLES --------- # list of subject IDs(excluded: 58 and 68 because of bad memory performance and MRI acquisition problems) subjects &lt;- c(&quot;031&quot;, &quot;032&quot;, &quot;033&quot;, &quot;034&quot;, &quot;035&quot;, &quot;036&quot;, &quot;037&quot;, &quot;038&quot;, &quot;039&quot;, &quot;054&quot;, &quot;055&quot;, &quot;056&quot;, &quot;057&quot;, &quot;059&quot;, &quot;061&quot;, &quot;062&quot;, &quot;063&quot;, &quot;064&quot;, &quot;065&quot;, &quot;066&quot;, &quot;069&quot;, &quot;070&quot;, &quot;071&quot;, &quot;072&quot;, &quot;073&quot;, &quot;074&quot;, &quot;075&quot;, &quot;076&quot;) n_subs &lt;- length(subjects) # PVT scanning runs and blocks within each run runs &lt;- c(&quot;pre&quot;, &quot;post&quot;) n_runs &lt;- length(runs) n_blocks &lt;- 10 # design parameters n_days &lt;- 4 n_events_day &lt;- 5 n_pics &lt;- n_days*n_events_day # Main regions of interest for analyses rois &lt;- c(&quot;aHPC_lr&quot;, &quot;alEC_lr&quot;) # Regions of interest in MNI space rois_ec = c(&quot;alEC_lr&quot;) rois_hpc = c(&quot;aHPC_lr&quot;) rois_mni &lt;- c(rois_hpc, rois_ec) roi_colors &lt;- c(&quot;#dd4731&quot;, &quot;#079cd6&quot;) # Regions of interest to get from Freesurfer rois_fs = c(&quot;hpc_lr&quot;, &quot;ec_lr&quot;) # numeric Freesurfer labels #(https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/AnatomicalROI/FreeSurferColorLUT) labels_fs = list(c(17,53), c(1006,2006)) roi_colors_fs &lt;- wes_palette(n = length(rois_fs), name = &quot;FantasticFox1&quot;) # Define colors to use in plots event_colors &lt;- scico::scico(n=5, begin = 0, end = 0.6, palette = &quot;bamako&quot;) time_colors &lt;- c(&quot;#26588E&quot;, &quot;#33602D&quot;, &quot;#C1AA6E&quot;) aHPC_colors &lt;- c(&quot;#dd4731&quot;, # main &quot;#26588E&quot;, # within main, from scico::scico(n=5, begin = 0.1, end = 0.7, palette = &quot;devon&quot;)[3], &quot;#e31a1c&quot;, # within low &quot;#800026&quot;, # within high &quot;#A54440&quot;, # across main, from scico::scico(n=5, begin = 0.3, end = 0.9, palette = &quot;lajolla&quot;)[3] &quot;#feb24c&quot;, # across low &quot;#fc4e2a&quot;) # across high names(aHPC_colors) &lt;- c(&quot;main&quot;, &quot;within_main&quot;, &quot;within_low&quot;, &quot;within_high&quot;, &quot;across_main&quot;, &quot;across_low&quot;, &quot;across_high&quot;) alEC_colors &lt;- c(&quot;#855C85&quot;, &quot;#225ea8&quot;, &quot;#1d91c0&quot;, &quot;#0c2c84&quot;, &quot;#7fcdbb&quot;, &quot;#c7e9b4&quot;, &quot;#41b6c4&quot;) names(alEC_colors) &lt;- c(&quot;main&quot;, &quot;within_main&quot;, &quot;within_low&quot;, &quot;within_high&quot;, &quot;across_main&quot;, &quot;across_low&quot;, &quot;across_high&quot;) day_time_int_color &lt;- &quot;#F5DF4D&quot; time_within_across_color &lt;- unname(alEC_colors[&quot;main&quot;]) ultimate_gray &lt;- &quot;#939597&quot; Folder Structure In a second step, we create some folders that will be used during the analysis. These are folders that contain raw data as well as folders into which processed data is written. CAVE: Some Markdown files still create their own folders. #-------- SET UP FOLDERS --------- dirs &lt;- c() # directory with logs from picture viewing task dirs$pvt_log_dir &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;logs&quot;, &quot;picture_viewing_task&quot;) # directory with logs from day learning task dirs$dlt_log_dir &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;logs&quot;, &quot;day_learning&quot;) # directory with logs from the timeline task dirs$timeline_dat_dir &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;timeline&quot;) # directories for freesurfer ROIs dirs$rois_fs_dirs = here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, rois_fs) # directories for final ROIs in analysis space dirs$rois_ss_dirs &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, rois, &quot;samespace&quot;) # directories for the MNI ROIs in analysis space dirs$rois_mni_ss_dirs &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, rois_mni, &quot;samespace&quot;) # directory where preprocessed data lies for each run and block dirs$feat_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;functionalDataPerBlock&quot;) # directory with MRI data in the analysis space (samespace) dirs$samespace_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;samespace&quot;) # base directory for RSA dirs$rsa_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rsa&quot;) # directories for RSA correlation matrices dirs$rsa_roi_corr_mat_dirs &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rsa&quot;, &quot;correlation_matrices&quot;, rois) # directories for RSA pattern similarity change dirs$rsa_roi_ps_change_dirs &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rsa&quot;, &quot;pattern_similarity_change&quot;, rois) # directories for the cleaned timeseries data dirs$rsa_roi_clean_timeseries_dirs &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rsa&quot;, &quot;clean_roi_timeseries&quot;, rois) # directories for the relevant volumes of each ROI dirs$rsa_roi_rel_vol_dirs &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rsa&quot;, &quot;relevant_roi_volumes&quot;, rois) # directory for data on which to run RSA dirs$rsa_dat_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rsa&quot;, &quot;data_for_rsa&quot;) dirs$mask_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;group_masks&quot;) # base directory for RSA searchlights dirs$searchlight &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rsa&quot;, &quot;searchlight&quot;) # data for final analysis (this will be shared) dirs$data4analysis &lt;- here(&quot;data_for_analysis&quot;) # directory for source data for nature communications dirs$source_dat_dir &lt;-here(&quot;source_data_natcomms&quot;) if(dir.exists(dirs$source_dat_dir)){unlink(dirs$source_dat_dir, recursive = TRUE)} # delete if already existing to make sure to start with empty directory # directory to save figures to fig_dir = here(&quot;figures&quot;) if(!dir.exists(fig_dir)){dir.create(fig_dir)} # create all the directories dirs_created &lt;- lapply(unlist(dirs), function(x) if(!dir.exists(x)) dir.create(x, recursive=TRUE)) 2.2 Define Analysis Functions Analysis Logic We run both the behavioral analysis and the RSA using two approaches: A summary statistics approach and with linear mixed effect models. In the analysis of the timeline task, we test whether virtual time explains remembered times when competing for variance with order and real time in seconds. In order to assess the change in pattern similarity that is due to the learning task, we will later calculate the difference of the Fisher transformed mean correlation coefficients for every pair of scenes between the pre and the post picture viewing tasks (see this section). We will analyze how these difference values relate to various predictor variables derived from the learning task, such as the temporal distance between pairs of scenes within a day. The two analysis approaches are briefly outlined below. Summary Statistics The summary statistics approach is based on permutation testing. # control number of permutations used throughout the analyses n_perm &lt;- 10000 We will use 10000 random permutations throughout the analyses. In the summary statistics approach, we use the different time metrics as predictors for the remembered times in the timeline task. We will thus run one GLM per participant. In RSA, we set up a GLM with the given variable from the learning task as a predictor and the pairwise RSA difference values as criterion for every participant. The resulting model coefficients are then compared to a null distribution obtained from shuffling the dependent variable of the linear model (i.e. pattern similarity change) for a large number of times. This results in a p-value for each coefficient, which is transformed to a Z-score. The Z-scores are then taken to the second level for group-level statistics. This is also described in the methods section of the manuscript: For the summary statistics approach, we ran a multiple regression analysis for each participant with virtual time, sequence position (order), and real time since the first event of a day as predictors of responses in the timeline task. To test whether virtual time indeed explained participants’ responses even when competing for variance with order and real time, included in the model as control predictors of no interest, we compared the participant-specific t-values of the resulting regression coefficients against null distributions obtained from shuffling the remembered times against the predictors 10000 times. We converted the resulting p-values to Z-values and tested these against zero using a permutation-based t-test (10000 random sign-flips, Figure 2E). In the summary statistics approach, we used the different time metrics as predictors for pattern similarity change. We set up a GLM with the given variable from the day learning task as a predictor and the pairwise representational change values as the criterion for every participant. The t-values of the resulting model coefficients were then compared to a null distribution obtained from shuffling the dependent variable of the linear model (i.e. pattern similarity change) 10000 times. This approach to permutation-testing of regression coefficients controls Type I errors even under situations of collinear regressors (Anderson and Legendre, 1999). Resulting p-values for each coefficient were transformed to a Z-score. The Z-scores were then used for group-level inferential statistics. We start by defining the function that permutes the dependent variable of the linear model. This approach was described e.g. by Manly (1997) and is referred to as permutation of raw data in Anderson &amp; Legendre (1999), who compare different ways to implement permutation tests for (partial) regression coefficients. Their simulations show that the chosen approach does well in terms of controlling type I errors and power, even under situations of collinear regressors. # define function that calculates z-value for permutation lm_perm_jb &lt;- function(in_dat = df, lm_formula = lm_formula, nsim = 1000){ # run the model for original data and store observed t-values lm_fit &lt;- lm(formula = lm_formula, data=in_dat) obs &lt;- coef(summary(lm_fit))[,&quot;t value&quot;] # extract the dependent variable from the formula dv &lt;- str_extract(lm_formula, &quot;[^~]+&quot;) dv &lt;- str_replace_all(dv, fixed(&quot; &quot;), &quot;&quot;) if(!(dv %in% colnames(in_dat))){stop(&quot;Cannot find dependent variable in input data&quot;);} # initialize df for permutation data_perm &lt;- in_dat # set aside space for results res &lt;- matrix(nrow = nsim, ncol = length(obs)) for (i in 1:nsim) { # scramble response value perm &lt;- sample(nrow(in_dat)) dv_dat &lt;- in_dat[dv] data_perm[dv] &lt;- dv_dat[perm,] # compute linear model and store the t-value of predictor lm_fit_perm &lt;- lm(formula = lm_formula, data=data_perm) res[i,] &lt;- coef(summary(lm_fit_perm))[,&quot;t value&quot;] } # append the observed value to the list of results res &lt;- rbind(res,obs) # calculate p-value for each coefficient and transform to z p &lt;- rep(0,length(obs)) z &lt;- rep(0,length(obs)) for (i_coef in 1:length(obs)){ p[i_coef] &lt;- sum(res[,i_coef] &gt;= obs[i_coef])/nrow(res) z[i_coef] &lt;- qnorm(1-p[i_coef]) } return(z) } The Z-values resulting from this first-level permutation are then analyzed on the group level. For t-tests, we use random sign-flips (c.f. the one-sample t-test in FSL Randomise) to non-parametrically test against 0 or assess within-participant differences between conditions. For this, we use the function defined below that is a reduced version of this Matlab implementation. Group-level statistics were carried out using permutation-based procedures. For t-tests, we compared the observed t-values against a surrogate distribution obtained from 10000 random sign-flips to non-parametrically test against 0 or to assess within-participant differences between conditions. Permutation-based repeated measures ANOVAs were carried out using the permuco package (Frossard and Renaud, 2019). # function for permutation-based one-sample t-test against 0 # requires tidyverse and broom # diff should be a vector of values (e.g. differences between paired samples) # returns the data frame from broom::tidy(t.test(diff)) with an additional column # for the p-value from n_perm random sign flips # this function is based on the mult_comp_perm_t1 function for Matlab by David Groppe) paired_t_perm_jb &lt;- function(in_dat, n_perm=10000){ # make sure input is a vector in_dat &lt;- unname(unlist(in_dat)) # number of observations n &lt;- length(in_dat) # get t of unpermuted data t_stats &lt;- t.test(in_dat) %&gt;% broom::tidy() # run n_perm iterations t_perm &lt;- vector(&quot;numeric&quot;, n_perm) for(i in 1:n_perm){ # randomly shuffle sign of each observation and get t-value dat_perm &lt;- in_dat*sample(c(-1,1),n,replace=TRUE) t_perm[i] &lt;- abs(t.test(dat_perm)$statistic) } #add the negative of all values since we assume null hypothesis distribution is symmetric t_perm &lt;-c(t_perm, -t_perm) # add observed t-value so p cannot be 0 t_perm &lt;-c(t_perm, t_stats$statistic) # calculate two-tailed p-value p_perm &lt;- mean(t_perm &gt;= abs(t_stats$statistic))*2 t_stats &lt;- t_stats %&gt;% tibble::add_column(p_perm = p_perm, .after = &quot;p.value&quot;) return(t_stats) } Linear Mixed Effects Linear Mixed Effects models consist of fixed and random effects. In our case the fixed case are of interest and consist of the temporal relationships that could explain remembered times or pattern similarity change (the dependent variables in the behavioral analysis and RSA, respectively). Random effects account for the fact that data points from different participants enter the estimation of the fixed effects. Following the recommendation for maximal random effect structures by Barr et al. (JML, 2013), we first attempt to fit a model with a random effects structure including random intercepts and random slopes for participants. If the model does not converge or results in singular fits, we reduce the random effects structure, attempting to always at least keep random slopes for the fixed effect of interest in the model as these are crucial to avoid anti-conservativity. Statistical inference about a model is done using a likelihood ratio tests against a nested, reduced model. The reduced model is identical to the full model, only the fixed effect of interest is removed. LME assumptions The code below defines a function to generate three diagnostic plots to visually assess the assumptions of a mixed model. Linearity &amp; Homoscedasticity: Residual plot Normality of residuals: QQ-Plot and histogram of the residuals It returns a ggplot object based on the input LME model. Probably these diagnostic plots will not end up in the manuscript because space is limited. lmm_diagplots_jb &lt;- function(lmm = lmm_full){ # residual plot to inspect homoscedasticity resids_gg &lt;- ggplot() + geom_point(aes(x = fitted(lmm), y = residuals(lmm)), size = 1, shape = 16, alpha = 0.5) + geom_smooth(aes(x = fitted(lmm), y = residuals(lmm)), formula = &quot;y ~ x&quot;, method=&#39;glm&#39;, se = TRUE, color = &quot;darkred&quot;) + ylab(&#39;residuals&#39;) + xlab(&#39;fitted values&#39;) + ggtitle(&quot;Residual Plot&quot;)+ theme_cowplot() # data frame for QQ plot and histogram df &lt;- data.frame(r = residuals(lmm)) # QQ-Plot look at residuals of the model qqplot_gg &lt;- ggplot(df, aes(sample = r)) + stat_qq(size = 1, shape = 16, alpha = 0.5) + stat_qq_line(color = &quot;darkred&quot;) + ggtitle(&quot;QQ-Plot&quot;)+ theme_cowplot() # Histogram of residuals hist_gg &lt;-ggplot(df, aes(x=r)) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;darkgrey&quot;, bins = 50) + geom_density(color=&quot;darkred&quot;) + xlab(&quot;magnitude of residual&quot;) + ggtitle(&quot;Histogram of Residuals&quot;)+ theme_cowplot() # collect for final figure diag_fig &lt;- resids_gg + qqplot_gg + hist_gg &amp; theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &#39;auto&#39;, aspect.ratio = 1, plot.title = element_text(hjust = 0.5)) &amp; plot_annotation(tag_levels = &#39;A&#39;, caption = paste0(deparse(formula(lmm)), collapse=&quot;&quot;)) return (diag_fig) } LME Summary Tables To summarize final models we create tables inspired by the best practice guidelines by Meteyard &amp; Davies (JML, 2020). Examples can be found on their OSF page, in particular the example reporting table on page 4 of this document. To create these tables we rely on the broom.mixed package to get the LME model summary in a tidy format. The tidy dataframes are then converted to huxtables, which can be nicely formatted. The function below takes as an input the tidy data frames for fixed effects, random effects as well as the ANOVA results from the comparison of the full model against a reduced model without the fixed effect of interest. All are merged into one table to limit the number of tables. make_lme_huxtable &lt;- function(fix_df, ran_df, aov_mdl, fe_terms=NULL, re_terms=NULL, re_groups=NULL, lme_form=NULL, caption=&quot;Summary of Linear Mixed Effects Model&quot;){ ########### FIXED EFFECTS # create huxtable for fixed effects and format it fix_hux &lt;- fix_df %&gt;% huxtable::huxtable(., add_colnames = TRUE) %&gt;% # set standard error and t-value column names set_contents(row = 1, col = 4, value = &quot;SE&quot;) %&gt;% set_contents(row = 1, col = 5, value = &quot;t-value&quot;) %&gt;% # merge the confidence column title and reset column name merge_cells(row = 1, col = 6:7) %&gt;% set_contents(row = 1, col = 6, value = &quot;95% CI&quot;) %&gt;% set_align(row = 1, col = 6, value = &quot;center&quot;) %&gt;% # align the contents set_align(row = 1, col = 2:5, &quot;center&quot;) %&gt;% set_align(row = 1, col = 1, &quot;center&quot;) %&gt;% set_align(row = 2:nrow(.), col = 3:7, &quot;center&quot;) %&gt;% # how many digits to print? set_number_format(row = 2:nrow(.), col = 3:7, 6) %&gt;% set_number_format(row = 2:nrow(.), col = 5, 2) %&gt;% # add header row for fixed effects dplyr::select(-effect) %&gt;% # remove column to the left because not needed huxtable::insert_row(.,c(&quot;fixed effects&quot;, rep(&quot;&quot;,ncol(.)-1)), after=0) %&gt;% set_align(row=1,col=1, value=&quot;center&quot;) %&gt;% set_header_cols(col=1, value=TRUE) %&gt;% # bottom border set_bottom_border(row=nrow(.), value=0.5) # if names for fixed effect terms are supplied use them if(!is.null(fe_terms)){fix_hux$term[3:nrow(fix_hux)] &lt;- fe_terms} ########### RANDOM EFFECTS # create huxtable for random effects and format it ran_hux &lt;- ran_df %&gt;% huxtable::huxtable(., add_colnames = TRUE) %&gt;% # how many digits to print? set_number_format(row = 2:nrow(.), col = 4, 6) %&gt;% # align header row set_align(row = 1, col = c(1,2), &quot;left&quot;) %&gt;% # add header row for random effects select(-effect) %&gt;% # remove column to the left because not needed huxtable::insert_row(.,c(&quot;random effects&quot;, rep(&quot;&quot;,ncol(.)-1)), after=0) %&gt;% set_align(row=1,col=1, value=&quot;center&quot;) %&gt;% set_header_cols(col=1, value=TRUE) %&gt;% # center estimate data set_align(row=2:nrow(.),col=ncol(.), value=&quot;center&quot;) %&gt;% # bottom border set_bottom_border(row=nrow(.), value=0.5) # if names for random effect grouping factor are supplied use them if(!is.null(re_groups)){ran_hux$group[3:nrow(ran_hux)] &lt;- re_groups} # if names for random effect terms are supplied use them if(!is.null(re_terms)){ran_hux$term[3:nrow(ran_hux)] &lt;- re_terms} ########### MODEL COMPARISON # determine how many digits of p-value to print if(aov_mdl$`Pr(&gt;Chisq)`[2]&gt;=0.001) {num_fmt&lt;-3} # 3 digits if p&gt;0.001 else{ num_fmt&lt;-NA # use default -&gt; scientific notation aov_mdl$`Pr(&gt;Chisq)`[2]&lt;-formatC(aov_mdl$`Pr(&gt;Chisq)`[2], format = &quot;e&quot;, digits = 2)} aov_hux &lt;- aov_mdl %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(var =&quot;term&quot;) %&gt;% huxtable(add_colnames = TRUE) %&gt;% # align contents set_align(row = 1:nrow(.), col = c(2:ncol(.)), &quot;center&quot;) %&gt;% set_align(row = 1, col = 1, &quot;left&quot;) %&gt;% # add header row for model comparison huxtable::insert_row(.,c(&quot;model comparison&quot;, rep(&quot;&quot;,ncol(.)-1)), after=0) %&gt;% #merge_cells(1:nrow(.), col=1) %&gt;% set_align(row=1,col=1, value=&quot;center&quot;) %&gt;% set_header_cols(col=1, value=TRUE) %&gt;% # add model names set_contents(row = 2:nrow(.), col = &quot;term&quot;, value = c(&quot;model&quot;, &quot;reduced model&quot;, &quot;full model&quot;)) %&gt;% set_align(row = 2, col = 2, &quot;left&quot;) %&gt;% # change some column names set_contents(row = 2, col=&quot;Pr(&gt;Chisq)&quot;, value = &quot;p&quot;) %&gt;% set_contents(row = 2, col=&quot;logLik&quot;, value = &quot;LL&quot;) %&gt;% set_contents(row = 2, col=&quot;Df&quot;, value = &quot;df&quot;) %&gt;% set_contents(row = 2, col=&quot;Chisq&quot;, value = &quot;X2&quot;) %&gt;% #set_contents(row = 2, col=&quot;Chisq&quot;, value = expression(&quot;$\\\\chi&quot;^&quot;2&quot;)) %&gt;% # how many digits to print? (do at the end because affected by adding columns) set_number_format(row = 3:4, col = 3:7, 2) %&gt;% set_number_format(row = 4, col = 9, num_fmt) %&gt;% # remove the deviance and BIC column remove_cols(deviance) %&gt;% remove_cols(BIC) %&gt;% # bottom border set_bottom_border(row=nrow(.), value=0.5) ####### MERGE THE THREE HUXTABLES # to be able to merge the tables, they need to have the same number of columns. # the AOV table has 7 columns, so we add 1 empty columns to the fixed effects # table and 4 empty columns to the random effects table. We merge these with # existing columns immediately ran_hux &lt;-ran_hux %&gt;% mutate(a=NA, b=NA, c=NA, d=NA, .after = &quot;term&quot;) %&gt;% merge_across(row = 1:nrow(.), col = 2:6) fix_hux &lt;-fix_hux %&gt;% mutate(a=NA, .after = &quot;term&quot;) %&gt;% merge_across(row = 1:nrow(.), col = 1:2) # merge lmm_hux &lt;- huxtable::add_rows(fix_hux, ran_hux) %&gt;% huxtable::add_rows(., aov_hux) %&gt;% # set the header and bottom border set_caption(caption) %&gt;% set_caption_pos(&quot;topleft&quot;) %&gt;% set_bottom_border(row=nrow(.), value=0.5) %&gt;% add_footnote(sprintf(&quot;model: %s; \\nSE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation&quot;, lme_form), border = 0.5) return(lmm_hux) } The resulting huxtables nicely summarize the mixed models in the HTML documentation. To collect the tables in a word file that accompanies the manuscript, we convert them to flextables. These can be written to Word documents using the officer package. The function below does the conversion plus some touching up to end up with nicely formatted tables in Word. convert_huxtable_to_flextable &lt;-function(ht = lmm_hux){ # define border style to apply to selected cells def_cell_l=officer::fp_cell(border.right = fp_border(), border.top = fp_border()) def_cell_t=officer::fp_cell(border.bottom = fp_border(), border.top = fp_border()) # define text style to apply to selected cells def_par=officer::fp_par(text.align = &quot;center&quot;, padding=3) def_tex=officer::fp_text(bold=TRUE) # find the header rows (where new sections of the table begin) head_rows &lt;- match(c(&quot;term&quot;, &quot;group&quot;, &quot;model&quot;), ht$term) # find the cell where we want to add Chi2 x2_cell &lt;- which(ht==&quot;X2&quot;, arr.ind = TRUE) # create the flextable ft &lt;- ht %&gt;% huxtable::as_flextable() %&gt;% # add border at the bottom to the rows that have the names flextable::style(i=head_rows, pr_c = def_cell_t, pr_p = def_par, pr_t = def_tex) %&gt;% flextable::style(i=head_rows-1, pr_c = def_cell_t, pr_p = def_par, pr_t = def_tex) %&gt;% flextable::bg(i=head_rows-1, bg=&quot;lightgrey&quot;, part=&quot;all&quot;) %&gt;% # left-align the first and second column (apart from no. of params in model comparison) and add consistent padding flextable::align(., j=c(1,2), align=&quot;left&quot;) %&gt;% flextable::align(., i = c(head_rows[3], head_rows[3]+1, head_rows[3]+2), j=2, align=&quot;center&quot;) %&gt;% flextable::padding(., padding = 3, part=&quot;all&quot;) %&gt;% # set font style flextable::fontsize(size=10, part=&quot;all&quot;) %&gt;% flextable::font(fontname = font2use) %&gt;% # replace X2 with greek letter chi and superscript 2 flextable::compose(i=x2_cell[1], j=x2_cell[2], value = as_paragraph(&quot;\\U03C7&quot;, as_sup(&quot;2&quot;))) %&gt;% # autofit to page flextable::set_table_properties(layout=&quot;autofit&quot;, width=1) # set caption style ft &lt;- flextable::set_caption(ft,caption = ft$caption$value, style=&quot;Normal&quot;) return(ft) } Here we open the word file that we want to write the tables to. It is based on a .docx-file where the themes for headings and text was manually modified to match the style of the manuscript. stables_docx &lt;- officer::read_docx(here(&quot;virtem_code&quot;, &quot;modified_headings.docx&quot;)) stables_docx &lt;- stables_docx %&gt;% officer::body_add_par(&quot;Supplemental Tables&quot;, style = &quot;heading 1&quot;, pos = &quot;on&quot;) Brain Plots in ggplot These functions are based on the ggBrain package. Plus, there are two simple custom functions to transform between 1mm MNI and matrix coordinates. ggBrain functions to get template background The first function returns a ggplot object of the template brain (MNI 1mm in our case). # function from ggBrain that returns a ggplot object of the template brain getggTemplate&lt;-function(col_template,row_ind,col_ind, ...){ templateFrame&lt;-getBrainFrame(row_ind=row_ind, col_ind=col_ind, ...) n&lt;-length(col_template) if(n&gt;1) col_cut&lt;-as.numeric(cut(templateFrame$value,n)) if(n==1) col_cut=1 p&lt;-ggplot()+facet_just_unique(row_ind,col_ind) for(i in 1:n){ if(all(col_cut!=i)) next drop_ind&lt;-which(names(templateFrame)==&#39;value&#39;) #so it doesn&#39;t conflict with mappings to &quot;value&quot; later on templateFrame_col&lt;-templateFrame[col_cut==i,-drop_ind] p&lt;- p + geom_tile(data=templateFrame_col,aes(x=row,y=col),fill=col_template[i]) } p } # another internal function from ggBrain used to plot the template facet_just_unique&lt;-function(row_ind,col_ind){ if( all(row_ind==row_ind[1]) &amp; all(col_ind==col_ind[1])) out&lt;-NULL if( all(row_ind==row_ind[1]) &amp; !all(col_ind==col_ind[1])) out&lt;-facet_grid(.~col_ind) if(!all(row_ind==row_ind[1]) &amp; all(col_ind==col_ind[1])) out&lt;-facet_grid(row_ind~.) if(!all(row_ind==row_ind[1]) &amp; !all(col_ind==col_ind[1])) out&lt;-facet_grid(row_ind~col_ind) return(out) } Coordinate transforms The code section below defines two helper functions that transform between the MNI coordinate system and the matrix coordinates of R. CAVE: Only use with 1mm MNI space. # to get accurate labels of panels in MNI space, define functions to convert coords mni2vox &lt;- function(mni_coords, output_zero_based = FALSE){ x&lt;- mni_coords[1] * -1 + 90 y&lt;- mni_coords[2] * 1 + 126 z&lt;- mni_coords[3] * 1 + 72 vox_coords &lt;- c(x,y,z) # add +1 if output coordinates should not be zero-based if(!output_zero_based){vox_coords &lt;- vox_coords+1} return(vox_coords) } # convert from voxel coordinates to MNI with option for whether voxel coordinates are from # R (i.e. not zero-based) or FSL (i.e. zero-based) vox2mni &lt;- function(vox_coords, input_zero_based = FALSE){ # if input coordinates are from R, i.e. not zero-based, subtract -1 if(!input_zero_based){vox_coords &lt;- vox_coords-1} # subtract voxel space coordinates of origin (MNI=0,0,0 at 90,126,72 in FSL voxel coordinates) mni_x &lt;- -1*(vox_coords[1]-90) mni_y &lt;- vox_coords[2]-126 mni_z &lt;- vox_coords[3]-72 mni_coords &lt;- c(mni_x,mni_y,mni_z) return(mni_coords) } Waiting for Cluster Jobs The function below is a convenient function to pause the execution of a script until a batch of condor jobs has finished. The function takes the ID of a condor batch as an input. It uses condor_q via the system-command. If 0 is returned, it terminates and moves on. Else the function sleeps for (a default of) 60s and then tries again. No checks are performed whether the jobs finished without errors. This function merely pauses the execution of the code to wait for long jobs to finish. pause_until_batch_done&lt;- function(batch_id, wait_interval = 60){ print(sprintf(&quot;Monitoring batch job %s. Checking status every %d seconds.&quot;, batch_id, wait_interval)) batch_running &lt;- TRUE tictoc::tic() while (batch_running){ # sleep for the specified number of seconds Sys.sleep(wait_interval) # check if batch finished (condor_q returns character(0)) status &lt;- system(sprintf(&quot;condor_q -l %s&quot;, batch_id), intern=TRUE) if(identical(status, character(0))){ print(&quot;Batch jobs finished. Moving on.&quot;) tictoc::toc() batch_running &lt;- FALSE } else{ print(sprintf(&quot;Batch jobs still running. Waiting another %d seconds.&quot;, wait_interval)) } } } "],
["illustrations-of-task-design-and-analysis-logic.html", "3 Illustrations of Task Design and Analysis Logic 3.1 Day Learning Task Design 3.2 Representational Similarity Analysis Logic", " 3 Illustrations of Task Design and Analysis Logic 3.1 Day Learning Task Design Here is a description of the design of the Day Learning Task from the methods section: In this task, 20 of the 21 scenes, which were shown in the picture viewing tasks, were presented repeatedly. This time, however, they were grouped into multiple sequences introduced to participants as “virtual days”. There were four different sequences, each comprising 5 events. Events from the same sequence were always shown in a specific order and with a specific time delay between them. Scenes were on screen for 1.5 s. At the end of each sequence, an image of a moon was shown for 5 s, then the next sequence began. Every sequence was presented 7 times. There were 7 mini-blocks in this task. Within each of these, every sequence was presented once. At the end of a mini-block, a 30-s break followed, then the next block started. The order in which the sequences were presented differed randomly across the 7 mini-blocks. We instructed participants that the scenes depicted events from the life of a family and that the sequences of event images corresponded to different days in the family’s life. Participants were asked to memorize which events made up the different sequences (Figure 1C). We further instructed them to learn when during the respective sequence each event occurred. Specifically, we asked participants to learn event times relative to a virtual clock. This clock was running hidden from participants and event images were shown whenever the hidden clock reached the specific event time (Figure 1C, Supplemental Figure 2AB). The task was devised such that participants had to rely on their experience of passing real time and mnemonic construction to infer the times of events. Specifically, to give participants an indication of virtual time, the hidden clock was made visible 6 times for every presentation of a sequence: once before the first event, once in between successive events, and once after the last event. Participants received no cues about elapsing real time, but had to use their experience of passing real time between virtual time cues to infer the event times relative to the hidden virtual clock. Importantly, the exposure of the hidden clock occurred at random times for each sequence presentation (Supplemental Figure 2CD), with the constraint that it could not be revealed closer than 2 s to a preceding or subsequent event. Thus, participants saw different time cues in each repetition of a sequence. For example, while a specific event always happened at the same virtual time, e.g. 2:07 p.m., the virtual clock could be exposed at any time before the event, e.g. corresponding to 1:32 p.m. in the first repetition of the sequence, and corresponding to 1:17 p.m. in the second repetition. Because true event times were never revealed, participants could not exclusively rely on associative learning to solve the task. Time cues were visible for 1.5 s, but displayed only the time at the start of exposure, i.e. the displayed time did not change within the duration of its presentation. In short, participants had to combine their experience-based estimates of passing time with the time cues provided by the exposures of the otherwise hidden clock to infer the time at which each event in each sequence took place. Crucially, we varied the speed of the hidden clock between sequences in an effort to partly dissociate real time (in seconds) from virtual time (in virtual hours). Thus, for two sequences more virtual time passed in a comparable amount of real elapsing time (Figure 1C, Supplemental Figure 2). Correlations between the linearly increasing time metrics are inevitably high (Pearson correlation of virtual time with order r=0.969 and virtual time with real time r=0.975). Still this manipulation allowed us to determine using multiple regression whether virtual time explained constructed event times when competing for variance with real elapsed time and event order and whether hippocampal pattern similarity changes related to temporal distances in virtual time beyond ordinal distances and real time distances. Regression models including collinear predictor variables do not result in biased parameter estimates96,97. Let’s illustrate some features of the design. First we need to load the data from the design files. These contain the virtual time (in virtual hours) for each event of each day and the scanner time (in seconds) since the time of the day start. day_id &lt;- rep(1:n_days, each = n_events_day) design_tbl &lt;- tibble(day_id) # load design file with virtual time fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;design&quot;, &quot;daysTime.txt&quot;) csv_txt &lt;- read.csv(fname, header = FALSE) design_tbl$virtual_time &lt;- c(t(as.matrix(csv_txt))) # load design file with event times in seconds (after day start), i.e. real time # this is the scanner time fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;design&quot;, &quot;daysTimeS.txt&quot;) csv_txt &lt;- read.csv(fname, header = FALSE) design_tbl$scanner_time &lt;- c(t(as.matrix(csv_txt))) # scanner time relative to first image design_tbl$rel_scanner_time &lt;- c( design_tbl$scanner_time[1:5] - design_tbl$scanner_time[1], design_tbl$scanner_time[6:10] - design_tbl$scanner_time[6], design_tbl$scanner_time[11:15] - design_tbl$scanner_time[11], design_tbl$scanner_time[16:20] - design_tbl$scanner_time[16]) Here are some plots to show some features of the design. Events in virtual and real time This plot shows an overview of the design. We have 4 virtual days with 5 events each. The times of these events are shown based on different metrics: The scanner time since the presentation of the first day of an event (global x-axis) The virtual hours of each days (separate x-axis for each row) In combination, you can see that across days different amounts of virtual time pass for a given amount of scanner time. # time labels in virtual hours rather than decimals design_tbl$virtual_hours &lt;- sprintf(&quot;%02.0f:%02.0f&quot;, floor(design_tbl$virtual_time), 60*(design_tbl$virtual_time-floor(design_tbl$virtual_time))) # create a new dataframe to use for plotting virtual time axes virt_h_df &lt;- tibble(x = c(rep(-5,4), rep(65,4))) virt_h_df$y &lt;- rep(1:4,2) # create a new dataframe to use for plotting virtual time axes tick marks tick_df &lt;- tibble(x = rep(design_tbl$rel_scanner_time, each=2)) tick_df$y &lt;- c(0.75, 0.8) + rep(design_tbl$day_id, each=2) -1 tick_df$grp &lt;- rep(1:20, each=2) tick_df$day &lt;- rep(1:4, each=10) # initialize the plot ggplot(design_tbl, aes(x = rel_scanner_time, y = day_id, group = day_id)) + # plot line for each day geom_line(size = 1, colour = &quot;darkgrey&quot;) + # add points for the data geom_point(aes(fill = virtual_time), colour = &quot;darkgrey&quot;, size = 2, stroke = 0.5, shape = 21) + # plot line for each day that will become virtual time axis geom_line(data = tick_df, aes(x = x, y = day-0.2, group = day), size = 0.5, colour = &quot;black&quot;) + # extend the axes beyond the events geom_line(data = virt_h_df, aes(x = x, y = y-0.2, group = y), size = 0.5, colour = &quot;black&quot;, linetype = 3) + # add tick marks geom_line(data = tick_df, aes(x = x, y = y, group = grp), size = 0.5, colour = &quot;black&quot;) + # add labels with virtual hours geom_label(aes(x = rel_scanner_time, y = day_id-0.45), size=6*0.352777778, label = design_tbl$virtual_hours, fill = &quot;#D1D3D4&quot;, label.size = 0, label.padding=unit(0.5, &quot;mm&quot;), family=font2use) + # set the x- and y-axis range and ticks scale_x_continuous(limits = c(-7,68.5), breaks=seq(0,65,10)) + scale_y_continuous(limits = c(0.3, 4.1), breaks=seq(1,4,1)) + scico::scale_fill_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + # set plot title, legend and axis labels labs(x = &quot;seconds since first event&quot;, y = &quot;day&quot;) + # make it pretty theme_cowplot() + theme(legend.key = element_rect(fill = &quot;transparent&quot;, colour = &quot;transparent&quot;), text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), axis.ticks = element_line(colour = &quot;black&quot;, size = 0.5), axis.line = element_line(colour = &#39;black&#39;, size = 0.5), legend.position = &quot;none&quot;) Events in virtual time Two of the virtual days were short, ranging from 8am to 8pm, and two of the days were long, lasting from 6am until midnight. This can easily be seen if we plot the events in virtual time. # initialize the plot sfiga &lt;- ggplot(design_tbl, aes(x = virtual_time, y = day_id, group = day_id, fill = virtual_time)) + # plot line for each day geom_line(size = 1, colour = ultimate_gray) + # add points for the data geom_point(aes(fill = virtual_time), colour = ultimate_gray, size = 2, stroke = 0.5, shape = 21) + # change color scico::scale_fill_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + # set the x-axis range and ticks scale_x_continuous(limits = c(6,24), breaks=seq(6,24,3)) + scale_y_continuous(limits = c(0.5, 4.2)) + # set plot title, legend and axis labels labs(x = &quot;virtual time&quot;, y = &quot;day&quot;) + # make it pretty theme_cowplot() + theme(legend.key = element_rect(fill = &quot;transparent&quot;, colour = &quot;transparent&quot;)) sfiga Events in real time In an effort to dissociate real time (in seconds) from virtual time (in virtual hours), both the long and short days were presented within the same actual time span, i.e. around 60 seconds from the first to the last event. In effect, the hidden clock was running faster for the long days than for the short days. If we plot the scanner time relative to the presentation of the first event picture of a given day it becomes obvious that for two pairs of days the respective scanner time elapsing between events is almost identical. # initialize the plot sfigb &lt;- ggplot(design_tbl, aes(x = rel_scanner_time, y = day_id, group = day_id, fill = virtual_time)) + # plot line for each day geom_line(size = 1, colour = ultimate_gray) + # add points for the data geom_point(aes(fill = virtual_time), colour = ultimate_gray, size = 2, stroke = 0.5, shape = 21) + # change color scico::scale_fill_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + # set the x-axis range and ticks scale_x_continuous(limits = c(-10,70), breaks=seq(-10,70,10)) + scale_y_continuous(limits = c(0.5, 4.2)) + # set plot title, legend and axis labels labs(x = &quot;seconds since first event&quot;, y = &quot;day&quot;) + # make it pretty theme_cowplot() + theme(legend.key = element_rect(fill = &quot;transparent&quot;, colour = &quot;transparent&quot;)) sfigb Correlations of virtual time with order and real time Correlation of virtual time and order: Pearson r=0.969 Correlation of virtual time and real time: Pearson r=0.975 Correlation of order and real time: Pearson r=0.991 Time cues in virtual and real time Let’s further illustrate the design for one randomly chosen example subject by plotting the time cues. The virtual times of events were identical for all participants, but the times cues differed between participants. Prepare the data by loading the logfile from the day learning task and creating a long design table. set.seed(7) i_sub &lt;- sample(subjects,1) # create a long version of the design table by repeating each row 7 times, # i.e. once for each repetition design_tbl_long &lt;- design_tbl %&gt;% dplyr::slice(rep(1:n(), each = 7)) %&gt;% mutate(rep = rep(c(1:7), n_days*n_events_day), day_id = factor(sprintf(&quot;day %d&quot;, day_id), levels = c(&quot;day 4&quot;, &quot;day 3&quot;, &quot;day 2&quot;, &quot;day 1&quot;))) # load the logfile and store as tibble fn &lt;- file.path(dirs$dlt_log_dir, sprintf(&quot;P%s-virtemdata.txt&quot;, i_sub)) dlt_log &lt;- read.table(fn) colnames(dlt_log) &lt;- c(&#39;fc_on&#39;, &#39;fc_off&#39;, &#39;cue_start&#39;, &#39;cue_id&#39;, &#39;cue_end&#39;, &#39;vol&#39;, &#39;rep&#39;, &#39;real_time&#39;, &#39;day&#39;, &#39;trial_endtime&#39;) dlt_log &lt;- as_tibble(dlt_log) # save the time cue log entries for each day and repetition timecue_log &lt;- dlt_log %&gt;% group_by(day, rep) %&gt;% # log entries for each of day repetition mutate(rel_scanner_time = real_time -real_time[2]) %&gt;% # add real time relative to first event dplyr::slice(seq(1,11,2)) %&gt;% # keep only the odd entries, which are the time cues ungroup() %&gt;% mutate( virtual_time_dec = floor(cue_id)+((cue_id-floor(cue_id))/.6), # virtual time in decimals as in design table day_id = factor(sprintf(&quot;day %d&quot;, day), levels = c(&quot;day 4&quot;, &quot;day 3&quot;, &quot;day 2&quot;, &quot;day 1&quot;)) ) Then let’s plot the events and time cues relative to virtual time, separately for the different repetitions of each day. # virtual time sfigc &lt;- ggplot(timecue_log, aes(x=virtual_time_dec, y=rep, group = rep)) + geom_line(size = 1, colour = ultimate_gray) + geom_point(shape=18, color=&quot;black&quot;, size=2) + geom_point(data=design_tbl_long, aes(x=virtual_time, y=rep, fill = virtual_time), size = 2, color = ultimate_gray, stroke = 0.5, shape = 21) + scico::scale_fill_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + facet_wrap(~day_id, ncol=1) + scale_x_continuous(limits = c(6,24), breaks=seq(6,24,3)) + scale_y_continuous(limits = c(0,8), breaks=seq(1,7,1)) + labs(x = &quot;virtual time&quot;, y = &quot;repetition&quot;) + theme_cowplot() + theme(strip.background = element_blank(), #strip.text = element_blank(), legend.position = &quot;none&quot;) sfigc Lastly, let’s plot the events and time cues in real time relative to the first event of a given day, separately for the different repetitions of each day. # real time relative to first event of the day sfigd &lt;- ggplot(timecue_log, aes(x=rel_scanner_time, y=rep, group = rep)) + geom_line(size = 1, color = &quot;darkgrey&quot;) + geom_point(shape=18, color=&quot;black&quot;, size=2) + geom_point(data=design_tbl_long, aes(x=rel_scanner_time, y=rep, fill = virtual_time), size = 2, color = &quot;darkgrey&quot;, stroke = 0.5, shape = 21) + scico::scale_fill_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + facet_wrap(~day_id, ncol=1) + scale_y_continuous(limits = c(0,8), breaks=seq(1,7,1)) + scale_x_continuous(limits = c(-10,70), breaks=seq(-10,70,10)) + labs(x = &quot;seconds since first event&quot;, y = &quot;repetition&quot;) + theme_cowplot() + theme(strip.background = element_blank(), #strip.text = element_blank(), legend.position = &quot;none&quot;) sfigd Supplemental figure for task design Now we are ready to compose the figure, which will go to the supplement. layout = &quot; AB CD CD CD CD CD&quot; sfig &lt;- sfiga + sfigb + sfigc + sfigd + plot_layout(design = layout, guides = &quot;collect&quot;) &amp; theme(text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), legend.title.align = 0, legend.position = &quot;right&quot;, legend.justification = &quot;bottom&quot;, legend.key.size = unit(3,&quot;mm&quot;)) &amp; guides(fill = guide_colorbar(title.position = &quot;top&quot;, title.hjust = 0.5)) &amp; labs(fill = &quot;virtual\\ntime&quot;) &amp; plot_annotation(theme = theme(plot.margin = margin(t=0, r=-5, b=-8, l=-5, unit=&quot;pt&quot;)), tag_levels = &#39;A&#39;) # save as png and pdf and print to screen fn &lt;- here(&quot;figures&quot;, &quot;sf02&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=sfig, units = &quot;cm&quot;, width = 17.4, height = 17, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=sfig, units = &quot;cm&quot;, width = 17.4, height = 17, dpi = &quot;retina&quot;, device = &quot;png&quot;) Supplemental Figure 2. Design of the day learning task. A. Each of the four virtual days consisted of a sequence of five events. Event sequences are shown in virtual time, i.e. relative to the hidden clock. Less virtual time passes within the bottom two sequences because clock speed was manipulated between sequences. B. Event sequences shown in real time relative to the first event. A comparable amount of real time (in seconds) elapses during each event sequence despite different amounts of virtual time passing. C, D. Sequences in virtual and real time as shown in (A) and (B), respectively, but separately for each of the seven repetitions of each sequence during the learning task. Black diamonds indicate the time cues shown to one randomly chosen example participant during the task. Time cues varied across repetitions and differed across participants. 3.2 Representational Similarity Analysis Logic To illustrate the logic of the RSA on representational change, let’s create some matrices that will be used in the analysis logic figure of the manuscript. # some general params day &lt;- rep(1:4, each=5) event &lt;- rep(1:5, 4) We start with a random matrix of similarity values in the pre-learning scan. There should be no pattern. # CREATE AND PLOT NOISE MATRIX FOR PRE-SIMILARITY pre_sim_mat &lt;- matrix(runif(400, -0.8, 0.8), n_days*n_events_day, n_days*n_events_day) # make data frame from matrix and add variables for plotting pre_sim_mat[lower.tri(pre_sim_mat, diag=TRUE)]&lt;-NA pre_sim_df &lt;- reshape2::melt(pre_sim_mat) %&gt;% filter(!is.na(value)) %&gt;% mutate( day1 = day[Var1], day2 = day[Var2], day_comb = sprintf(&quot;%d_%d&quot;, day1, day2), same_day = day1==day2) # plot fig_pre_sim &lt;- ggplot(data = pre_sim_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile()+ scale_fill_gradient(low = &quot;white&quot;, high=&quot;black&quot;, na.value = NA, breaks = range(pre_sim_df$value), labels=c(&quot;low&quot;, &quot;high&quot;)) + coord_flip(xlim = c(0.5,19.5), ylim = c(20.5,0.5), expand = FALSE) + guides(fill=guide_colorbar(title = &quot;similarity&quot;, title.position = &quot;top&quot;, title.hjust = 0.5)) + theme(aspect.ratio = 1, axis.title=element_blank(), axis.text=element_blank(), axis.ticks=element_blank(), panel.background = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), legend.position = &quot;none&quot;, plot.margin=unit(c(0,0,0,0), units = &quot;cm&quot;)) # save and print fn &lt;- here(&quot;figures&quot;, &quot;f3_pre_sim&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=fig_pre_sim, units = &quot;cm&quot;, width = 2, height = 2, dpi = &quot;retina&quot;, device = cairo_pdf) fig_pre_sim To plot the RSA prediction matrix, we generate a matrix that has the pairwise temporal distances between events in its cell. We also generate a similarity matrix that mirrors our effects in the hippocampus, e.g. where similarity increases with distance within a sequence and decreases with distance between sequences. # CREATE DISTANCE AND IDEALIZED SIMILARITY CHANGE MATRICES # load design file with virtual time fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;design&quot;, &quot;daysTime.txt&quot;) virtual_time &lt;- read_csv(fname, col_names = FALSE, col_types = &quot;ddddd&quot;) virtual_time &lt;- c(t(as.matrix(virtual_time))) dist_mat &lt;- matrix(NA, n_days*n_events_day, n_days*n_events_day) change_sim_mat &lt;- matrix(NA, n_days*n_events_day, n_days*n_events_day) # loop over all pairs of events for (i1 in 1:20){ for (i2 in 1:20){ # temporal distance for this pair dist_mat[i1,i2] &lt;- abs(virtual_time[i2]-virtual_time[i1]) # similarity depends on whether we are in the same day or not if(day[i1]==day[i2]){ change_sim_mat[i1,i2] &lt;- abs(virtual_time[i2]-virtual_time[i1])} else { change_sim_mat[i1,i2] &lt;- range(virtual_time)[2]-range(virtual_time)[1]-abs(virtual_time[i2]-virtual_time[i1])} } } # PLOT DISTANCE MATRIX # remove lower triangle from distance matrix dist_mat[lower.tri(dist_mat, diag=TRUE)]&lt;-NA # make data frame from matrix and add variables for plotting dist_df &lt;- reshape2::melt(dist_mat) %&gt;% filter(!is.na(value)) %&gt;% mutate( day1 = day[Var1], day2 = day[Var2], day_comb = sprintf(&quot;%d_%d&quot;, day1, day2), same_day = day1==day2) # plot distance matrix fig_dist_mat &lt;- ggplot(data = dist_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile()+ scale_fill_gradient(low = &quot;white&quot;, high=&quot;black&quot;, na.value = NA, breaks = c(0,4), labels=c(&quot;low&quot;, &quot;high&quot;)) + coord_flip(xlim = c(0.5,19.5), ylim = c(20.5,0.5), expand = FALSE) + guides(fill=guide_colorbar(title = &quot;temporal distance&quot;, title.position = &quot;top&quot;, title.hjust = 0.5)) + theme(aspect.ratio = 1, axis.title=element_blank(), axis.text=element_blank(), axis.ticks=element_blank(), panel.background = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), legend.position = &quot;none&quot;, plot.margin=unit(c(0,0,0,0), units = &quot;cm&quot;)) + ggforce::geom_mark_rect(aes(group = day_comb, filter = same_day==FALSE), expand = 0.018, radius = 0, color=unname(aHPC_colors[&quot;across_main&quot;]), fill=NA, size=0.5)+ ggforce::geom_mark_rect(aes(group = day_comb, filter = same_day==TRUE), expand = 0.018, radius = 0, color=unname(aHPC_colors[&quot;within_main&quot;]), fill=NA, size=0.5) # save and print fn &lt;- here(&quot;figures&quot;, &quot;f3_dist_mat&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=fig_dist_mat, units = &quot;cm&quot;, width = 2, height = 2, dpi = &quot;retina&quot;, device = cairo_pdf) fig_dist_mat # PLOT SIMILARITY CHANGE MATRIX # add noise to change similarity matrix (uniform distribution) change_sim_mat &lt;- change_sim_mat+ matrix(runif(400, -0.4, 0.4), dim(change_sim_mat)[1]) # remove lower triangle from similarity change matrix change_sim_mat[lower.tri(change_sim_mat, diag=TRUE)]&lt;-NA # make data frame from matrix and add variables for plotting change_sim_df &lt;- reshape2::melt(change_sim_mat) %&gt;% filter(!is.na(value)) %&gt;% mutate( day1 = day[Var1], day2 = day[Var2], day_comb = sprintf(&quot;%d_%d&quot;, day1, day2), same_day = day1==day2) # plot fig_sim_change &lt;- ggplot(data = change_sim_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile()+ scale_fill_gradient(low = &quot;white&quot;, high=&quot;black&quot;, na.value = NA, breaks = range(change_sim_df$value), labels=c(&quot;low&quot;, &quot;high&quot;)) + coord_flip(xlim = c(0.5,19.5), ylim = c(20.5,0.5), expand = FALSE) + guides(fill=guide_colorbar(title = &quot;similarity&quot;, title.position = &quot;top&quot;, title.hjust = 0.5)) + theme(aspect.ratio = 1, axis.title=element_blank(), axis.text=element_blank(), axis.ticks=element_blank(), panel.background = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), legend.position = &quot;none&quot;, plot.margin=unit(c(0,0,0,0), units = &quot;cm&quot;)) + ggforce::geom_mark_rect(aes(group = day_comb, filter = same_day==FALSE), expand = 0.018, radius = 0, color=unname(aHPC_colors[&quot;across_main&quot;]), fill=NA, size=0.5)+ ggforce::geom_mark_rect(aes(group = day_comb, filter = same_day==TRUE), expand = 0.018, radius = 0, color=unname(aHPC_colors[&quot;within_main&quot;]), fill=NA, size=0.5) # save and print fn &lt;- here(&quot;figures&quot;, &quot;f3_sim_change&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=fig_sim_change, units = &quot;cm&quot;, width = 2, height = 2, dpi = &quot;retina&quot;, device = cairo_pdf) fig_sim_change Lastly, we create a post-similarity matrix, which is the sum of the simulated pre-learning and the similarity change matrix. # CREATE AND PLOT POST SIMILARITY MATRIX # post similarity is sum of pre similarity and change similarity post_sim_mat &lt;- pre_sim_mat + change_sim_mat # remove lower triangle from similarity change matrix post_sim_mat[lower.tri(post_sim_mat, diag=TRUE)]&lt;-NA # make data frame from matrix and add variables for plotting post_sim_df &lt;- reshape2::melt(post_sim_mat) %&gt;% filter(!is.na(value)) %&gt;% mutate( day1 = day[Var1], day2 = day[Var2], day_comb = sprintf(&quot;%d_%d&quot;, day1, day2), same_day = day1==day2) # plot fig_post_sim &lt;- ggplot(data = post_sim_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile()+ scale_fill_gradient(low = &quot;white&quot;, high=&quot;black&quot;, na.value = NA, breaks = range(post_sim_df$value), labels=c(&quot;low&quot;, &quot;high&quot;)) + coord_flip(xlim = c(0.5,19.5), ylim = c(20.5,0.5), expand = FALSE) + guides(fill=guide_colorbar(title = &quot;similarity&quot;, title.position = &quot;top&quot;, title.hjust = 0.5)) + theme(aspect.ratio = 1, axis.title=element_blank(), axis.text=element_blank(), axis.ticks=element_blank(), panel.background = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), legend.position = &quot;none&quot;, plot.margin=unit(c(0,0,0,0), units = &quot;cm&quot;)) # save fn &lt;- here(&quot;figures&quot;, &quot;f3_post_sim&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=fig_post_sim, units = &quot;cm&quot;, width = 2, height = 2, dpi = &quot;retina&quot;, device = cairo_pdf) fig_post_sim "],
["prepare-behavioral-data.html", "4 Prepare behavioral data 4.1 Sorting task 4.2 Timeline task 4.3 Picture viewing tasks", " 4 Prepare behavioral data As a first step, we create a design table for each participant that collects information about the assignment of events to virtual days and when the events occur with respect to virtual time and real time (seconds). # define the function virtem_behavior_prepare_design_tbl &lt;- function(subjects = c(&quot;031&quot;, &quot;032&quot;)){ # load design file with virtual time fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;design&quot;, &quot;daysTime.txt&quot;) virtual_time &lt;- read.csv(fname, header = FALSE) virtual_time &lt;- c(t(as.matrix(virtual_time))) # load design file with times of events in actual time in seconds (after day start) --&gt; real time fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;design&quot;, &quot;daysTimeS.txt&quot;) real_time &lt;- read.csv(fname, header = FALSE) real_time &lt;- c(t(as.matrix(real_time))) for (i_sub in 1:length(subjects)){ sub_id &lt;- subjects[i_sub] # initialize the tibble design_tbl &lt;- tibble(sub_id = rep(sub_id, n_days * n_events_day), day=c(rep(1,5),rep(2,5),rep(3,5),rep(4,5)), event=rep(1:n_events_day,4), virtual_time, real_time, pic = numeric(n_days * n_events_day)) # load design file to get stimulus assignments fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;logs&quot;, &quot;input_files&quot;, sprintf(&quot;P%s_input.txt&quot;, sub_id)) input_data &lt;- read.table(fname) # extract the trials where pics were presented pic_trial_idx = input_data[,5] == 1; # picture trial if column 5 is a 1 pic_trials &lt;- input_data[pic_trial_idx,] # assign the 5 picture IDs of each day to the respective days for (i_day in 1:n_days){ all_reps &lt;- pic_trials[pic_trials[,3]==i_day,6] # all repetitions for this day design_tbl$pic[design_tbl$day==i_day] &lt;- all_reps[1:n_events_day] # extract IDs for first rep # check assumption that IDs are correct is met if (!identical(all_reps, rep(all_reps[1:n_events_day],7))){ stop(&quot;error when finding picture IDs for subject &quot;, sub_id) } } # sanity check: compare to Lorena&#39;s day order #fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;design&quot;, sprintf(&quot;P%s_daysOrder.txt&quot;, sub_id)) #days_order &lt;- read.csv(fname, header = FALSE) #days_order &lt;- c(t(as.matrix(days_order))) #stopifnot(all(days_order == design_tbl$pic)) # write design tibble to file for this subject out_dir = here(&quot;data&quot;, &quot;behavior&quot;, &quot;design_tbl&quot;) if(!dir.exists(out_dir)){dir.create(out_dir)} write_csv(design_tbl, file.path(out_dir, sprintf(&quot;%s_design_tbl.txt&quot;, sub_id))) } } # call the function to get the design tibble for each subject and write it to file virtem_behavior_prepare_design_tbl(subjects = subjects) 4.1 Sorting task Here is a description of the task from the methods: The day sorting task (Figure 1D) was performed in front of a computer screen. The 20 event images from the day learning task were presented on the screen in a miniature version. They were arranged in a circle around a central area displaying 4 rectangles. Participants were instructed to drag and drop all events of the same sequence into the same rectangle with a computer mouse. Participants freely chose which rectangle corresponded to which sequence as the sequences were not identifiable by any label and were presented in differing orders across mini-blocks during learning. Thus, in analysis, we take the grouping as provided by the rectangles and assign the four groups of events to the four days in a way that maximizes the overlap between actual days and sorted days. This may be trivial if performance is perfect or very high, but if several scenes were mixed up between days, it can become difficult to ascertain which day participants were trying to re-assemble. The best fitting permutation is written to file. For analysis of the sorting task, we took the grouping of event images as provided by the participants and assigned them to the four sequences to ensure maximal overlap between actual and sorted sequence memberships. While the assignment of groupings to sequences is unambiguous when performance is, as in our sample, high, this procedure is potentially liberal at lower performance levels. We then calculated the percentage of correctly sorted event images for each participant, see the raincloud plot(100) in Figure 2A. # define function to prepare the data from the day sorting task virtem_behavior_prepare_data_day_sorting &lt;- function(subjects = c(&quot;036&quot;, &quot;037&quot;)){ # initialize n_correct = numeric(length(subjects)) out_dir = here(&quot;data&quot;, &quot;behavior&quot;, &quot;day_sorting&quot;) if(!dir.exists(out_dir)){dir.create(out_dir)} for (i_sub in 1:length(subjects)){ sub_id &lt;- subjects[i_sub] # load design file with true assignment of stimuli to days fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;design_tbl&quot;, sprintf(&quot;%s_design_tbl.txt&quot;, sub_id)) col_classes &lt;- c(&quot;sub_id&quot; = &quot;c&quot;, &quot;day&quot;=&quot;n&quot;, &quot;event&quot;=&quot;n&quot;, &quot;pic&quot;=&quot;n&quot;) design_tbl &lt;- as_tibble(read_csv(fname, col_types = col_classes)) # load the data from the day sorting task and store what we need fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;logs&quot;, &quot;day_sorting&quot;, sprintf(&quot;%s_rectangles_results.mat&quot;, sub_id)) sorting_log &lt;- readMat(fname) obj_pos &lt;- sorting_log$objectPositions # extract quadrant for each image pic_q &lt;- numeric(n_days*n_events_day) for (i_pic in 1:nrow(obj_pos)){ # coordinates of current image curr_x &lt;- obj_pos[i_pic,1] curr_y &lt;- obj_pos[i_pic,2] if (curr_x &lt; 0 &amp;&amp; curr_y &gt; 0){ # x smaller than 0, y bigger than 0 --&gt; quadrant 1 pic_q[i_pic] &lt;- 1 } else if (curr_x &gt; 0 &amp;&amp; curr_y &gt; 0){ # x bigger than 0, y bigger than 0 --&gt; quadrant 2 pic_q[i_pic] &lt;- 2 } else if (curr_x &lt; 0 &amp;&amp; curr_y &lt; 0){ # x smaller than 0, y smaller than 0 --&gt; quadrant 3 pic_q[i_pic] &lt;- 3 } else if (curr_x &gt; 0 &amp;&amp; curr_y &lt; 0){ # x bigger than 0, y smaller than 0 --&gt; quadrant 4 pic_q[i_pic] &lt;- 4 } else {stop(&quot;Error, can&#39;t sort!&quot;)} } # match quadrant number to days for all possible combinations # surely there must be a better way to get all combinations as a matrix?! all_perms &lt;- t(array(unlist(permn(1:4)), dim = c(4, 24))) n_matches &lt;- numeric(nrow(all_perms)) for (i_perm in 1:nrow(all_perms)){ # get vector with pic-day assignment for this permutation comb_to_test &lt;-numeric(20) for (i_day in 1:n_days){ comb_to_test[design_tbl$pic[design_tbl$day == all_perms[i_perm, i_day]]] &lt;- i_day } # count the number of matches n_matches[i_perm] &lt;- sum(comb_to_test == pic_q) } # find the best permutation and the number of hits winner_perm &lt;- which.max(n_matches) n_correct[i_sub] &lt;- max(n_matches) # sort the behavioral responses in the same way as the design table to_sort &lt;- 1:20 # this works because the data in the sorting task logfile follows the picture number sort_idx &lt;- match(design_tbl$pic,to_sort) stopifnot(design_tbl$pic == to_sort[sort_idx]) # make sure we have identical vectors now sorted_pic_q &lt;- pic_q[sort_idx] # store which pic was sorted to which day based on winning permutation design_tbl$sorted_day &lt;- numeric(n_days*n_events_day) for (i_day in 1:n_days){ # recode the day labels to match the day labels in the design tibble idx &lt;- sorted_pic_q == i_day design_tbl$sorted_day[idx] &lt;- all_perms[winner_perm,i_day] } stopifnot(sum(design_tbl$day == design_tbl$sorted_day) == n_correct[i_sub]) # write design tibble to file for this subject write_csv(design_tbl, file.path(out_dir,sprintf(&quot;%s_behavior_tbl_day_sorting.txt&quot;, sub_id))) } } # call the function for all subjects virtem_behavior_prepare_data_day_sorting(subjects = subjects) 4.2 Timeline task In this task, participants saw a timeline ranging from 6 a.m. to midnight together with miniature versions of the five event images belonging to one sequence (Figure 1E). Participants were instructed to drag and drop the event images next to the timeline so that scene positions reflected the event times they had inferred in the day learning task. To facilitate precise alignment to the timeline, event images were shown with an outward pointing triangle on their left side, on which participants were instructed to base their responses. Participants responses are read out from the logfiles of this task and converted to virtual hours. The data are saved in the text file including all behavioral data (virtem_behavioral_data.txt). # define the function to prepare data from the timeline task virtem_behavior_prepare_data_timeline &lt;- function(subjects = c(&quot;036&quot;, &quot;037&quot;, &quot;039&quot;)){ # initialize out_dir = here(&quot;data&quot;, &quot;behavior&quot;, &quot;timeline&quot;) if(!dir.exists(out_dir)){dir.create(out_dir)} for (i_sub in 1:length(subjects)){ sub_id &lt;- subjects[i_sub] # load design file with true assignment of stimuli to days (&amp; data from day sorting) fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;day_sorting&quot;, sprintf(&quot;%s_behavior_tbl_day_sorting.txt&quot;, sub_id)) col_classes &lt;- c(&quot;sub_id&quot; = &quot;c&quot;, &quot;day&quot;=&quot;n&quot;, &quot;event&quot;=&quot;n&quot;, &quot;virtual_time&quot; = &quot;d&quot;, &quot;real_time&quot; = &quot;d&quot;, &quot;pic&quot;=&quot;n&quot;, &quot;sorted_day&quot; = &quot;d&quot;) design_tbl &lt;- as_tibble(read_csv(fname, col_types = col_classes)) # add column for remembered time based on timeline task design_tbl$memory_time &lt;- numeric(n_days * n_events_day) for (i_day in 1:n_days){ # load the data from the timeline task from this day fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;logs&quot;, &quot;timeline&quot;, sprintf(&quot;%s_timeline_results_day%d.mat&quot;, sub_id, i_day)) timeline_log &lt;- readMat(fname) for (i_pic in 1:n_events_day){ # get the number of this picture pic_name &lt;- timeline_log$ud[[2]][1+(i_pic-1)*2] pic &lt;- as.numeric(str_extract(pic_name, &quot;\\\\d{1,2}&quot;)) # which row in our table are we looking at tbl_idx &lt;- which(design_tbl$pic == pic) # extract the response from the logfile and transform it to be in virtual hours design_tbl$memory_time[tbl_idx] &lt;- (timeline_log$objectPositions[i_pic,2]+0.5)*18+6 } } # store the remembered order for each virtual day based on remembered times design_tbl &lt;- design_tbl %&gt;% group_by(day) %&gt;% mutate( memory_order = rank(memory_time, ties.method = &quot;first&quot;)) # write design tibble to file for this subject write_csv(design_tbl, file.path(out_dir, sprintf(&quot;%s_behavior_tbl_timeline.txt&quot;, sub_id))) } } # run for all subjects virtem_behavior_prepare_data_timeline(subjects = subjects) Now we are ready to combine the data from the two memory tests into the final dataframe that we write to file for the actual analyses. # define function virtem_behavior_prepare_data_combine_across_subjects &lt;- function(subjects = c(&quot;036&quot;, &quot;037&quot;)){ # set up a dataframe to collect the data beh_data = tibble() for (i_sub in 1:length(subjects)){ sub_id &lt;- subjects[i_sub] # load data from CSV fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;timeline&quot;, sprintf(&quot;%s_behavior_tbl_timeline.txt&quot;, sub_id)) col_types_list &lt;- cols_only( sub_id = col_integer(), day = col_integer(), event = col_integer(), pic = col_integer(), virtual_time = col_double(), real_time = col_double(), memory_time = col_double(), memory_order = col_double(), sorted_day = col_integer()) timeline_tbl &lt;- as_tibble(read_csv(fname, col_types = col_types_list)) # append to table with data from all subjects beh_data &lt;- bind_rows(beh_data, timeline_tbl) } # reorder to have a more intuitive order beh_data &lt;- beh_data[,c(1,2,3,6,4,5,8,9,7)] # write data to file write_csv(beh_data, file.path(dirs$data4analysis, &quot;behavioral_data.txt&quot;)) } # combine behavioral data frames across subjects virtem_behavior_prepare_data_combine_across_subjects(subjects = subjects) We have a similar dataframe from Nicole Montijn’s study conducted at Utrecht University that we will later use to replicate the generalization bias. Let’s move it to the folder with the analysis data. # copy to analysis data folder (that will be shared) fname &lt;- here(&quot;data&quot;, &quot;behavior&quot;, &quot;replication_data_montijn&quot;, &quot;beh_dataNDM.txt&quot;) file.copy(fname, dirs$data4analysis) ## [1] FALSE 4.3 Picture viewing tasks In the picture viewing tasks (Figure 1B), participants viewed a stream of the event images. Their task was to look at the images attentively and to respond via button press whenever a target picture, which showed the father feeding the family’s dog, was presented. Below, we check how well participants detected the targets. pvt_target_detect &lt;- tibble() for (i_run in 1:n_runs){ for (i_sub in 1:length(subjects)){ # load the logfile for this run (pre or post) log_fn &lt;- file.path(dirs$pvt_log_dir, sprintf(&#39;P%s_%svirtem.txt&#39;, subjects[i_sub], runs[i_run])) log &lt;- read.table(log_fn) colnames(log) &lt;- c(&quot;pic&quot;, &quot;fix_start&quot;, &quot;pic_start&quot;, &quot;volume&quot;, &quot;response&quot;, &quot;RT&quot;, &quot;trial_end&quot;) # add column for block and subject ID to log log &lt;- log %&gt;% add_column(run = runs[i_run]) %&gt;% add_column(sub_id = subjects[i_sub]) # calculate proportion of hits and average RT curr_dat &lt;- log %&gt;% filter(pic == 21) %&gt;% summarise(sub_id = unique(sub_id), run = unique(run), perc_hits = sum(response)/nrow(.)*100, avg_rt = mean(RT[response==1])) # calculate average RT for hits # add to the overall tibble pvt_target_detect &lt;- rbind(pvt_target_detect, curr_dat) } } head(pvt_target_detect) sub_idrunperc_hitsavg_rt 031pre100921 032pre90993 033pre70904 034pre100948 035pre100710 036pre100747 # calculate mean and standard deviation across participants pvt_target_detect_summary &lt;- pvt_target_detect %&gt;% group_by(run) %&gt;% summarise(mean_perc_hits = mean(perc_hits), sd_perc_hits = sd(perc_hits), mean_rt = mean(avg_rt), sd_rt = sd(avg_rt), .groups=&quot;drop&quot;) head(pvt_target_detect_summary) runmean_perc_hitssd_perc_hitsmean_rtsd_rt post95.76.9841162 pre95.77.9881131 Target detection in picture viewing task: Pre-learning: 95.71% ± 7.90% mean±standard deviation of percentage of hits; 881.34ms ± 131.43ms mean±standard deviation of average reaction times Post-learning: 95.71% ± 6.90% mean±standard deviation of percentage of hits; 841.40ms ± 162.16ms mean±standard deviation of average reaction times "],
["behavioral-analysis.html", "5 Behavioral Analysis 5.1 Sorting Task 5.2 Timeline Task Compose Figure for Behavioral Data 5.3 Generalization bias 5.4 Replication of Generalization Bias 5.5 Swap Errors", " 5 Behavioral Analysis We are now ready to analyze and plot the behavioral data. Let’s start with a quick recap of the data and tasks we have: Sorting Task The day sorting task (Figure 1D) was performed in front of a computer screen. The 20 event images from the day learning task were presented on the screen in a miniature version. They were arranged in a circle around a central area displaying 4 rectangles. Participants were instructed to drag and drop all events of the same sequence into the same rectangle with a computer mouse. Participants freely chose which rectangle corresponded to which sequence as the sequences were not identifiable by any label and were presented in differing orders across mini-blocks during learning. Thus, in analysis, we took the grouping as provided by the rectangles and assigned the four groups of events to the four days in a way that there was maximal overlap between actual days and sorted days. We found the best solution for this by trying all combinations in a preparation step. Timeline task In this task, participants saw a timeline ranging from 6 a.m. to midnight together with miniature versions of the five event images belonging to one sequence (Figure 1E). Participants were instructed to drag and drop the event images next to the timeline so that scene positions reflected the event times they had inferred in the day learning task. To facilitate precise alignment to the timeline, event images were shown with an outward pointing triangle on their left side, on which participants were instructed to base their responses. Participants responses were read out from the logfiles of this task and converted to virtual hours. The data are saved in the text file including all behavioral data (virtem_behavioral_data.txt). Let’s begin by loading the data into a tibble (dataframe) with the following columns: sub_id = subject identifier for all rows of this subject day = virtual day. There are 4 virtual days event = number of event in a day, i.e. order. Each day has 5 events. pic = picture identifier. Pictures were randomly assigned to events and days for each subject virtual_time = true virtual time of an event real_time = real time of an event memory_time = data from the timeline task, where participants indicated remembered (virtual) time of each event memory_order = remembered ordinal position of an event sorted_day = data from the day sorting task, where participants sorted all event pictures into the 4 days. # load data from CSV fname &lt;- file.path(dirs$data4analysis, &quot;behavioral_data.txt&quot;) col_types_list &lt;- cols_only( sub_id = col_factor(), day = col_factor(), event = col_factor(), pic = col_integer(), virtual_time = col_double(), real_time = col_double(), memory_time = col_double(), memory_order = col_double(), sorted_day = col_integer() ) beh_data &lt;- as_tibble(read_csv(fname, col_types = col_types_list)) head(beh_data) sub_iddayeventpicvirtual_timereal_timememory_timememory_ordersorted_day 3111129.257.819.4511 31122011.8&nbsp;23.4&nbsp;11.5&nbsp;21 3113214.8&nbsp;42.2&nbsp;14.5&nbsp;31 3114816.2&nbsp;51.6&nbsp;16.5&nbsp;41 31151918.2&nbsp;64.1&nbsp;19&nbsp;&nbsp;&nbsp;51 312119.5&nbsp;9.389.5312 5.1 Sorting Task For analysis of the sorting task, we took the grouping of event images as provided by the participants and assigned them to the four sequences to ensure maximal overlap between actual and sorted sequence memberships. While the assignment of groupings to sequences is unambiguous when performance is, as in our sample, high, this procedure is potentially liberal at lower performance levels. We then calculated the percentage of correctly sorted event images for each participant, see the raincloud plot(100) in Figure 2A. Calculate percentage of correct responses So to calculate participants’ accuracy in this task we need to figure out how often the true day label matches the label of the quadrant into which an event was sorted. # calculate the number and percentages of correct sorts day_sorting &lt;- beh_data %&gt;% group_by(sub_id) %&gt;% summarise( n_correct = sum(day == sorted_day), prcnt_correct = sum(day == sorted_day)/(n_days*n_events_day)*100, group = as.factor(1), .groups = &quot;drop&quot; ) # print a simple summary of descriptive statistics summary(day_sorting$prcnt_correct) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 40.00 75.00 95.00 86.43 100.00 100.00 Results of the sorting task: 86.43 ± 16.82 mean±standard deviation of correct sorts Plot sorting performance # raincloud plot f2a &lt;- ggplot(day_sorting,aes(x=1,y=prcnt_correct, fill = group, colour = group)) + # plot the violin gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, fill=ultimate_gray, color= NA) + # single subject data points with horizontal jitter geom_point(aes(x = 1-.2), alpha = 0.7, shape=16, colour=ultimate_gray, size = 1, position = position_jitter(width = .1, height = 0)) + # box plot of distribution geom_boxplot(width = .1, colour = &quot;black&quot;) + # add plot of mean and SEM stat_summary(fun = mean, geom = &quot;point&quot;, size=1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5) + # edit axis labels and limit ylab(&#39;% correct&#39;) + ylim(0, 100) + xlab(&#39;sorting task&#39;) + # make it pretty theme_cowplot() + theme(legend.position = &quot;none&quot;, axis.ticks.x = element_blank(), axis.text.x = element_blank()) + # change color scale_color_manual(values = ultimate_gray) + scale_fill_manual(values = ultimate_gray) + guides(color = &quot;none&quot;, fill = &quot;none&quot;) f2a # save source data source_dat &lt;-ggplot_build(f2a)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f2a.txt&quot;)) Sorting errors as a function of sequence position # DISTRIBUTION OF SORTING ERRORS # test whether frequencies of sorting errors for the sequence positions differs from uniformity stats &lt;- beh_data %&gt;% filter(sorted_day != day) %&gt;% group_by(event) %&gt;% summarise(n_sort_errs = sum(sorted_day != day), .groups=&quot;drop&quot;) %&gt;% pull(n_sort_errs) %&gt;% chisq.test() %&gt;% tidy() # histogram of sorting errors as a function of sequence position fig_sort_err_hist &lt;- ggplot(beh_data %&gt;% filter(sorted_day != day), aes(x=event, fill = event)) + geom_bar(stat=&quot;count&quot;) + scale_fill_manual(values = event_colors) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) fn &lt;- here(&quot;figures&quot;, &quot;letter_sort_errors_histogram&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=fig_sort_err_hist, units = &quot;cm&quot;, width = 4, height = 5.5, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=fig_sort_err_hist, units = &quot;cm&quot;, width = 4, height = 5.5, dpi = &quot;retina&quot;, device = &quot;png&quot;) print(fig_sort_err_hist) \\(\\chi^2\\) test against uniformity for distribution of sorting errors: \\(\\chi^2\\)=2.55, p=0.635 5.2 Timeline Task Visualize the raw data First, let’s get an overview of participants’ behavior in this task. For this, we plot the responses from the timeline task for all participants. In these plots, each row is one virtual day. The circles along the gray lines represent the true virtual times that participants were supposed to learn. For each event on each day, we then add a full plot of the behavior. This includes the single-subject data points (colored circles), their mean and standard error across subjects (black circle and line) as well as the boxplot and kernel density plot of the distribution. # reduce data frame to the times specified by the design design_temp_struct &lt;- beh_data %&gt;% group_by(day, event) %&gt;% summarise(virtual_time = unique(virtual_time), .groups = &quot;drop&quot;) # raincloud plot f2b &lt;- ggplot(beh_data, aes(x=day,y=memory_time, fill = virtual_time, colour = virtual_time, group=paste(day, event, sep = &quot;_&quot;))) + gghalves::geom_half_violin(position = &quot;identity&quot;, side = &quot;r&quot;, color = NA) + # plot the violin #geom_flat_violin(position = position_nudge(x = 0.1, y = 0),adjust = 1, trim = TRUE) + # single subject data points with horizontal jitter geom_point(aes(x = as.numeric(day)-0.25, y = memory_time, fill = virtual_time), alpha = 0.7, position = position_jitter(width = .05, height = 0), shape=16, size = 1) + # box plot of distribution geom_boxplot(position = position_nudge(x = -0.1, y = 0), aes(x = day, y = memory_time), width = .05, colour = &quot;black&quot;, outlier.shape = NA) + # add plot of mean and SEM stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, colour = &quot;black&quot;, shape = 16) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5)+ # plot point and line for true virtual times geom_line(data = design_temp_struct, aes(x = day, y = virtual_time, group = day), position = position_nudge(-0.45), size = 1, colour = ultimate_gray)+ geom_point(data = design_temp_struct, aes(x = day, y = virtual_time, fill = virtual_time), position = position_nudge(-0.45), size = 2, color = ultimate_gray, stroke = 0.5, shape = 21) + ylab(&#39;virtual time&#39;) + xlab(&#39;sequence&#39;) + scale_y_continuous(limits = c(6, 24), breaks=seq(6,24,4)) + scico::scale_fill_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + scico::scale_color_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + coord_flip() + guides(color=guide_colorbar(title.position = &quot;top&quot;, direction = &quot;horizontal&quot;, title = element_blank(), barwidth = unit(20, &quot;mm&quot;), barheight = unit(3, &quot;mm&quot;)), fill = &quot;none&quot;)+ theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = c(1,1), legend.justification = c(1,1)) f2b # save source data source_dat &lt;-ggplot_build(f2b)$data[[2]] %&gt;% rename(x=y,y=x) # rename because of flipped coordinates readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f2b.txt&quot;)) Accuracy of remembered times We analyzed how well participants constructed the event times based on the day learning task. We quantified absolute errors across all events (Figure 2C) as well as separately for the five sequence positions (Figure 2D), the four sequences (Supplemental Figure 3A) and as a function of virtual clock speed (Supplemental Figure 3B). Average absolute error per participant Now, let’s look at the average error per participant collapsed across all trials. # calculate signed timeline error as difference between virtual time and response beh_data &lt;- beh_data %&gt;% mutate(timeline_error = virtual_time - memory_time) # average absolute timeline error across subjects timeline_group &lt;- beh_data %&gt;% group_by(sub_id) %&gt;% summarise( avg_error = mean(abs(timeline_error)), .groups = &quot;drop&quot;) # print summary of the average error summary(timeline_group$avg_error) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2377 0.5625 0.9093 0.9103 1.1221 2.0963 f2c &lt;- ggplot(timeline_group, aes(x=1,y=avg_error), colour= time_colors[1], fill=time_colors[1]) + # plot the violin gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, fill=time_colors[1], color = NA) + # single subject data points with horizontal jitter geom_point(aes(x = 1-.2), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, colour=time_colors[1], size = 1)+ # box plot of distribution geom_boxplot(width = .1, fill=time_colors[1], colour = &quot;black&quot;, outlier.shape = NA) + # add plot of mean and SEM stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ # edit axis labels ylab(&#39;absolute error&#39;) + xlab(&#39;timeline task&#39;) + guides(color = &quot;none&quot;, fill = &quot;none&quot;)+ theme_cowplot() + theme(legend.position = &quot;none&quot;, axis.text.x = element_blank(), axis.ticks.x = element_blank()) f2c # save source data source_dat &lt;-ggplot_build(f2c)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f2c.txt&quot;)) Timeline task: 0.91 ± 0.47 mean±standard deviation of average absolute errors. Event-wise average absolute errors We can also aggregate the data across days by group the events as a function of event position (i.e. the order). Let’s look at the absolute errors in the timeline task for the events at positions 1-5, averaged across days. # calculate mean absolute error per event for each subject timeline_per_event &lt;- beh_data %&gt;% group_by(sub_id, event) %&gt;% summarise( mean_abs_error =mean(abs(timeline_error)), .groups = &quot;drop&quot;) # raincloud plot f2d &lt;- ggplot(timeline_per_event,aes(x=event,y=mean_abs_error, fill = event)) + gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, colour=NA) + geom_point(aes(x = as.numeric(event)-.2, y = mean_abs_error, colour=event), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), aes(x = event, y = mean_abs_error), width = .1, colour = &quot;black&quot;, outlier.shape = NA, outlier.size = 1) + stat_summary(fun = mean, geom = &quot;point&quot;, size=1, shape = 16, position = position_nudge(.1), colour = &quot;BLACK&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;BLACK&quot;, width = 0, size = 0.5)+ ylab(&#39;absolute error&#39;)+ xlab(&#39;event&#39;) + scale_colour_manual(values = event_colors, name=&quot;event position&quot;) + scale_fill_manual(values = event_colors) + guides(fill = &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2))) + theme_cowplot() + theme(legend.position = &quot;none&quot;) f2d # save source data source_dat &lt;-ggplot_build(f2d)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y, group), file = file.path(dirs$source_dat_dir, &quot;f2d.txt&quot;)) Timeline error differences between sequences Let’s test if absolute errors differ between sequences. We use a permutation-based repeated measures ANOVA with permutation-based t-tests for post-hoc comparisons. # mean absolute timeline error per day timeline_group_by_day &lt;- beh_data %&gt;% group_by(sub_id, day) %&gt;% summarise(avg_timeline_error = mean(abs(virtual_time-memory_time)), .groups = &quot;drop&quot;) # permutation repeated measures from permuco package with day as the only factor set.seed(511) # set seed for reproducibility aov_fit_perm &lt;- permuco::aovperm(formula = avg_timeline_error ~ day + Error(sub_id/day), data = timeline_group_by_day, method = &quot;Rd_kheradPajouh_renaud&quot;, np =n_perm) summary(aov_fit_perm) SSndfnSSddfdMSEnMSEdFparametric P(&gt;F)permutation P(&gt;F) 5.54325.5811.850.3155.860.001130.0008 # posthoc pairwise t-tests set.seed(511) # set seed for reproducibility for (i in 1:(n_days-1)){ for (j in (i+1):n_days){ stats &lt;- paired_t_perm_jb(timeline_group_by_day$avg_timeline_error[timeline_group_by_day$day==i]- timeline_group_by_day$avg_timeline_error[timeline_group_by_day$day==j], n_perm = n_perm) print(sprintf(&quot;sequence %d vs. %d: t=%.2f, p=%.3f&quot;, i, j, stats$statistic, round(stats$p_perm, 3))) } } ## [1] &quot;sequence 1 vs. 2: t=3.38, p=0.001&quot; ## [1] &quot;sequence 1 vs. 3: t=-0.12, p=0.912&quot; ## [1] &quot;sequence 1 vs. 4: t=2.59, p=0.013&quot; ## [1] &quot;sequence 2 vs. 3: t=-2.92, p=0.001&quot; ## [1] &quot;sequence 2 vs. 4: t=-1.15, p=0.271&quot; ## [1] &quot;sequence 3 vs. 4: t=2.15, p=0.023&quot; # raincloud plot sfig_beh_a &lt;- ggplot(timeline_group_by_day,aes(x=day,y=avg_timeline_error)) + gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, colour=NA, fill=ultimate_gray) + geom_point(aes(x = as.numeric(day)-.2, y = avg_timeline_error), alpha = 0.7, fill=ultimate_gray, position = position_jitter(width = .1, height = 0), shape=16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), aes(x = day, y = avg_timeline_error), width = .1, colour = &quot;black&quot;, fill=ultimate_gray, outlier.shape = NA, outlier.size = 1) + stat_summary(fun = mean, geom = &quot;point&quot;, size=1, shape = 16, position = position_nudge(.1), colour = &quot;BLACK&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;BLACK&quot;, width = 0, size = 0.5)+ ylab(&#39;timeline error&#39;)+ xlab(&#39;sequence&#39;) + annotate(geom = &quot;text&quot;, x = c(1.5, 2.5), y = Inf, label = &#39;underline(&quot; * &quot;)&#39;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use, parse=TRUE) + guides(fill = &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2))) + theme_cowplot() + theme(legend.position = &quot;none&quot;) sfig_beh_a Effect of Clock Speed on Timeline Errors Next, we test whether behavior differed between sequences as a function of clock speed (fast or slow). # add clock speed variable beh_data &lt;- beh_data %&gt;% mutate(clock_speed = factor(ifelse((day == 1 | day ==2), &quot;slow&quot;, &quot;fast&quot;), levels=c(&quot;slow&quot;, &quot;fast&quot;))) # average absolute errors as a function of speed (and jitter for plotting) timeline_group_by_speed &lt;- beh_data %&gt;% group_by(sub_id, clock_speed) %&gt;% summarise(avg_timeline_error = mean(abs(virtual_time-memory_time)), .groups = &quot;drop&quot;) %&gt;% mutate(x_jit = as.numeric(clock_speed) + rep(jitter(rep(0,n_subs), amount=0.05), each=2) * rep(c(-1,1),n_subs)) # t-test between errors in slow vs. fast stats &lt;- paired_t_perm_jb(timeline_group_by_speed$avg_timeline_error[timeline_group_by_speed$clock_speed==&quot;slow&quot;] - timeline_group_by_speed$avg_timeline_error[timeline_group_by_speed$clock_speed==&quot;fast&quot;], n_perm=n_perm) # raincloud plot sfig_beh_b &lt;- ggplot(data=timeline_group_by_speed, aes(x=clock_speed, y=avg_timeline_error, fill = clock_speed, color = clock_speed)) + geom_boxplot(aes(group=clock_speed), position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + scale_fill_manual(values = c(&quot;#ece7f2&quot;, &quot;#a6bddb&quot;)) + scale_color_manual(values = c(&quot;#ece7f2&quot;, &quot;#a6bddb&quot;), name = &quot;clock speed&quot;, labels=c(&quot;slow&quot;, &quot;fast&quot;)) + gghalves::geom_half_violin(data = timeline_group_by_speed %&gt;% filter(clock_speed == &quot;slow&quot;), aes(x=clock_speed, y=avg_timeline_error), position=position_nudge(-0.1), side = &quot;l&quot;, color = NA) + gghalves::geom_half_violin(data = timeline_group_by_speed %&gt;% filter(clock_speed == &quot;fast&quot;), aes(x=clock_speed, y=avg_timeline_error), position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;, width = 0, size = 0.5) + geom_line(aes(x = x_jit, group=sub_id,), color = ultimate_gray, position = position_nudge(c(0.15, -0.15))) + geom_point(aes(x=x_jit, fill = clock_speed), position = position_nudge(c(0.15, -0.15)), shape=16, size = 1) + ylab(&#39;timeline error&#39;) + xlab(&#39;clock speed&#39;) + annotate(geom = &quot;text&quot;, x = 1.5, y = Inf, label = &#39;underline(&quot; n.s. &quot;)&#39;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use, parse=TRUE) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;) sfig_beh_b Summary Statistics: paired t-test comparing timeline errors of slow vs. fast sequences t27=-0.82, p=0.423 Relationship of sorting errors and timeline errors Correlation To test whether performance in the two tasks is related, we run an across-subject correlation of the number of sorting errors with mean absolute timeline errors. # make usable data frame beh_group &lt;- inner_join(timeline_group, day_sorting) ## Joining, by = &quot;sub_id&quot; beh_group &lt;- beh_group %&gt;% rename(avg_timeline_error = avg_error, n_correct_sorts = n_correct, prcnt_correct_sorts = prcnt_correct) %&gt;% mutate(n_sorting_errors = n_events_day*n_days - n_correct_sorts, perfect_sort = n_sorting_errors==0, perfect_sort = factor(if_else(perfect_sort, &quot;yes&quot;, &quot;no&quot;), levels = c(&quot;yes&quot;, &quot;no&quot;))) # SORTING AND TIMELINE ERRORS # Spearman correlation for all subjects and non-perfect sorters stats &lt;- beh_group %$% cor.test(n_sorting_errors, avg_timeline_error,method = &quot;spearman&quot;) %&gt;% tidy() ## Warning in cor.test.default(n_sorting_errors, avg_timeline_error, method = &quot;spearman&quot;): Cannot compute exact p-value with ties # scatter plot of errors in the two tasks sfig_beh_c &lt;- ggplot(beh_group, aes(x=n_sorting_errors, y=avg_timeline_error)) + geom_smooth(method=&#39;lm&#39;, formula= y~x, size=1, color=ultimate_gray, fill=&quot;lightgrey&quot;) + geom_point(shape = 16, size = 1, color=ultimate_gray) + scale_x_continuous(breaks=seq(0,12,3)) + xlab(&quot;sorting errors&quot;) + ylab(&quot;timeline error&quot;) + annotate(&quot;text&quot;, x = Inf, y = Inf, hjust=&quot;inward&quot;, vjust=&quot;inward&quot;, size=6/.pt, label=sprintf(&quot;r=%.2f\\np=%.3f&quot;, round(stats$estimate, digits = 2), round(stats$p.value, digits=3)), family=font2use) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) sfig_beh_c Spearman correlation of number of sorting errors and mean absolute timeline errors: r=0.23, p=0.246 Comparison of perfect sorters and participants with sorting errors The correlational approach is limited because there is limited variance for sorting errors as a large group of participants performs the sorting task at ceiling and does not make sorting errors. We thus compare in a next step the timeline errors between participants who do make at least one sorting erorr and those who don’t make sorting errors using an independent t-test. # t-test based on perfect sorter or not stats &lt;- tidy(t.test(beh_group$avg_timeline_error[beh_group$perfect_sort==&quot;yes&quot;], beh_group$avg_timeline_error[beh_group$perfect_sort==&quot;no&quot;], var.equal = TRUE)) sfig_beh_d &lt;- ggplot(beh_group, aes(x=perfect_sort, y=avg_timeline_error, fill=perfect_sort, color=perfect_sort)) + geom_boxplot(width = .1, colour = &quot;black&quot;, outlier.shape = NA, outlier.size = 2) + gghalves::geom_half_violin(data=beh_group %&gt;% filter(perfect_sort==&quot;yes&quot;), aes(x=1, y=avg_timeline_error), position=position_nudge(-0.1), side = &quot;l&quot;, colour=NA) + gghalves::geom_half_violin(data=beh_group %&gt;% filter(perfect_sort==&quot;no&quot;), aes(x=2, y=avg_timeline_error), position=position_nudge(0.1), side = &quot;r&quot;, colour=NA) + geom_point(aes(x=ifelse(n_sorting_errors==0, 1.2, 1.8)), alpha = 1, position = position_jitter(width = .1, height = 0), shape = 16, size = 1) + stat_summary(fun = mean, geom = &quot;point&quot;, size=1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;BLACK&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;BLACK&quot;, width = 0, size = 0.5) + ylab(&#39;timeline error&#39;)+xlab(&#39;sorting errors&#39;) + annotate(geom = &quot;text&quot;, x = 1.5, y = Inf, label = &#39;underline(&quot; n.s. &quot;)&#39;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use, parse=TRUE) + scale_x_discrete(labels = c(&quot;0&quot;, &quot;≥1&quot;)) + scale_color_manual(values=c(&quot;#81A88D&quot;, &quot;#972D15&quot;), labels=c(&quot;no sorting error&quot;, &quot;≥1 sorting error&quot;)) + scale_fill_manual(values=c(&quot;#81A88D&quot;, &quot;#972D15&quot;)) + guides(color=guide_legend(&quot;sorting task&quot;, override.aes = list(shape=16, size=1, linetype=0, fill=NA, alpha=1)), fill = &quot;none&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) sfig_beh_d Summary Statistics: independent t-test comparing timeline errors for participants without and with sorting errors t26=-1.79, p=0.085, 95% CI [-0.66, 0.05] Time metrics underlying remembered times Using two approaches we tested whether virtual time drove participants’ responses rather than the sequence order or objectively elapsing time. While participants were asked to reproduce the virtual time, any of the other two factors could have an impact on their behavioral responses. If participants had, for example, only memorized the order of scenes in a given day, they would probably do reasonably well on the timeline task by distributing the scenes evenly along the timeline in the correct order. Summary Statistics For the summary statistics approach, we ran a multiple regression analysis for each participant with virtual time, sequence position (order), and real time since the first event of a day as predictors of responses in the timeline task. To test whether virtual time indeed explained participants’ responses even when competing for variance with order and real time, included in the model as control predictors of no interest, we compared the participant-specific t-values of the resulting regression coefficients against null distributions obtained from shuffling the remembered times against the predictors 10,000 times. We converted the resulting p-values to Z-values and tested these against zero using a permutation-based t-test (two-sided; α=0.05; 10,000 random sign-flips, Figure 2E). As a measure of effect size, we report Cohen’s d with Hedges’ correction and its 95% confidence interval as computed using the effsize-package(101). set.seed(115) # set seed for reproducibility # do RSA using linear model and calculate z-score for model fit from permutations fit &lt;- beh_data %&gt;% group_by(sub_id) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;memory_time ~ virtual_time + event + real_time&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;, &quot;z_order&quot;, &quot;z_real_time&quot;)))) %&gt;% unnest_longer(z) %&gt;% filter(z_id != &quot;z_intercept&quot;) %&gt;% mutate(z_id = factor(z_id, levels = c(&quot;z_virtual_time&quot;, &quot;z_order&quot;, &quot;z_real_time&quot;))) # run group-level t-test for virtual time stats &lt;- fit %&gt;% filter(z_id ==&quot;z_virtual_time&quot;) %&gt;% select(z) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(fit %&gt;% filter(z_id ==&quot;z_virtual_time&quot;) %&gt;% select(z))$z, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 2.4410.63.82e-110.0001271.972.91One Sample t-testtwo.sided1.951.382.7 # raincloud plot of the results f2e &lt;- ggplot(fit, aes(x=z_id, y=z, fill = z_id, colour = z_id)) + # plot the violin gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, colour=NA) + # single subject data points with horizontal jitter geom_point(aes(x = as.numeric(z_id)-.2, y = z, colour = z_id), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape = 16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), aes(x = z_id, y = z), width = .1, colour = &quot;black&quot;, outlier.shape = NA, outlier.size = 2) + stat_summary(fun = mean, geom = &quot;point&quot;, size=1, shape = 16, position = position_nudge(.1), colour = &quot;BLACK&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;BLACK&quot;, width = 0, size = 0.5) + ylab(&#39;Z multiple regression&#39;)+xlab(&#39;time metric&#39;) + scale_x_discrete(labels = c(&quot;virt. time&quot;, &quot;order&quot;, &quot;real time&quot;)) + coord_cartesian(ylim = c(-2,4.5))+ scale_color_manual(labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;),values=time_colors) + scale_fill_manual(labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;),values=time_colors) + guides(fill= &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2), title = element_blank(), direction =&quot;vertical&quot;, title.position = &quot;bottom&quot;)) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;***&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(text = element_text(size=8), axis.text = element_text(size=8), legend.position = c(1,1), legend.justification = c(1,1), legend.spacing.x = unit(0, &#39;mm&#39;), #legend.spacing.y = unit(2, &#39;mm&#39;), legend.key.size = unit(3,&quot;mm&quot;), legend.box.margin = margin(0,0,0,0,&quot;mm&quot;), legend.background = element_rect(size=0)) f2e # save source data source_dat &lt;-ggplot_build(f2e)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y, group), file = file.path(dirs$source_dat_dir, &quot;f2e.txt&quot;)) Summary Statistics: t-test against 0 for virtual time when order and real time are in the model t27=10.62, p=0.000, d=1.95, 95% CI [1.38, 2.70] Linear mixed effects A potentially more elegant way of testing the above is to use linear mixed effects models. However, drawing statistical inferences from these data is less straight forward and care needs to be taken with respect to the precise hypotheses that are tested. Second, we addressed this question using linear mixed effects modeling. Here, we included the three z-scored time metrics as fixed effects. Starting from a maximal random effect structure(102), we simplified the random effects structure to avoid convergence failures and singular fits. The final model included random intercepts and random slopes for virtual time for participants. The model results are visualized by dot plots showing the fixed effect parameters with their 95% confidence intervals (Supplemental Figure 4A) and marginal effects (Supplemental Figure 4B) estimated using the ggeffects package103. To assess the statistical significance (α=0.05) of virtual time above and beyond the effects of order and real time, we compared this full model to a nested model without the fixed effect of virtual time, but including order and real time, using a likelihood ratio test. Supplemental Table 1 provides an overview of the final model and the model comparison. The main point we want to make is that virtual time explains variance above and beyond order. To test this, we run the full LMM and a reduced version of the model without virtual time. These two models are then compared using a likelihood ratio test to obtain a p-value. Following Barr et al. (2013), we want to implement a maximal random effect structure. However, for the models to converge and to avoid singular fits, we have to simplify the random effects structure to include only random intercepts for each subject and a random slope for the effect of virtual time for each subject. Barr et al. (2013, p. 275) “propose the working assumption that it is not essential for one to specify random effects for control predictors to avoid anti-conservative inference, as long as interactions between the control predictors and the factors of interest are not present in the model”. However, “in the common case where one is interested in a minimally anti-conservative evaluation of the strength of evidence for the presence of an effect, our results indicate that keeping the random slope for the predictor of theoretical interest is important” (Barr et al., 2013, p. 276). Thus, we keep the random slopes for virtual time in the model. More specifically, we follow these steps: we start with a model that includes random intercepts and slopes for all 3 fixed effects for each subject. This results in a singular fit Thus, we drop the random effects of real time and order. We do not incorporate random intercepts for the individual events as this sampling unit cannot be dissociated from the predictors. See the description of the exception by Brauer &amp; Curtin (Psych Methods, 2018, p. 13). # center the predictor variables (set scale to true for z-score) beh_data &lt;- beh_data %&gt;% group_by(sub_id) %&gt;% mutate( order_z = scale(as.numeric(event), scale = TRUE), virtual_time_z = scale(virtual_time, scale = TRUE), real_time_z = scale(real_time, scale = TRUE) ) %&gt;% ungroup() # set RNG set.seed(27) # define the full model with all 3 time metrics as fixed effects and # by-subject random intercepts and random slopes for each time metric --&gt; singular fit formula &lt;- &quot;memory_time ~ virtual_time_z + order_z + real_time_z + (1 + virtual_time_z + order_z + real_time_z | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = beh_data, REML = FALSE, control=lmerControl(optCtrl=list(maxfun=20000))) ## boundary (singular) fit: see ?isSingular # remove random slopes for the fixed effects of non-interest (order and real time) # by-subject random intercepts and random slopes for virtual time set.seed(212) # set seed for reproducibility formula &lt;- &quot;memory_time ~ virtual_time_z + order_z + real_time_z + (1 + virtual_time_z | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = beh_data, REML = FALSE, control=lmerControl(optCtrl=list(maxfun=20000))) summary(lmm_full) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: memory_time ~ virtual_time_z + order_z + real_time_z + (1 + virtual_time_z | sub_id) ## Data: beh_data ## Control: lmerControl(optCtrl = list(maxfun = 20000)) ## ## AIC BIC logLik deviance df.resid ## 1940.0 1974.6 -962.0 1924.0 552 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -7.3874 -0.4034 0.0429 0.4259 7.8191 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## sub_id (Intercept) 0.04928 0.2220 ## virtual_time_z 0.02742 0.1656 0.23 ## Residual 1.75541 1.3249 ## Number of obs: 560, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 14.01002 0.06996 200.252 ## virtual_time_z 3.06932 0.25997 11.807 ## order_z 1.66763 0.43023 3.876 ## real_time_z -0.33226 0.47331 -0.702 ## ## Correlation of Fixed Effects: ## (Intr) vrtl__ ordr_z ## virtul_tm_z 0.017 ## order_z 0.000 -0.112 ## real_time_z 0.000 -0.426 -0.841 # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) To test whether virtual time is relevant to explaining the data even when order and real time are in the model, we compare the full model defined above against a reduced model. In this reduced model, we do not include the fixed effect of virtual time. We compare the full against the nested (reduced) model using a likelihood ratio test. # to test for significance let&#39;s by comparing the likelihood against a simpler model. # Here, we drop the effect of virtual time and run an ANOVA. See e.g. Bodo Winter tutorial # this is the current best way of testing the effect of virtual time on behavior formula &lt;- &quot;memory_time ~ order_z + real_time_z + (1 + virtual_time_z | sub_id)&quot; # random intercepts for each subject and random slopes for virtual time set.seed(213) # set seed for reproducibility lmm_no_vir_time &lt;- lme4::lmer(formula, data = beh_data, REML = FALSE, control=lmerControl(optCtrl=list(maxfun=20000))) # because the model fails to converge with a warning that the scaled gradient at the fitted (RE)ML estimates # is large, we restart the model as described here (https://rdrr.io/cran/lme4/man/troubleshooting.html) # obtaining consistent results (with no warning) suggests a false positive lmm_no_vir_time &lt;- update(lmm_no_vir_time, start=getME(lmm_no_vir_time, &quot;theta&quot;)) # run the ANOVA lmm_aov &lt;- anova(lmm_no_vir_time, lmm_full) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 72.05e+032.08e+03-1.02e+032.04e+03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 81.94e+031.97e+03-962&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.92e+0311614.88e-27 Mixed Model: Fixed effect of virtual time time with order and real time in the model of remembered times \\(\\chi^2\\)(1)=115.95, p=0.000 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,3), &quot;residual&quot;) re_names &lt;- c(&quot;intercept&quot;, &quot;virtual time (SD)&quot;, &quot;correlation random intercepts and random slopes&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Virtual time explains constructed times with order and real time in the model&quot;) # convert the huxtable to a flextable for word export stable_lme_memtime_time_metrics &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Virtual time explains constructed times with order and real time in the model fixed effects termestimateSEt-value95% CI intercept14.0100190.069962200.2513.86805614.151981 virtual time3.0693240.25996711.812.5588743.579774 order1.6676300.4302303.880.8227852.512476 real time-0.3322610.473306-0.70-1.2616960.597173 random effects grouptermestimate participantintercept0.221991 participantvirtual time (SD)0.232089 participantcorrelation random intercepts and random slopes0.165592 residualSD1.324919 model comparison modelnparAICLLX2dfp reduced model72053.90-1019.95 full model81939.95-961.98115.9514.88e-27 model: memory_time~virtual_time_z+order_z+real_time_z+(1+virtual_time_z|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation To visualize the model, we use the confidence intervals to create dot plots for the model coefficients. Further, we estimate marginal means for each fixed effect while holding the other parameters constant. We do this for the quartiles (including minimum and maximum values). # make the time metrics a factor lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=as.factor(term) %&gt;% factor(levels = c(&quot;virtual_time_z&quot;, &quot;order_z&quot;, &quot;real_time_z&quot;))) # dot plot of Fixed Effect Coefficients with CIs sfigmm_a &lt;- ggplot(data = lmm_full_bm[2:4,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), size = 1, shape = 16) + scale_fill_manual(values = time_colors) + scale_color_manual(values = time_colors, labels = c(&quot;virtual time (same seq.)&quot;, &quot;order&quot;, &quot;real time&quot;), name = element_blank()) + labs(#title = &quot;Fixed Effect Coefficients&quot;, x = element_blank(), y=&quot;fixed\\neffect estimate&quot;, color = element_blank()) + theme_cowplot() + #coord_fixed(ratio = 3) + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) + guides(fill = &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2, linetype=0)))+ annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;***&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) sfigmm_a # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% get_complete_df # convert the group variable to a factor to control the order of facets below lmm_full_emm$group &lt;- factor(lmm_full_emm$group, levels = c(&quot;virtual_time_z&quot;, &quot;order_z&quot;, &quot;real_time_z&quot;)) # plot marginal means sfigmm_b &lt;- ggplot(data = lmm_full_emm, aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .3, linetype=0) + scale_color_manual(values = time_colors, name=element_blank(), labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;)) + scale_fill_manual(values = time_colors, labels = c(&quot;Virtual Time&quot;, &quot;Order&quot;, &quot;Real Time&quot;)) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;z time metric&#39;) + scale_y_continuous(breaks = c(9, 14, 19), labels = c(9, 14, 19)) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), strip.background = element_blank(), strip.text.x = element_blank()) sfigmm_b LME model assumptions lmm_diagplots_jb(lmm_full) Absence of collinearity: The different time metrics are correlated. This correlation is slightly reduced by z-scoring the predictors. In any case, these correlations are inherent to the design and not really a problem as long as estimated coefficients are interpreted correctly. For more info, see e.g. this opinion piece. Compose Figure for Behavioral Data This figure consists of the plots showing the results of the sorting task and the timeline task. It will probably be figure 2 of the manuscript. layout = &quot; AABBBBBBDDDDDD AABBBBBBDDDDDD AABBBBBBDDDDDD AABBBBBBDDDDDD AABBBBBBDDDDDD AABBBBBBDDDDDD CCBBBBBBEEEEEE CCBBBBBBEEEEEE CCBBBBBBEEEEEE CCBBBBBBEEEEEE CCBBBBBBEEEEEE CCBBBBBBEEEEEE&quot; f2 &lt;- f2a + f2b + f2c + f2d + f2e + plot_layout(design = layout, guides = &quot;keep&quot;) &amp; theme(text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8) ) &amp; plot_annotation(theme = theme(plot.margin = margin(t=0, r=0, b=0, l=-5, unit=&quot;pt&quot;)), tag_levels = &#39;A&#39;) # save as png and pdf and print to screen fn &lt;- here(&quot;figures&quot;, &quot;f2&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=f2, units = &quot;cm&quot;, width = 17.4, height = 16, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=f2, units = &quot;cm&quot;, width = 17.4, height = 16, dpi = &quot;retina&quot;, device = &quot;png&quot;) Figure 2. Participants learn the temporal structure of the sequences relative to the virtual clock. A. Plot shows the percentage of correctly sorted event images in the sorting task. B. Constructed event times were assessed in the timeline task. Responses are shown separately for the five events (color coded according to true virtual time) of each sequence (rows). Colored circles with gray outline show true event times. C, D. Mean absolute errors in constructed times (in virtual hours) are shown (C) averaged across events and sequences and (D) averaged separately for the five event positions. E. Z-values for the effects of different time metrics from participant-specific multiple regression analyses and permutation tests show that virtual time explained constructed event times with event order and real time in the model as control predictors. A-E. Circles are individual participant data; boxplots show median and upper/lower quartile along with whiskers extending to most extreme data point within 1.5 interquartile ranges above/below the upper/lower quartile; black circle with error bars corresponds to mean±S.E.M.; distributions show probability density function of data points. ***p&lt;0.001 5.3 Generalization bias If participants use structural knowledge about the sequences when constructing times of events, then we might expect biases in their behavior: Errors in constructed event times could be non-random. Specifically, when constructing the time of one specific event, participants could be biased in their response by the times of the events from other sequences at that sequence position. This would indicate that knowledge about the other sequences in generalized to influence specific mnemonic constructions, resulting in a bias. Quantify relative time of other events To explore whether structural knowledge about general time patterns biases the construction of event times, we assessed errors in remembered event times. Specifically, when constructing the time of one specific event, participants could be biased in their response by the times of the events from other sequences at that sequence position. For each event, we quantified the average time of events in the other sequences at the same sequence position (Figure 8A). For example, for the fourth event of the first sequence, we calculated the average time of the fourth events of sequences two, three and four. To test for such a bias, we quantify, for each event, the relative time of the other events at that sequence position. We then calculate by how much virtual time each individual event time deviates from the average virtual time of the other events at that sequence position (e.g. the difference in virtual time for event 1 from sequence 1 compared to the average virtual time of events 1 from sequences 2-4). We do this so that positive values of this relative time measure indicate that the other events happened later than the event of interest. # quantify the deviation in virtual time for each event relative to other events # at the same sequence position beh_data$rel_time_other_events&lt;-NA for (i_day in 1:n_days){ for (i_event in 1:n_events_day){ # find the events at this sequence position from all four sequences curr_events &lt;- beh_data[beh_data$event == i_event,] # find the average virtual time of the other events at this sequence position avg_vir_time_other_events &lt;- curr_events %&gt;% filter(day != i_day) %&gt;% summarise(avg_vir_time = mean(virtual_time)) %&gt;% select(avg_vir_time) %&gt;% as.numeric(.) # for the given event, store the relative time of the other events, # as the difference in virtual time (positive values mean other events happen later) event_idx &lt;- beh_data$day==i_day &amp; beh_data$event==i_event beh_data$rel_time_other_events[event_idx] &lt;- avg_vir_time_other_events - beh_data$virtual_time[event_idx] } } Test Generalization Bias We then asked whether the deviation between the average time of other events and an event’s true virtual time was systematically related to signed errors in constructed event times. A positive relationship between the relative time of other events and time construction errors indicates that, when other events at the same sequence position are relatively late, participants are biased to construct a later time for a given event than when the other events took place relatively early. The crucial test is then whether over- and underestimates of remembered time can be explained by this deviation measure. # calculate timeline error so that positive numbers mean overestimates (later than true virtual time) beh_data &lt;- beh_data %&gt;% mutate(timeline_error=memory_time-virtual_time) Summary statistics In the summary statistics approach, we ran a linear regression for each participant (Figure 8B, Supplemental Figure 10A) and tested the resulting coefficients for statistical significance using the permutation-based procedures described above (Figure 8C). In the summary statistics approach, we run a linear regression model for each participant and test the resulting coefficients against a permutation-based null distribution. The resulting z-scores are then tested against 0 on the group level. # SUMMARY STATISTICS set.seed(117) # set seed for reproducibility # test for generalization bias using linear model and calculate z-score for model fit from permutations fit_beh_bias &lt;- beh_data %&gt;% group_by(sub_id) %&gt;% # run the linear model (also without permutation tests to get betas) do(model = lm(timeline_error ~ rel_time_other_events, data=.), z = lm_perm_jb(in_dat = ., lm_formula = &quot;timeline_error ~ rel_time_other_events&quot;, nsim = n_perm)) %&gt;% # store beta estimates and their z-values mutate(beta_rel_time_other_events = coef(summary(model))[2,&quot;Estimate&quot;], t_rel_time_other_events = coef(summary(model))[2,&quot;t value&quot;], #p_rel_time_other_events = coef(summary(model))[2,&quot;Pr(&gt;|t|)&quot;], z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;rel_time_other_events&quot;)))) %&gt;% # get rid of intercept z-values and model column unnest_longer(z) %&gt;% filter(z_id != &quot;z_intercept&quot;) %&gt;% select (.,-c(model)) # add Pearson correlation (for plot annotation only) fit_beh_bias$r &lt;- beh_data %&gt;% group_by(sub_id) %&gt;% summarise(r = cor(rel_time_other_events, timeline_error)) %&gt;% pull(r) ## `summarise()` ungrouping output (override with `.groups` argument) # run group-level t-tests on the RSA fits from the first level in aHPC for within-day stats &lt;- fit_beh_bias %&gt;% filter(z_id ==&quot;rel_time_other_events&quot;) %&gt;% select(z) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=fit_beh_bias$z, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 1.385.321.3e-050.0001270.8491.92One Sample t-testtwo.sided0.9760.5521.48 Summary Statistics: t-test against 0 for generalization bias t27=5.32, p=0.000, d=0.98, 95% CI [0.55, 1.48] We observe a significant positive effect of the virtual time of other events at the same sequence position on remembered virtual time. That means that when other events at the same sequence position are later than a given event, participants are likely to overestimate the event time. Conversely, when the other events at this sequence position relatively early, participants underestimate. This demonstrates an across-sequence effect of virtual time. Virtual time is generalized across sequences to bias remembered times at similar sequence positions. To illustrate this effect, lets plot the data for one example subject (data from all subjects will be plotted below). For this, we pick a subject with an average fit. # pick average subject based on t-value of regression example_sub &lt;- fit_beh_bias$sub_id[sort(fit_beh_bias$t_rel_time_other_events, decreasing = TRUE)[n_subs/2]==fit_beh_bias$t_rel_time_other_events] example_dat &lt;- beh_data %&gt;% filter(sub_id==example_sub) f8b &lt;- ggplot(example_dat, aes(x=rel_time_other_events, y=timeline_error)) + geom_smooth(method=&#39;lm&#39;, formula= y~x, color=aHPC_colors[&quot;across_main&quot;], fill=aHPC_colors[&quot;across_main&quot;])+ geom_point(size = 1, shape = 16) + scale_x_continuous(breaks = c(-2.5, 0, 2.5), labels= c(&quot;-2.5&quot;, &quot;0&quot;, &quot;2.5&quot;)) + xlab(&quot;relative time\\nof other events&quot;) + ylab(&quot;timeline error&quot;) + annotate(&quot;text&quot;, x=Inf, y=Inf, hjust=&quot;inward&quot;, vjust=&quot;inward&quot;, size=6/.pt, label=sprintf(&quot;r=%.2f&quot;, fit_beh_bias$r[fit_beh_bias$sub_id==example_sub]), family=font2use) + theme_cowplot() + theme(strip.background = element_blank(), strip.text = element_blank(), text = element_text(size=10, family=font2use), axis.text = element_text(size=8)) f8b # save source data source_dat &lt;-ggplot_build(f8b)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f8b.txt&quot;)) To visualize the effect from the summary statistics approach on the group-level, we create a raincloud plot of the z-values from the linear model permutations for each subject. # plot regression z-values f8c&lt;-ggplot(fit_beh_bias, aes(x=as.factor(1),y=z), colour= aHPC_colors[&quot;across_main&quot;], fill=aHPC_colors[&quot;across_main&quot;]) + # plot the violin gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, fill=aHPC_colors[&quot;across_main&quot;], color = NA) + geom_point(aes(x = 1-.2), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, colour=aHPC_colors[&quot;across_main&quot;], size = 1)+ geom_boxplot(width = .1, fill=aHPC_colors[&quot;across_main&quot;], colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ ylab(&#39;Z regression&#39;) + xlab(&#39;generalization\\n bias&#39;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;***&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + guides(color = &quot;none&quot;, fill = &quot;none&quot;)+ theme_cowplot() + theme(axis.text.x = element_blank()) f8c # save source data source_dat &lt;-ggplot_build(f8c)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f8c.txt&quot;)) Mixed Model Further, we analyzed these data using the linear mixed model approach (Supplemental Figure 4OP, Supplemental Table 16). We also want to this effect using a linear mixed model. We use the maximal random effects structure with random intercepts and random slopes for participants. Let’s fit the model and get tidy summaries. # z-score the relative time of other events for each participant beh_data &lt;- beh_data %&gt;% group_by(sub_id) %&gt;% mutate(rel_time_other_events_z = scale(rel_time_other_events)) %&gt;% ungroup() # Fit the model set.seed(245) # set seed for reproducibility lmm_full &lt;- lme4::lmer(&quot;timeline_error ~ rel_time_other_events_z + (1+rel_time_other_events_z|sub_id)&quot;, data=beh_data, REML=FALSE) # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) Compare against a reduced model without the fixed effect of interest. # fit reduced model set.seed(248) # set seed for reproducibility lmm_reduced &lt;- lme4::lmer(&quot;timeline_error ~ 1 + (1+rel_time_other_events_z|sub_id)&quot;, data=beh_data, REML=FALSE) lmm_aov&lt;-anova(lmm_full, lmm_reduced) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 51.96e+031.98e+03-9741.95e+03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 61.94e+031.97e+03-9651.93e+0317.912.32e-05 Mixed Model: Fixed effect of relative time of other events on timeline errors \\(\\chi^2\\)(1)=17.90, p=0.000 Make summary table. fe_names &lt;- c(&quot;intercept&quot;, &quot;relative time other events&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,3), &quot;residual&quot;) re_names &lt;- c(&quot;intercept&quot;, &quot;relative time other events (SD)&quot;, &quot;correlation random intercepts and random slopes&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Behavioral generalization bias&quot;) # convert the huxtable to a flextable for word export stable_lme_beh_gen_bias &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Behavioral generalization bias fixed effects termestimateSEt-value95% CI intercept-0.3524810.069962-5.04-0.494444-0.210518 relative time other events0.3372620.0673605.010.2005790.473945 random effects grouptermestimate participantintercept0.220016 participantrelative time other events (SD)-0.114173 participantcorrelation random intercepts and random slopes0.183681 residualSD1.331485 model comparison modelnparAICLLX2dfp reduced model51958.57-974.29 full model61942.67-965.3417.9012.32e-05 model: timeline_error~rel_time_other_events_z+(1+rel_time_other_events_z|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation To visualize the mixed model we create a dot plot of the fixed effect coefficient and the estimated marginal means. # dot plot of Fixed Effect Coefficients with CIs sfigmm_o &lt;- ggplot(data = lmm_full_bm[2,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), size = 1, shape = 16) + scale_fill_manual(values = unname(aHPC_colors[&quot;across_main&quot;])) + scale_color_manual(values = unname(aHPC_colors[&quot;across_main&quot;]), labels = c(&quot;across sequence bias&quot;)) + labs(x = element_blank(), y=&quot;fixed\\neffect estimate&quot;) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) + guides(color = &quot;none&quot;, fill = &quot;none&quot;)+ annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;***&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% get_complete_df # plot marginal means sfigmm_p &lt;- ggplot(data = lmm_full_emm, aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .3, linetype=0) + scale_color_manual(values = unname(aHPC_colors[&quot;across_main&quot;]), name=element_blank()) + scale_fill_manual(values = unname(aHPC_colors[&quot;across_main&quot;])) + scale_x_continuous(breaks = c(-1.5, 0, 1.5), labels= c(&quot;-1.5&quot;, &quot;0&quot;, &quot;1.5&quot;)) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;relative time\\nof other events&#39;) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + theme_cowplot() sfigmm_o+sfigmm_p Lastly, we show that the effect is quite visible from the single-subject plots: The slopes of the least-squares lines are, on average, positive. # plot the relationship for each subject sfig_bias_single_sub &lt;- ggplot(beh_data, aes(x=rel_time_other_events, y=timeline_error)) + geom_smooth(method=&#39;lm&#39;, formula= y~x, color=aHPC_colors[&quot;across_main&quot;], fill=aHPC_colors[&quot;across_main&quot;])+ geom_point(size = 1, shape = 16) + geom_text(data = fit_beh_bias, aes(label = sprintf(&quot; r=%.2f&quot;, round(r,digits = 2))), x=-Inf, y=Inf, hjust=&quot;inward&quot;, vjust=&quot;inward&quot;, size=6/.pt, family=font2use) + scale_x_continuous(breaks = c(-2.5, 0, 2.5), labels= c(&quot;-2.5&quot;, &quot;0&quot;, &quot;2.5&quot;)) + facet_wrap(~sub_id, scales=&quot;free_y&quot;, nrow=4) + xlab(&quot;relative time of other events (virtual hours)&quot;) + ylab(&quot;timeline error&quot;) + theme_cowplot() + theme(strip.background = element_blank(), strip.text = element_blank(), text = element_text(size=10, family=font2use), axis.text = element_text(size=8)) sfig_bias_single_sub 5.4 Replication of Generalization Bias To replicate the results from this exploratory analysis, we conducted the same analysis in an independent group of participants. These participants (n=46) constituted the control groups of a behavioral experiment testing the effect of stress induction on temporal memory(66). They underwent the same learning task as described above with the only difference being the duration of this learning phase (4 rather than 7 mini-blocks of training). The timeline task was administered on the day after learning. The procedures are described in detail in Montijn et al.(66). The data from this independent sample are shown in Figure 8D and Supplemental Figure 10B. # load data from CSV fname &lt;- file.path(dirs$data4analysis, &quot;beh_dataNDM.txt&quot;) col_types_list &lt;- cols_only( sub_id = col_factor(), day = col_factor(), event = col_factor(), virtual_time = col_double(), memory_time = col_double() ) beh_data_replication &lt;- as_tibble(read_csv(fname, col_types = col_types_list)) head(beh_data_replication) sub_iddayeventvirtual_timememory_time P001119.259.21 P0011211.8&nbsp;11.1&nbsp; P0011314.8&nbsp;16.1&nbsp; P0011416.2&nbsp;15&nbsp;&nbsp;&nbsp; P0011518.2&nbsp;18.4&nbsp; P001219.5&nbsp;8.5&nbsp; Replication data: Quantify relative time of other events To test for such a bias, we quantify, for each event, the relative time of the other events at that sequence position. We then calculate by how much virtual time each individual event time deviates from the average virtual time of the other events at that sequence position (e.g. the difference in virtual time for event 1 from sequence 1 compared to the average virtual time of events 1 from sequences 2-4). We do this so that positive values of this relative time measure indicate that the other events happened later than the event of interest. # quantify the deviation in virtual time for each event relative to other events # at the same sequence position beh_data_replication &lt;- beh_data_replication %&gt;% add_column(rel_time_other_events=NA) for (i_day in 1:n_days){ for (i_event in 1:n_events_day){ # find the events at this sequence position from all four sequences curr_events &lt;- beh_data_replication[beh_data_replication$event == i_event,] # find the average virtual time of the other events at this sequence position avg_vir_time_other_events &lt;- curr_events %&gt;% filter(day != i_day) %&gt;% summarise(avg_vir_time = mean(virtual_time)) %&gt;% select(avg_vir_time) %&gt;% as.numeric(.) # for the given event, store the relative time of the other events, # as the difference in virtual time (positive values mean other events happen later) event_idx &lt;- beh_data_replication$day==i_day &amp; beh_data_replication$event==i_event beh_data_replication$rel_time_other_events[event_idx] &lt;- avg_vir_time_other_events - beh_data_replication$virtual_time[event_idx] } } Replication data: Test Generalization Bias The crucial test is then whether over- and underestimates of remembered time can be explained by this deviation measure. # calculate timeline error so that positive numbers mean overestimates (later than true virtual time) beh_data_replication &lt;- beh_data_replication %&gt;% mutate(timeline_error=memory_time-virtual_time) Replication data: Summary statistics In the summary statistics approach, we run a linear regression model for each participant and test the resulting coefficients against a permutation-based null distribution. The resulting z-scores are then tested against 0 on the group level. Both of the tests rely on the custom stats function defined above. # SUMMARY STATISTICS set.seed(120) # set seed for reproducibility # test generalization bias using linear model and calculate z-score for model fit from permutations fit_beh_bias_replication &lt;- beh_data_replication %&gt;% group_by(sub_id) %&gt;% # run the linear model (also without permutation tests to get betas) do(model = lm(timeline_error ~ rel_time_other_events, data=.), z = lm_perm_jb(in_dat = ., lm_formula = &quot;timeline_error ~ rel_time_other_events&quot;, nsim = n_perm)) %&gt;% # store beta estimates and their z-values mutate(beta_rel_time_other_events = coef(summary(model))[2,&quot;Estimate&quot;], t_rel_time_other_events = coef(summary(model))[2,&quot;t value&quot;], z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;rel_time_other_events&quot;)))) %&gt;% # get rid of intercept z-values and model column unnest_longer(z) %&gt;% filter(z_id != &quot;z_intercept&quot;) %&gt;% select (.,-c(model)) # add Pearson correlation (for plot annotation only) fit_beh_bias_replication$r &lt;- beh_data_replication %&gt;% group_by(sub_id) %&gt;% summarise(r = cor(rel_time_other_events, timeline_error)) %&gt;% pull(r) ## `summarise()` ungrouping output (override with `.groups` argument) # run group-level t-tests on the RSA fits from the first level in aHPC for within-day stats &lt;- fit_beh_bias_replication %&gt;% filter(z_id ==&quot;rel_time_other_events&quot;) %&gt;% select(z) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=fit_beh_bias_replication$z, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) ## Warning in pt(q = t, df = df, ncp = x): full precision may not have been achieved in &#39;pnt{final}&#39; ## Warning in pt(q = t, df = df, ncp = x): full precision may not have been achieved in &#39;pnt{final}&#39; stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 1.6611.39.76e-150.0001451.371.96One Sample t-testtwo.sided1.641.232.13 Summary Statistics: t-test against 0 for generalization bias in replication sample t45=11.3, p=0.000, d=1.64, 95% CI [1.23, 2.13] We observe a significant positive effect of the virtual time of other events at the same sequence position on remembered virtual time. That means that when other events at the same sequence position are later than a given event, participants are likely to overestimate the event time. Conversely, when the other events at this sequence position relatively early, participants underestimate. This demonstrates an across-sequence effect of virtual time. Virtual time is generalized across sequences to bias remembered times at similar sequence positions. To visualize this effect from the summary statistics approach, we create a raincloud plot of the z-values from the linear model permutations for each subject. # plot regression z-values f8d &lt;- ggplot(fit_beh_bias_replication, aes(x=as.factor(1),y=z), colour= aHPC_colors[&quot;across_main&quot;], fill=aHPC_colors[&quot;across_main&quot;]) + # plot the violin gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, fill=aHPC_colors[&quot;across_main&quot;], color = NA) + geom_point(aes(x = 1-.2), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, colour=aHPC_colors[&quot;across_main&quot;], size = 1)+ geom_boxplot(width = .1, fill=aHPC_colors[&quot;across_main&quot;], colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ ylab(&#39;Z regression&#39;) + xlab(&#39;generalization\\nbias&#39;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;***&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + guides(color = &quot;none&quot;, fill = &quot;none&quot;)+ theme_cowplot() + theme(axis.text.x = element_blank()) f8d # save source data source_dat &lt;-ggplot_build(f8d)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f8d.txt&quot;)) Replication data: Mixed Model We also want to this effect using a linear mixed model. We use the maximal random effects structure with random intercepts and random slopes for participants. Let’s fit the model and get tidy summaries. # z-score the relative time of other events for each participant beh_data_replication &lt;- beh_data_replication %&gt;% group_by(sub_id) %&gt;% mutate(rel_time_other_events_z = scale(rel_time_other_events)) %&gt;% ungroup() # Fit the model with full random effect structure -&gt; singular lmm_full &lt;- lme4::lmer(&quot;timeline_error ~ rel_time_other_events_z + (1+rel_time_other_events_z|sub_id)&quot;, data=beh_data_replication, REML=FALSE) ## boundary (singular) fit: see ?isSingular # fit again after removing correlation of slope and intercept lmm_full &lt;- lme4::lmer(&quot;timeline_error ~ rel_time_other_events_z + (1+rel_time_other_events_z||sub_id)&quot;, data=beh_data_replication, REML=FALSE) ## boundary (singular) fit: see ?isSingular # fit again after removing correlation of also random intercept (only random slopes left) set.seed(278) # set seed for reproducibility lmm_full &lt;- lme4::lmer(&quot;timeline_error ~ rel_time_other_events_z + (0+rel_time_other_events_z|sub_id)&quot;, data=beh_data_replication, REML=FALSE) ## boundary (singular) fit: see ?isSingular # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) Compare against a reduced model without the fixed effect of interest. # fit reduced model set.seed(221) # set seed for reproducibility lmm_reduced &lt;- lme4::lmer(&quot;timeline_error ~ 1 + (0+rel_time_other_events_z|sub_id)&quot;, data=beh_data_replication, REML=FALSE) lmm_aov&lt;-anova(lmm_full, lmm_reduced) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 34.5e+03&nbsp;4.52e+03-2.25e+034.5e+03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 44.45e+034.47e+03-2.22e+034.44e+0353.712.29e-13 Mixed Model: Fixed effect of relative time of other events on timeline errors \\(\\chi^2\\)(1)=53.74, p=0.000 Make summary table. fe_names &lt;- c(&quot;intercept&quot;, &quot;relative time other events&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,1), &quot;residual&quot;) re_names &lt;- c(&quot;relative time other events (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Behavioral generalization bias (replication)&quot;) # convert the huxtable to a flextable for word export stable_lme_beh_gen_bias_replication &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Behavioral generalization bias (replication) fixed effects termestimateSEt-value95% CI intercept-0.3205640.089155-3.60-0.495488-0.145640 relative time other events0.8636310.0914729.440.6841521.043110 random effects grouptermestimate participantrelative time other events (SD)0.000000 residualSD2.704218 model comparison modelnparAICLLX2dfp reduced model34501.04-2247.52 full model44449.30-2220.6553.7412.29e-13 model: timeline_error~rel_time_other_events_z+(0+rel_time_other_events_z|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation To visualize the mixed model we create a dot plot of the fixed effect coefficient and the estimated marginal means. # dot plot of Fixed Effect Coefficients with CIs sfigmm_q &lt;- ggplot(data = lmm_full_bm[2,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), size = 1, shape = 16) + scale_fill_manual(values = unname(aHPC_colors[&quot;across_main&quot;])) + scale_color_manual(values = unname(aHPC_colors[&quot;across_main&quot;]), labels = c(&quot;across sequence bias&quot;)) + labs(x = element_blank(), y=&quot;fixed\\neffect estimate&quot;) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) + guides(color = &quot;none&quot;, fill = &quot;none&quot;)+ annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;***&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% get_complete_df # plot marginal means sfigmm_r &lt;- ggplot(data = lmm_full_emm, aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .3, linetype=0) + scale_color_manual(values = unname(aHPC_colors[&quot;across_main&quot;]), name=element_blank()) + scale_fill_manual(values = unname(aHPC_colors[&quot;across_main&quot;])) + scale_x_continuous(breaks = c(-1.5, 0, 1.5), labels= c(&quot;-1.5&quot;, &quot;0&quot;, &quot;1.5&quot;)) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;relative time\\nof other events&#39;) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + theme_cowplot() sfigmm_q+sfigmm_r Lastly, we show that the effect is quite visible from the single-subject plots: The slopes of the least-squares lines are, on average, positive. # plot the relationship for each subject sfig_bias_replication_single_sub &lt;- ggplot(beh_data_replication, aes(x=rel_time_other_events, y=timeline_error)) + geom_smooth(method=&#39;lm&#39;, formula= y~x, color=aHPC_colors[&quot;across_main&quot;], fill=aHPC_colors[&quot;across_main&quot;])+ geom_point(size = 1, shape = 16) + geom_text(data = fit_beh_bias_replication, aes(label = sprintf(&quot; r=%.2f&quot;, round(r,digits = 2))), x=-Inf, y=Inf, hjust=&quot;inward&quot;, vjust=&quot;inward&quot;, size=6/.pt, family=font2use) + facet_wrap(~sub_id, scales=&quot;free_y&quot;, ncol=7) + scale_x_continuous(breaks = c(-2.5, 0, 2.5), labels= c(&quot;-2.5&quot;, &quot;0&quot;, &quot;2.5&quot;)) + xlab(&quot;relative time of other events (virtual hours)&quot;) + ylab(&quot;timeline error&quot;) + theme_cowplot() + theme(strip.background = element_blank(), strip.text = element_blank(), text = element_text(size=10, family=font2use), axis.text = element_text(size=8)) layout = &quot; A A A A B B B B B B&quot; sfig_bias_single_sub_both_samples &lt;- sfig_bias_single_sub + sfig_bias_replication_single_sub + plot_layout(design = layout, guides = &quot;keep&quot;) &amp; theme(text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), legend.position = &quot;none&quot; ) &amp; plot_annotation(tag_levels=&quot;A&quot;, theme = theme(plot.margin = margin(t=0, r=0, b=0, l=-5, unit=&quot;pt&quot;))) # save as png and pdf and print to screen fn &lt;- here(&quot;figures&quot;, &quot;sf10&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=sfig_bias_single_sub_both_samples, units = &quot;cm&quot;, width = 17.4, height = 22.5, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=sfig_bias_single_sub_both_samples, units = &quot;cm&quot;, width = 17.4, height = 22.5, dpi = &quot;retina&quot;, device = &quot;png&quot;) Supplemental Figure 10. Generalization bias in individual participants. A, B. Each panel shows the data from one participant. Each circle corresponds to one event. The x-axis indicates the average relative time of the events occupying the same sequence position in other sequences. The y-axis shows the signed error of constructed event times as measured in the timeline task. The regression line and its confidence interval are overlaid in red. Positive slopes of the regression line indicate that constructed event times are biased by the average time of events in the other sequences. Correlation coefficients are based on Pearson correlation. A shows data from the main sample; B from the replication sample. 5.5 Swap Errors Given that participants’ behavior indicates that they generalize across sequences, we next also explore whether there might be systematic errors in the sorting task as well. Specifically, it could be that events belonging to the same sequence position are swapped. In an exploratory analysis, we searched for systematic errors in the sorting task. Specifically, we looked for swap errors where participants interchanged events occurring at the same position between two or more sequences. We begin by defining a function that identifies swap errors based on the sorting task data and logic. The function takes a dataframe as an input that corresponds to the sorting task data of one participant with one row per event image. The expected columns are: day: true day (1:4) event: sequence position of the image (1:5) sorted day: day the event was sorted to correct_sort: logical indicating whether event was sorted to correct day or not. find_swap_errors &lt;- function(in_df = tibble(day = rep(1:4, each=5), event = rep(1:5, times=4), sorted_day = rep(1:4, each=5), correct_sort = day==sorted_day)){ out_df &lt;- in_df out_df$swap_err &lt;- FALSE for (i_event in 1:5){ n_correct &lt;- sum(in_df$day[in_df$event==i_event] == in_df$sorted_day[in_df$event==i_event]) n_unique_days &lt;- n_distinct(in_df$sorted_day[in_df$event==i_event]) # if the four events are sorted to only two days and no event is sorted correctly, # there is a swap between the events that objectively belong to the two days that # all events were sorted to --&gt; 2 swap errors if (n_correct==0 &amp; n_unique_days==2){ sorted_days &lt;- unique(in_df$sorted_day[in_df$event==i_event &amp; in_df$correct_sort==FALSE]) swap_errs_idx &lt;- in_df$event == i_event &amp; in_df$day %in% sorted_days out_df$swap_err[swap_errs_idx] &lt;- TRUE stopifnot(2==sum(swap_errs_idx)) } # if the four events are sorted to three days and no event is sorted correctly, # there is a three-way swap error. The event belonging to the day that no event was sorted too is the error that is not a swap. if (n_correct==0 &amp; n_unique_days==3){ sorted_days &lt;- unique(in_df$sorted_day[in_df$event==i_event &amp; in_df$correct_sort==FALSE]) swap_errs_idx &lt;- in_df$event == i_event &amp; in_df$day %in% sorted_days # selects the 3 events that objectively belong to the days that events were incorrectly sorted to out_df$swap_err[swap_errs_idx] &lt;- TRUE stopifnot(3==sum(swap_errs_idx)) } # if the four events are sorted to four days and no event is sorted correctly, # all four events have been swapped if (n_correct==0 &amp; n_unique_days==4){ swap_errs_idx &lt;- in_df$event == i_event out_df$swap_err[swap_errs_idx] &lt;- TRUE } # if the four events are sorted to three days and one event is sorted correctly, # there are multiple possibilities that need to be tested. if (n_correct==1 &amp; n_unique_days==3){ # There can&#39;t be a swap error for the event(s) sorted to the day where the correct sort was. # So let&#39;s only look at the other days. This means we are looking at two or three events, # depending on whether an incorrect sort occurred to the day whose event was sorted correctly or not correctly_sorted_day &lt;- in_df$day[in_df$event==i_event &amp; in_df$correct_sort == TRUE] curr_dat &lt;- in_df[event == i_event &amp; in_df$sorted_day != correctly_sorted_day,] # if there are two events left, we check for both events whether they were sorted to # the correct day of the respective other event. If yes, the two were swapped! if (nrow(curr_dat)==2){ if(curr_dat$day[1]==curr_dat$sorted_day[2] &amp; curr_dat$day[2]==curr_dat$sorted_day[1]){ swap_errs_idx &lt;- (in_df$event == i_event &amp; in_df$day %in% curr_dat$day) out_df$swap_err[swap_errs_idx] &lt;- TRUE } } # if there are three events left, two of the three are swapped. # we compare all pairwise comparisons of sorted and true days to find out for which pair # the true days match the sorted days if (nrow(curr_dat)==3){ comps &lt;- combinat::combn(3,2) swap_comp1 &lt;- which(curr_dat$day[comps[1,]]==curr_dat$sorted_day[comps[2,]]) swap_comp2 &lt;- which(curr_dat$day[comps[2,]]==curr_dat$sorted_day[comps[1,]]) swap_rows &lt;- comps[, intersect(swap_comp1, swap_comp2)] swap_errs_idx &lt;- (in_df$event == i_event &amp; in_df$day %in% curr_dat$day[swap_rows]) out_df$swap_err[swap_errs_idx] &lt;- TRUE } } # if the four events are sorted to four days and only one event is sorted correctly, # there is a three-way swap between the incorrectly sorted events if (n_correct==1 &amp; n_unique_days==4){ swap_errs_idx &lt;- (in_df$event == i_event &amp; in_df$correct_sort == FALSE) out_df$swap_err[swap_errs_idx] &lt;- TRUE } # if the four events are sorted to four days and two events are sorted correctly, # there is a swap between the two incorrectly sorted events if (n_correct==2 &amp; n_unique_days==4){ swap_errs_idx &lt;- which(in_df$event==i_event &amp; in_df$correct_sort == FALSE) out_df$swap_err[swap_errs_idx] &lt;- TRUE } } return(out_df) } Let’s apply this function to the data of each participant. # add variable denoting if event was sorted correctly beh_data &lt;- beh_data %&gt;% mutate(correct_sort = as.numeric(day==sorted_day), swap_err = FALSE) for (i_sub in as.numeric(subjects)){ # if a subject has sorting errors (i.e. not all 20 events were correctly sorted) run function to identify swap errors if(sum(beh_data$correct_sort[beh_data$sub_id==i_sub])&lt;20){ sub_dat &lt;- find_swap_errors(beh_data %&gt;% filter(sub_id == i_sub)) beh_data$swap_err[beh_data$sub_id==i_sub]&lt;-sub_dat$swap_err } } Descriptives of swap errors Let’s see how many swap errors were made and plot a histogram of swap errors as a function of sorting errors. # sum of swap errors for each participant beh_group$n_swap_errs &lt;- beh_data %&gt;% group_by(sub_id) %&gt;% summarise(n_swap_errs=sum(swap_err), .groups=&quot;drop&quot;) %&gt;% pull(n_swap_errs) # print descriptive stats sprintf(&quot;%d of %d participants made at least one sorting error&quot;, sum(beh_group$n_correct_sorts&lt;20), nrow(beh_group)) ## [1] &quot;14 of 28 participants made at least one sorting error&quot; sprintf(&quot;Of the %d participants with sorting errors, %d made swap errors (mean±S.D: %.1f±%.1f, %.1f%%±%.1f%% of sorting errors were swap errors)&quot;, sum(beh_group$n_correct_sorts&lt;20), sum(beh_group$n_swap_errs[beh_group$n_correct_sorts&lt;20]&gt;0), mean(beh_group$n_swap_errs[beh_group$n_correct_sorts&lt;20]), sd(beh_group$n_swap_errs[beh_group$n_correct_sorts&lt;20]), mean(beh_group$n_swap_errs[beh_group$n_correct_sorts&lt;20]/beh_group$n_sorting_errors[beh_group$n_correct_sorts&lt;20])*100, sd(beh_group$n_swap_errs[beh_group$n_correct_sorts&lt;20]/beh_group$n_sorting_errors[beh_group$n_correct_sorts&lt;20])*100) ## [1] &quot;Of the 14 participants with sorting errors, 12 made swap errors (mean±S.D: 3.1±2.1, 57.5%±34.3% of sorting errors were swap errors)&quot; # histogram of number swap errors sfig_beh_e &lt;- ggplot(beh_group, aes(x=n_swap_errs, group=perfect_sort, fill=perfect_sort)) + geom_histogram(position=position_dodge(width = 0.2),binwidth = 1, alpha=1) + scale_y_continuous(name = &quot;count&quot;, breaks = seq(0,14,2)) + xlab(&quot;swap errors&quot;) + scale_fill_manual(values=c(&quot;#81A88D&quot;, &quot;#972D15&quot;), name=element_blank(), labels = c(&quot;no sorting\\nerror&quot;, &quot;≥1 sorting\\nerror&quot;)) + theme_cowplot() + theme(legend.position = c(1,1), legend.key.size = unit(0.15, &quot;cm&quot;), legend.justification = c(1,1), text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), legend.spacing.x = unit(0.2, &#39;cm&#39;)) sfig_beh_e Swap errors as a function of sequence position Next, we test whether the distribution of swap errors across sequence positions differs from uniformity. We used a χ2-test to assess whether the number of swap errors deviated from uniformity across sequence positions. # DISTRIBUTION OF SWAP ERRORS # test whether frequencies of swap errors for the sequence positions differs from uniformity stats &lt;- beh_data %&gt;% filter(swap_err==TRUE) %&gt;% group_by(event) %&gt;% summarise(n_swap_errs = sum(swap_err), .groups=&quot;drop&quot;) %&gt;% pull(n_swap_errs) %&gt;% chisq.test() %&gt;% tidy() # histogram of swap errors as a function of sequence position sfig_beh_f &lt;- ggplot(beh_data %&gt;% filter(swap_err==TRUE), aes(x=event, fill = event)) + geom_bar(stat=&quot;count&quot;) + scale_fill_manual(values = event_colors) + scale_y_continuous(name = &quot;swap error count&quot;, breaks = seq(0,12,2), labels = seq(0,12,2)) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) sfig_beh_f \\(\\chi^2\\) test against uniformity for distribution of swap errors: \\(\\chi^2\\)=1.07, p=0.899 Swap error: More frequent than expected from chance? So we do observe swap errors, but are they more frequent than what would be expected from chance given the number of sorting errors in our data? To test that, we run a permutation test where we generate a fake sample of participants that has the same number of swap errors as the real participants, but swap errors are introduced at random positions. To test whether participants made more swap errors than expected from chance we ran a permutation test where we introduced sorting errors for randomly selected events. For each of 10 000 iterations, we generated a surrogate sample of sorting results with the number of randomly introduced sorting errors matching the number of errors made by the different participants in our sample. We then quantified the proportion of swap errors across this surrogate sample. This resulted in a distribution of the proportion of swap errors that would be expected from random sorting errors. We assessed how many permutations yielded proportions of swap errors larger or equal to the proportion of swap errors observed in the fMRI sample to compute a p-value and further quantified a z-value as the difference between the observed swap error proportion and the mean of the chance distribution divided by the standard deviation of the chance distribution. # tibble with a perfect sorting result perfect_sort &lt;- tibble(day = rep(1:n_days,each=5), event = rep(1:n_events_day, 4), sorted_day = rep(1:n_days, each=5), sort_err = FALSE) # from the data, find the participants with sorting errors and their number of swap errors n_sort_errs &lt;- beh_group$n_sorting_errors[beh_group$n_sorting_errors&gt;0] n_swap_errs &lt;- beh_group$n_swap_errs[beh_group$n_sorting_errors&gt;0] # matrix in which to store output n_swap_errs_perm &lt;- matrix(NA, nrow=length(n_sort_errs), ncol=n_perm) prop_swap_errs_perm &lt;- matrix(NA, nrow=length(n_sort_errs), ncol=n_perm) for (i_perm in 1:n_perm){ # takes ~30min with 10000 shuffles for (i_sub in 1:length(n_sort_errs)){ found_rand_sort_errs &lt;- FALSE while(found_rand_sort_errs == FALSE){ # shuffle given number of sorts until we end up with the correct number of errors # determine a random subset of trials as sorting errors (number of errors based on actual number of errors of current subject) error_sort &lt;- perfect_sort error_sort$sort_err[sample(x=n_days*n_events_day,size=n_sort_errs[i_sub])] &lt;- TRUE # shuffle the sorted days of the inserted sorting errors error_sort$sorted_day[error_sort$sort_err] &lt;- sample(error_sort$sorted_day[error_sort$sort_err]) error_sort$correct_sort &lt;- error_sort$sorted_day == error_sort$day # break loop if we end up with the correct number of sorting errors # (number of errors can be smaller if the above sample assigns to the correct day by chance) if(sum(!error_sort$correct_sort)==n_sort_errs[i_sub]){ found_rand_sort_errs &lt;- TRUE } } # find swap errors error_sort &lt;- find_swap_errors(in_df = error_sort) # count swap for this permutation subject and store n_swap_errs_perm[i_sub, i_perm] &lt;- sum(error_sort$swap_err) prop_swap_errs_perm[i_sub, i_perm] &lt;- sum(error_sort$swap_err)/n_sort_errs[i_sub] } # subject } # permutation # make tibble of average proportion of swap errors perm_dist &lt;- tibble(prop_swap_errs = colMeans(prop_swap_errs_perm)) # get p and z-value p_sort_perm &lt;- sum(c(perm_dist$prop_swap_errs, mean(n_swap_errs/n_sort_errs)) &gt;= mean(n_swap_errs/n_sort_errs))/ length(c(perm_dist$prop_swap_errs, mean(n_swap_errs/n_sort_errs))) z_sort_perm &lt;- (mean(n_swap_errs/n_sort_errs)-mean(perm_dist$prop_swap_errs))/sd(perm_dist$prop_swap_errs) Permutation test for proportion of swap errors given the number of sorting errors: z=5.07, p=0.000 Plot histogram of the chance distribution. # histogram of average proportions prctile &lt;- as.character(expression(95^{th}*&quot; percentile&quot;)) sfig_beh_g &lt;- ggplot(perm_dist, aes(x=prop_swap_errs)) + geom_histogram(aes(y=..density..), binwidth = 0.01, fill = ultimate_gray) + annotate(geom = &quot;line&quot;, x = quantile(perm_dist$prop_swap_errs, probs=0.95), y=c(0,1.5), color=&quot;BLACK&quot;) + geom_label(aes(x=quantile(prop_swap_errs, probs=0.95), y=1.5, label = prctile), hjust=0.5, vjust=0, size=6/.pt, fill = &quot;white&quot;, alpha=0.5, color=&quot;BLACK&quot;, parse = TRUE, label.padding = unit(0.1, &quot;lines&quot;), label.size = 1/.pt,) + annotate(geom = &quot;line&quot;, x = mean(n_swap_errs/n_sort_errs), y=c(0,3), color=&quot;#972D15&quot;) + geom_label(aes(x=Inf, y=3, label=&quot;% swap errors\\nin fMRI sample&quot;), label.padding = unit(0.1, &quot;lines&quot;), label.size = 1/.pt, hjust=1, vjust=0.5, size=6/.pt, fill = &quot;white&quot;, alpha=0.5, color=&quot;#972D15&quot;) + scale_x_continuous(labels = scales::percent) + xlab(&quot;% swaps of sorting errors&quot;) + ylab(&quot;% of shuffles&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) sfig_beh_g Swap errors and generalization bias Both swap errors and the generalization bias point towards subjects generalizing across sequences in memory. Let’s test if the two are related across subjects. We tested whether the number of swap errors was related to absolute errors in the timeline task (see below) using Spearman’s correlation and a t-test for independent samples. beh_group &lt;- inner_join(beh_group, fit_beh_bias) ## Joining, by = &quot;sub_id&quot; # CORRELATION OF SWAP ERRORS AND GENERALIZATION BIAS # Spearman correlation stats &lt;- beh_group %$% cor.test(n_swap_errs, beta_rel_time_other_events,method = &quot;spearman&quot;) %&gt;% tidy() ## Warning in cor.test.default(n_swap_errs, beta_rel_time_other_events, method = &quot;spearman&quot;): Cannot compute exact p-value with ties # scatter plot of swap errors and generalization bias sfig_beh_h &lt;- ggplot(beh_group, aes(x=n_swap_errs, y=beta_rel_time_other_events)) + geom_smooth(method=&#39;lm&#39;, formula= y~x, size=1, color=ultimate_gray, fill=&quot;lightgrey&quot;) + geom_point(shape = 16, size = 1, color=ultimate_gray) + scale_x_continuous(breaks=seq(0,7,2)) + xlab(&quot;swap errors&quot;) + ylab(&quot;generalization bias&quot;) + annotate(&quot;text&quot;, x=Inf, y=Inf, hjust=&quot;inward&quot;, vjust=&quot;inward&quot;, size=6/.pt, label=sprintf(&quot;r=%.2f\\np=%.3f&quot;, round(stats$estimate, digits=2), round(stats$p.value, digits=3)), family=font2use) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) sfig_beh_h Spearman correlation of number of swap errors and generalization bias: r=-0.12, p=0.528 Again, this correlation is limited by the fact that many subject don’t make sorting errors and thus cannot make any swap errors. So let’s contrast participants with and without swap errors. # t-test based on swap error or not beh_group &lt;- beh_group %&gt;% mutate(min_1swap = factor(if_else(n_swap_errs&gt;0, &quot;yes&quot;, &quot;no&quot;), levels = c(&quot;no&quot;, &quot;yes&quot;))) stats &lt;- tidy(t.test(beh_group$beta_rel_time_other_events[beh_group$min_1swap==&quot;yes&quot;], beh_group$beta_rel_time_other_events[beh_group$min_1swap==&quot;no&quot;], var.equal = TRUE)) sfig_beh_i &lt;- ggplot(beh_group, aes(x=min_1swap, y=beta_rel_time_other_events, fill=min_1swap, color=min_1swap)) + geom_boxplot(width = .1, colour = &quot;black&quot;, outlier.shape = NA, outlier.size = 2) + gghalves::geom_half_violin(data=beh_group %&gt;% filter(min_1swap==&quot;yes&quot;), aes(x=2, y=beta_rel_time_other_events), position=position_nudge(0.1), side = &quot;r&quot;, colour=NA) + gghalves::geom_half_violin(data=beh_group %&gt;% filter(min_1swap==&quot;no&quot;), aes(x=1, y=beta_rel_time_other_events), position=position_nudge(-0.1), side = &quot;l&quot;, colour=NA) + geom_point(aes(x=ifelse(min_1swap==&quot;yes&quot;, 1.8, 1.2)), alpha = 1, position = position_jitter(width = .1, height = 0), shape = 16, size = 1) + stat_summary(fun = mean, geom = &quot;point&quot;, size=1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;BLACK&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;BLACK&quot;, width = 0, size = 0.5) + ylab(&#39;generalization bias&#39;)+xlab(&#39;swap errors&#39;) + annotate(geom = &quot;text&quot;, x = 1.5, y = Inf, label = &#39;underline(&quot; n.s. &quot;)&#39;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use, parse=TRUE) + scale_x_discrete(labels = c(&quot;0&quot;, &quot;≥1&quot;)) + scale_color_manual(values=c(&quot;#81A88D&quot;, &quot;#972D15&quot;), labels=c(&quot;no swap error&quot;, &quot;≥1 swap error&quot;)) + scale_fill_manual(values=c(&quot;#81A88D&quot;, &quot;#972D15&quot;)) + guides(color = guide_legend(&quot;swap errors&quot;, override.aes = list(shape=16, size=1, linetype=0, fill=NA, alpha=1)), fill = &quot;none&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) sfig_beh_i Summary Statistics: independent t-test comparing generalization bias for participants without and with swap errors t26=0.18, p=0.861, 95% CI [-0.18, 0.22] We can now make the Supplemental Figure with all the behavioral supplemental plots. # make new supplemental figure layout &lt;- &quot; AAAAAAAAAABBBBBBBBBBCCCCCCCCCCDDDDDDDDDD EEEEEEFFFFGGGGGGGGGGHHHHHHHHHHIIIIIIIIII&quot; sfig &lt;- sfig_beh_a + sfig_beh_b + sfig_beh_c + sfig_beh_d + sfig_beh_e+ sfig_beh_f + sfig_beh_g + sfig_beh_h + sfig_beh_i + plot_layout(design = layout) + plot_annotation(tag_levels = &quot;A&quot;) &amp; theme(text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=6), legend.title=element_text(size=8)) sfig[[5]] &lt;- sfig[[5]] + theme(legend.spacing.x = unit(0.05, &quot;cm&quot;)) # save fn &lt;- here(&quot;figures&quot;, &quot;sf03&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=sfig, units = &quot;cm&quot;, width = 17.4, height = 11, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=sfig, units = &quot;cm&quot;, width = 17.4, height = 11, dpi = &quot;retina&quot;, device = &quot;png&quot;) Supplemental Figure 3. Memory performance. A. A permutation-based repeated measures ANOVA revealed a significant effect of sequence on mean absolute errors in the timeline task (F3,81=5.86, p&lt;0.001, post hoc contrasts: sequence 1 vs. 2: t27=3.38, p=0.001, sequence 1 vs. 3: t27=-0.12, p=0.912, sequence 1 vs. 4: t27=2.59, p=0.013, sequence 2 vs. 3: t27=-2.92, p=0.001, sequence 2 vs. 4: t27=-1.15, p=0.271, sequence 3 vs. 4: t27=2.15, p=0.023). *p &lt; Bonferroni-adjusted alpha-level of 0.008, corrected for 6 pairwise post hoc comparisons. B. Mean absolute timeline errors did not differ statistically between sequences with fast and slow clock speed (t27=-0.82, p=0.423). C. The number of errors in the sorting task did not correlate with the mean absolute error in the timeline task across participants (r=0.23, p=0.246). D. Mean absolute errors in the timeline task were not statistically different between participants who made one or more errors (red) or no errors in the sorting task (green) in the sorting task (t-test for independent samples, t26=-1.79, p=0.085). E. Histogram shows the number of swap errors for participants with (red) and without (green) errors in the sorting task. F. The distribution of swap errors over sequence positions did not deviate statistically from uniformity (𝝌2(1)=1.07, p=0.899). G. Histogram shows the null distribution of the proportion of swap errors expected under random sorting errors. The proportion of swap errors observed in our sample (red line) exceeded the 95th percentile of the null distribution (black line). H. The number of swap errors was not significantly correlated with the generalization bias (Spearman r=0.12, p=0.528). I. The generalization bias in the timeline task was not significantly different between participants who made one or more swap errors (red) or no swap errors (green) in the sorting task (t26=0.18, p=0.861). A, B, D, H. Circles show individual participant values; boxplot shows median and upper/lower quartile along with whiskers extending to most extreme data point within 1.5 interquartile ranges above/below the upper/lower quartile; black circle with error bars corresponds to mean±S.E.M.; distribution shows probability density function of data points. C, H. Each circle shows data from one participant, grey line and shaded region indicate least squares line and confidence interval. "],
["prepare-fmri-data.html", "6 Prepare (f)MRI Data 6.1 Regions of Interest 6.2 Group-Level Masks 6.3 Quantify Representational Change 6.4 Combine behavioral and fMRI data for RSA", " 6 Prepare (f)MRI Data 6.1 Regions of Interest We want to run ROI-based representational similarity analyses to test how the hippocampal-entorhinal system represents the learned temporal relationships. We will focus our analysis on the anterior hippocampus (aHPC) and anterolateral entorhinal cortex (alEC). Our previous work demonstrates representations reflecting the temporal relations of events from one sequence in the anterior hippocampus(21) and the anterior-lateral entorhinal cortex(27). More generally, these regions have been implicated in temporal coding and memory (for review, see(10)). Further, the hippocampus has been linked to inferential reasoning and generalization(46,48,49,51,53). We thus focused our analyses on these regions. To define participant-specific ROI masks, we want to combine masks from the individual Freesurfer parcellations of the HPC and EC with masks dividing the subregions of the HPC and EC. Specifically, these are based on the Harvard-Oxford atlas distributed with FSL for the hippocampus and on the EC subregion masks from Navarro Schröder et al. (eLife, 2015). We use the Freesurfer segmentation as run by Lorena Deuker. This was obtained via the recon-all command. All masks will be co-registered to the analysis space, which is the space of the wholebrain functional images. This relies on the FSL transformation matrices and warp files obtained from running FEAT on the wholebrain functional images. FreeSurfer Masks Region of interest (ROI) masks were based on participant-specific FreeSurfer segmentations (version 6.0.0-2), which yielded masks for the entire hippocampus and entorhinal cortex. These were co-registered to participants’ functional space. Create Masks from Parcellation First, we want to create nifti masks from the FreeSurfer parcellation. These are saved in the space of the participant’s highres structural space. # create freesurfer subfolder for in each ROI folder invisible(lapply(file.path(dirs$rois_fs_dirs, &quot;freesurfer_highres_space&quot;), function(x) if(!dir.exists(x)) dir.create(x))) for (sub_id in subjects){ # get the name of the ROIs(fs = from freesurfer, hs = highres space) out_roi_fn &lt;- file.path(dirs$rois_fs_dirs, &quot;freesurfer_highres_space&quot;, sprintf(&quot;P%s_%s_fs_hs.nii.gz&quot;, sub_id, rois_fs)) # load the structural image highres_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;wholebrain&quot;, paste0(&quot;VIRTEM_P&quot;, sub_id, &quot;.feat&quot;), &quot;reg&quot;, &quot;highres.nii.gz&quot;) highres_nii &lt;- readNIfTI2(highres_fn) # load the freesurfer output freesurf_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;freesurfer&quot;, paste0(&quot;VIRTEM_P&quot;, sub_id), &quot;mri&quot;, &quot;aparc+aseg-in-rawavg.nii&quot;) aparc_nii &lt;- readNIfTI2(freesurf_fn) for (i_roi in 1:length(rois_fs)){ # initialize all mask voxels as zeros mask_idx &lt;- array(0, dim(img_data(highres_nii))) # set voxels of the ROI to one mask_idx[img_data(aparc_nii) %in% labels_fs[[i_roi]]] &lt;- 1 # create nifti based on the Freesurfer image roi_nii &lt;- highres_nii img_data(roi_nii) &lt;- mask_idx # write the nifti to file writenii(nim = roi_nii, filename = out_roi_fn[i_roi]) # find coordinates for sensible slices to plot (most frequent value in each dim) coords &lt;- arrayInd(which(as.logical(mask_idx)), dim(mask_idx)) coords &lt;- c(min(statip::mfv(coords[,1])), min(statip::mfv(coords[,2])), min(statip::mfv(coords[,3]))) # save a diagnostic plot as a PDF fname &lt;- tools::file_path_sans_ext(out_roi_fn[i_roi], compression = &quot;TRUE&quot;) pdf(paste0(fname,&quot;.pdf&quot;)) ortho2(robust_window(highres_nii, probs = c(0, 0.999)), y = roi_nii, col.y = roi_colors_fs[i_roi], xyz = coords, text = sprintf(&quot;P%s: %s&quot;, sub_id, rois_fs[i_roi]), crosshairs = FALSE) invisible(dev.off()) } } To get an overview of the ROIs that we just created, we create a merged PDF of the diagnostic plots. We do this for each ROI separately. The files are saved to the specific ROI folders. # create a PDF of diagnostic plots for each ROI for (i_roi in 1:length(rois_fs)){ # find all the PDF files of this ROI fnames &lt;- tools::file_path_sans_ext( sprintf(&quot;P%s_%s_fs_hs.nii.gz&quot;, subjects, rois_fs[i_roi]), compression = &quot;TRUE&quot;) fnames &lt;- file.path(dirs$rois_fs_dirs[i_roi], &quot;freesurfer_highres_space&quot;, paste0(fnames, &quot;.pdf&quot;)) # merge PDFs together merged_pdf &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, rois_fs[i_roi], paste0(rois_fs[i_roi], &quot;_freesurfer_highres_space.pdf&quot;)) cmd = paste(&quot;pdfunite&quot;, paste(fnames, collapse = &quot; &quot;), merged_pdf) system(cmd) } Coregister FreeSurfer masks to functional space Next we coregister the masks from FreeSurfer from the highres structural space to the functional analysis space. For this we will use FSL flirt based on the registration files obtained during preprocessing of the wholebrain functional images. Further we will mask them with our partial field of view mask based on the sequence used during the picture viewing tasks. To make sure everything went well, we create diagnostic image for every subject, which we collect in a PDF for visual inspection. # create samespace freesurfer subfolder for in each ROI folder invisible(lapply(file.path(dirs$rois_fs_dirs, &quot;freesurfer_samespace&quot;), function(x) if(!dir.exists(x)) dir.create(x))) # name of transformation matrix file to move from highres to functional space highres2func &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;wholebrain&quot;, paste0(&quot;VIRTEM_P&quot;, subjects, &quot;.feat&quot;), &quot;reg&quot;, &quot;highres2example_func.mat&quot;) # use the mean EPI of wholebrain image as a reference mean_epi &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;samespace&quot;, paste0(&quot;VIRTEM_P&quot;, subjects), paste0(&quot;VIRTEM_P&quot;, subjects, &quot;_wholebrain.nii.gz&quot;)) # functional mask (picture viewing tasks not scanned with wholebrain coverage) brain_mask_ss &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;samespace&quot;, paste0(&quot;VIRTEM_P&quot;, subjects), paste0(&quot;VIRTEM_P&quot;, subjects, &quot;_common_mask.nii.gz&quot;)) for (i_roi in 1:length(rois_fs)){ # define the file names of ROI files in highres space roi_hs &lt;- file.path(dirs$rois_fs_dirs[i_roi], &quot;freesurfer_highres_space&quot;, sprintf(&quot;P%s_%s_fs_hs.nii.gz&quot;, subjects, rois_fs[i_roi])) # define output files in samespace (ss) = analysis space based on the wholebrain EPI roi_ss &lt;- file.path(dirs$rois_fs_dirs[i_roi], &quot;freesurfer_samespace&quot;, sprintf(&quot;P%s_%s_fs_ss.nii.gz&quot;, subjects, rois_fs[i_roi])) # define file names for masked ROI files roi_ss_masked &lt;- file.path(dirs$rois_fs_dirs[i_roi], &quot;freesurfer_samespace&quot;, sprintf(&quot;P%s_%s_fs_ss_masked.nii.gz&quot;, subjects, rois_fs[i_roi])) # apply FSL flirt to move ROI from highres to wholebrain functional space out &lt;- mapply(flirt_apply, infile = roi_hs, reffile = mean_epi, initmat = highres2func, outfile = roi_ss, verbose = FALSE, retimg = FALSE) # mask to make sure to reduce to partial field of view invisible(mapply(fsl_maths, file = roi_ss, opts = sprintf(&quot;-min %s&quot;, brain_mask_ss), outfile = roi_ss_masked, verbose = FALSE, retimg = FALSE)) } # Diagnostic plotting for (i_roi in 1:length(rois_fs)){ # define file names for binarized masks roi_to_plot &lt;- file.path(dirs$rois_fs_dirs[i_roi], &quot;freesurfer_samespace&quot;, sprintf(&quot;P%s_%s_fs_ss_masked.nii.gz&quot;, subjects, rois_fs[i_roi])) fnames &lt;- vector(mode = &quot;character&quot;, length(subjects)) for (i_sub in 1:length(subjects)){ # load nifti of mean epi and the roi mask mean_epi_nii &lt;- readNIfTI2(mean_epi[i_sub]) roi_nii &lt;- readNIfTI2(roi_to_plot[i_sub]) # find coordinates for sensible slices to plot coords &lt;- arrayInd(which(as.logical(img_data(roi_nii))), dim(img_data(roi_nii))) coords &lt;- c(min(statip::mfv(coords[,1])), min(statip::mfv(coords[,2])), min(statip::mfv(coords[,3]))) # save a diagnostic plot as a PDF fnames[i_sub] &lt;- paste0(tools::file_path_sans_ext(roi_to_plot[i_sub], compression = &quot;TRUE&quot;),&quot;.pdf&quot;) pdf(fnames[i_sub]) ortho2(robust_window(mean_epi_nii, probs = c(0, 0.999)), y = roi_nii, col.y = roi_colors_fs[i_roi], xyz = coords, text = sprintf(&quot;P%s: %s\\nmasked&quot;, subjects[i_sub], rois_fs[i_roi]), crosshairs = FALSE) invisible(dev.off()) } # merge PDFs together merged_pdf &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, rois_fs[i_roi], paste0(rois_fs[i_roi], &quot;_freesurfer_samespace_masked.pdf&quot;)) #pdf_combine(fnames, output = merged_pdf) # pdftools doesn&#39;t work on remote linux? cmd = paste(&quot;pdfunite&quot;, paste(fnames, collapse = &quot; &quot;), merged_pdf) system(cmd) } Subregion masks (MNI) We defined anterior hippocampus using the Harvard-Oxford atlas mask (thresholded at 50% probability), selecting all voxels anterior to MNI y=-21 based on Poppenk et al. (2013). The resulting anterior hippocampus mask was also co-registered to participants’ functional space and intersected with the participant-specific hippocampal mask from FreeSurfer. The mask for the anterior-lateral entorhinal cortex was based on Navarro Schröder et al. (2015). It was co-registered to participants’ functional space and intersected with the entorhinal cortex mask from FreeSurfer. Above, we generated masks of the Hippocampus and Entorhinal Cortex based on the Freesurfer segmentation of each participant’s structural image. Our hypotheses concern the anterior portion of the hippocampus and the anterior-lateral entorhinal subregion, specifically. Hence, we want to split the participant-specific masks. For the hippocampus, we will start out with the probabilistic masks based on the Harvard-Oxford atlas delivered with FSL. Masks for the right and left hippocampus will be combined, thresholded at 50% probability and split in an anterior and posterior section. We will define the anterior hippocampus as all voxels anterior to MNI y = -21 based on Poppenk et al. (TiCS, 2013). They “propose that foci at or anterior to y = -21 mm in MNI space (y = -20 mm in Talairach space) may be regarded as falling in the aHPC, as this coordinate incorporates the uncal apex in the MNI152 template and current neuroanatomical atlases”. We will define the anterolateral entorhinal cortex based on Navarro Schröder et al. (eLife, 2015). Specifically, we will use a mask for the dominant mode of connectivity change within the EC, which is shown in Figure 2 of the original paper. The code below assumes that masks for the left and right hippocampus were extracted from the Harvard-Oxford atlas (MNI space 1mm). Likewise, it assumes the EC subregion masks (MNI space 1mm, obtained from Tobias Navarro Schröder to be included in the folder. After the hippocampus masks are combined, thresholded, and split, the masks will be co-registered to the functional analysis space. # define the name of the ROIs in MNI space rois_hpc_fnames &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, paste0(rois_hpc,&quot;.nii.gz&quot;)) # combine atlas mask across hemispheres and split into anterior &amp; posterior if not done atlas_fnames &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, c(&quot;harvardoxford-subcortical_prob_Left_Hippocampus.nii.gz&quot;, &quot;harvardoxford-subcortical_prob_Right_Hippocampus.nii.gz&quot;, &quot;harvardoxford-hpc_lr.nii.gz&quot;, &quot;harvardoxford-hpc_lr_tresh50_bin.nii.gz&quot;)) # combine the left and right HPC mask from the Harvard-Oxford atlas fsl_add(atlas_fnames[1], atlas_fnames[2],outfile = atlas_fnames[3], retimg = FALSE, verbose = FALSE) # threshold at 50 percent probability fsl_thresh(atlas_fnames[3], outfile = atlas_fnames[4], thresh = 50, opts = &quot;-bin&quot;, retimg = FALSE, verbose = FALSE) # create anterior hippocampus ROI by creating a volume with all voxels anterior # to MNI y=-21 (y=105 in matrix coordinates) and mask this with the binary HPC ROI opts = sprintf(&quot;-mul 0 -add 1 -roi 0 182 105 -1 0 182 0 1 -mas %s&quot;, atlas_fnames[4]) fsl_maths(file = atlas_fnames[4], outfile = rois_hpc_fnames[1], opts = opts, retimg = FALSE, verbose = FALSE) Here are some diagnostic plots for the ROIs that we will use (in MNI 1mm space). # EC ROIs rois_ec_fnames &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, paste0(rois_ec,&quot;.nii.gz&quot;)) # file names for mni ROIs rois_mni_fnames &lt;- c(rois_hpc_fnames, rois_ec_fnames) # load mni 1mm (here we use the ch2better template from MRIcron) #mni_nii &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, &quot;MNI152_T1_1mm_brain.nii.gz&quot;) #%&gt;% readNIfTI2() mni_nii &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, &quot;ch2better_mni1mm.nii.gz&quot;) # register to MNI 1mm space if needed if(!file.exists(mni_nii)){ flirt(infile = here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, &quot;ch2better.nii.gz&quot;), reffile = here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, &quot;MNI152_T1_1mm_brain.nii.gz&quot;), outfile = mni_nii, omat = &quot;ch2better_to_mni1mm&quot;) } mni_nii &lt;- readNIfTI2(mni_nii) for (i_roi in 1:length(rois_mni)){ # load roi mask nifti roi_nii &lt;- readNIfTI2(rois_mni_fnames[i_roi]) # find coordinates for sensible slices to plot coords &lt;- arrayInd(which(as.logical(img_data(roi_nii))), dim(img_data(roi_nii))) coords &lt;- c(min(statip::mfv(coords[,1])), min(statip::mfv(coords[,2])), min(statip::mfv(coords[,3]))) # create a diagnostic plot and save as a PDF fname &lt;- paste0(tools::file_path_sans_ext(rois_mni_fnames[i_roi], compression = &quot;TRUE&quot;),&quot;.pdf&quot;) ortho2(robust_window(mni_nii, probs = c(0, 0.999)), y = roi_nii, col.y = roi_colors[i_roi], xyz = coords, text = rois_mni[i_roi], crosshairs = FALSE) dev.copy(pdf, fname) invisible(dev.off()) } The resulting ROI masks can now be coregistered from MNI 1mm space to the analysis space of the wholebrain functional sequence. Finally, they are thresholded at a probability of 0.5. # name of transformation matrix file to move from highres to functional space standard2func &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;wholebrain&quot;, paste0(&quot;VIRTEM_P&quot;, subjects, &quot;.feat&quot;), &quot;reg&quot;, &quot;standard2example_func.mat&quot;) # use the mean EPI of wholebrain image as a reference mean_epi &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;samespace&quot;, paste0(&quot;VIRTEM_P&quot;, subjects), paste0(&quot;VIRTEM_P&quot;, subjects, &quot;_wholebrain.nii.gz&quot;)) for (i_roi in 1:length(rois_mni)){ #file name of ROI file in standard MNI space roi_mni &lt;- rois_mni_fnames[i_roi] # define output files in samespace (ss) = analysis space based on the wholebrain EPI roi_ss &lt;- file.path(dirs$rois_mni_ss_dirs[i_roi], sprintf(&quot;P%s_%s_ss.nii.gz&quot;, subjects, rois_mni[i_roi])) # apply FSL flirt to move ROI from standard to wholebrain functional space invisible(mapply(flirt_apply, infile = roi_mni, reffile = mean_epi, initmat = standard2func, outfile = roi_ss, verbose = FALSE, retimg = FALSE)) # use fslmaths to binarize the masked ROIs using a threshold of 0.5 out &lt;- mapply(fsl_thresh, file = roi_ss, outfile = roi_ss, thresh = 0.5, opts = &quot;-bin&quot;, verbose = FALSE, retimg = FALSE) } Final ROIs from intersection of masks We defined anterior hippocampus using the Harvard-Oxford atlas mask (thresholded at 50% probability), selecting all voxels anterior to MNI y=-21 based on Poppenk et al. (2013). The resulting anterior hippocampus mask was also co-registered to participants’ functional space and intersected with the participant-specific hippocampal mask from FreeSurfer. The mask for the anterior-lateral entorhinal cortex was based on Navarro Schröder et al. (2015). It was co-registered to participants’ functional space and intersected with the entorhinal cortex mask from FreeSurfer. In the last step, we intersect the masks from FreeSurfer with the respective subregion masks. At this point, both masks are in the functional analysis space and have been binarized. Finally, we generate a PDF of diagnostic plots to check the final ROI masks. for (i_roi in 1:length(rois)){ # define ROI files in samespace (ss) = analysis space based on the wholebrain EPI roi_ss &lt;- file.path(dirs$rois_ss_dirs[i_roi], sprintf(&quot;P%s_%s_ss.nii.gz&quot;, subjects, rois[i_roi])) # define file names for ROI files masked with freesurfer (i.e. our output) roi_ss_fs &lt;- file.path(dirs$rois_ss_dirs[i_roi], sprintf(&quot;P%s_%s_ss_fs.nii.gz&quot;, subjects, rois[i_roi])) # use HPC or EC mask from freesurfer? if not HPC or EC use graymatter mask if(grepl(&quot;HPC&quot;, rois[i_roi])){ fs_roi &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;hpc_lr&quot;, &quot;freesurfer_samespace&quot;, #sprintf(&quot;P%s_%s_fs_ss_masked_bin.nii.gz&quot;,subjects, &quot;hpc_lr&quot;)) sprintf(&quot;P%s_%s_fs_ss_masked.nii.gz&quot;,subjects, &quot;hpc_lr&quot;)) } else if (grepl(&quot;EC&quot;, rois[i_roi])){ fs_roi &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;ec_lr&quot;, &quot;freesurfer_samespace&quot;, #sprintf(&quot;P%s_%s_fs_ss_masked_bin.nii.gz&quot;,subjects, &quot;ec_lr&quot;)) sprintf(&quot;P%s_%s_fs_ss_masked.nii.gz&quot;,subjects, &quot;ec_lr&quot;)) } else {#/data/pt_02261/virtem/data/mri/processed/samespace/VIRTEM_P035/VIRTEM_P035_common_graymatter_mask.nii.gz # mask gray matter mask with brain mask to account for partial FOV. Result will be used to mask ROIs gm_mask_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;samespace&quot;, paste0(&quot;VIRTEM_P&quot;, subjects), sprintf(&quot;VIRTEM_P%s_common_graymatter_mask.nii.gz&quot;,subjects)) brain_mask_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;samespace&quot;, paste0(&quot;VIRTEM_P&quot;, subjects), sprintf(&quot;VIRTEM_P%s_common_mask_perBlock.nii.gz&quot;,subjects)) fs_roi &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;samespace&quot;, paste0(&quot;VIRTEM_P&quot;, subjects), sprintf(&quot;VIRTEM_P%s_common_graymatter_mask_brainmasked.nii.gz&quot;,subjects)) invisible(mapply(fsl_mask, file = gm_mask_fn, mask = brain_mask_fn, outfile = fs_roi, verbose = FALSE, retimg = FALSE)) } #} else {stop(&quot;Don&#39;t know which Freesurfer mask to use!&quot;)} # mask the subregion mask with the Freesufer ROI invisible(mapply(fsl_maths, file = roi_ss, opts = sprintf(&quot;-min %s&quot;, fs_roi), outfile = roi_ss_fs, verbose = FALSE, retimg = FALSE)) #invisible(mapply(fsl_mask, file = roi_ss, mask = fs_roi, outfile = roi_ss_fs, # verbose = FALSE, retimg = FALSE)) # threshold the resulting mask at a probability of 0.5 and binarize it invisible(mapply(fsl_thresh, file = roi_ss_fs, outfile = roi_ss_fs, thresh = 0.500000000000000000000000000000000000001, opts = &quot;-bin&quot;, verbose = FALSE, retimg = FALSE)) } 6.2 Group-Level Masks Lastly, we create some group-level masks for visualization and further analysis: probabilistic ROI image for visualization of ROIs a field of view mask in MNI space a gray matter mask in MNI space a small volume correction mask We create these images in their respective folders, but also copy them to the analysis data folder so they can be shared. # create the group mask folder if (!dir.exists(file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;))){ dir.create(file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;))} # copy MNI image there file.copy(mni_fname(mm=1, brain = TRUE), file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;), overwrite=TRUE) ## [1] TRUE Probabilistic ROIs for visualization For later visualization we move the masks to MNI space and threshold at 0.5. We then add the images together and divide by the number of subjects to obtain an image giving us the probablity of each voxel to be included in the ROI. Visualizations of the ROIs will be created before implementing ROI-based RSA. # name of transformation matrix file to move from common functional to MNI 1mm space func2standard &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;wholebrain&quot;, paste0(&quot;VIRTEM_P&quot;, subjects, &quot;.feat&quot;), &quot;reg&quot;, &quot;example_func2standard.mat&quot;) # use MNI 1mm image as a reference mni_1mm &lt;- mni_fname(mm=1) for (i_roi in 1:length(rois)){ # where to put the output out_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, rois[i_roi], &quot;mni_1mm&quot;) if (!dir.exists(out_dir)){dir.create(out_dir)} # define input files in samespace (ss) = analysis space based on the wholebrain EPI roi_ss &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, rois[i_roi], &quot;samespace&quot;, sprintf(&quot;P%s_%s_ss.nii.gz&quot;, subjects, rois[i_roi])) #file name of output ROI file in standard MNI space roi_mni &lt;- file.path(out_dir, sprintf(&quot;P%s_%s_mni1mm.nii.gz&quot;, subjects, rois[i_roi])) # apply FSL flirt to move ROI from wholebrain functional space to MNI space invisible(mapply(flirt_apply, infile = roi_ss, reffile = mni_1mm, initmat = func2standard, outfile = roi_mni, verbose = FALSE, retimg = FALSE)) # use fslmaths to binarize the ROI using a threshold of 0.5 out &lt;- mapply(fsl_thresh, file = roi_mni, outfile = roi_mni, thresh = 0.5, opts = &quot;-bin&quot;, verbose = FALSE, retimg = FALSE) # create summary image out_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, rois[i_roi], sprintf(&quot;%s_group_prob_mni1mm.nii.gz&quot;,rois[i_roi])) fslmaths(file = roi_mni, outfile = out_fn, opts = sprintf(&quot;-add %s&quot;, paste0(roi_mni[2:length(roi_mni)])), verbose=FALSE) fslmaths(file = out_fn, outfile = out_fn, opts = sprintf(&quot;-div %d&quot;, length(roi_mni)), verbose=FALSE) # copy to data sharing folder file.copy(from = out_fn, to = file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;), overwrite = TRUE) } Field of view mask for visualization Next, we create a mask of our field of view, i.e. voxels covered in our functional images. We do so by registering the subject-specific brain masks from FEAT (in whole-brain functional space, already combined across blocks) to MNI space. After thresholding, the result is a binary mask to illustrate the FOV when plotting brain images in the main analysis script. # folder for output if(!dir.exists(file.path(dirs$mask_dir, &quot;fov&quot;))){ dir.create(file.path(dirs$mask_dir, &quot;fov&quot;), recursive=TRUE)} # name of transformation matrix file to move from the shared functional space to MNI 1mm func2standard &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;wholebrain&quot;, paste0(&quot;VIRTEM_P&quot;, subjects, &quot;.feat&quot;), &quot;reg&quot;, &quot;example_func2standard.mat&quot;) # subject-specific brain (FOV) masks in whole-brain functional space subj_masks &lt;- file.path(dirs$samespace_dir, sprintf(&quot;VIRTEM_P%s&quot;,subjects), sprintf(&quot;VIRTEM_P%s_common_mask_perBlock.nii.gz&quot;,subjects)) # output file name after moving to MNI space subj_masks_mni &lt;- file.path(file.path(dirs$mask_dir, &quot;fov&quot;), sprintf(&quot;%s_fov_mask_mni.nii.gz&quot;, subjects)) # apply FSL flirt to move from wholebrain functional space to 1mm MNI space invisible(mapply(flirt_apply, infile = subj_masks, reffile = mni_fname(&quot;1&quot;), initmat = func2standard, outfile = subj_masks_mni, verbose = FALSE, retimg = FALSE)) # merge the files into a 4D file mask_4d &lt;- file.path(dirs$mask_dir, &quot;fov&quot;, &quot;4d_fov_mask_mni.nii.gz&quot;) fsl_merge(infiles = subj_masks_mni, direction = &quot;t&quot;, outfile = mask_4d, retimg = FALSE, verbose = FALSE) # create binary mask fov_mask &lt;- file.path(dirs$mask_dir, &quot;fov&quot;, &quot;fov_mask_mni.nii.gz&quot;) fov_mask_nii &lt;- fslmaths(mask_4d, outfile = fov_mask, opts = &quot;-thr 0.3 -bin -Tmean -thr 0.3 -bin&quot;, verbose = FALSE, retimg = TRUE) # quick plot ortho2(fov_mask_nii, xyz=c(100,100,50)) # copy to data sharing folder file.copy(from = fov_mask, to = file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;)) Gray matter mask From the subject-specific gray matter masks (in whole-brain functional space) we create a merged, binary mask in MNI space. We use this to run FSL Randomise for all gray matter voxels (we only use gray matter voxels as features for our searchlight analysis). Gray matter segmentation was done on the structural images and the results were mapped back to the space of the whole-brain functional scan for later use in the analysis. # folder for output if(!dir.exists(file.path(dirs$mask_dir, &quot;gray_matter&quot;))){ dir.create(file.path(dirs$mask_dir, &quot;gray_matter&quot;), recursive=TRUE)} # name of transformation matrix file to move from the shared functional space to MNI 1mm func2standard &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;wholebrain&quot;, paste0(&quot;VIRTEM_P&quot;, subjects, &quot;.feat&quot;), &quot;reg&quot;, &quot;example_func2standard.mat&quot;) # subject-specific gray matter masks in whole-brain functional space subj_masks &lt;- file.path(dirs$samespace_dir, sprintf(&quot;VIRTEM_P%s&quot;,subjects), sprintf(&quot;VIRTEM_P%s_common_graymatter_mask_brainmasked.nii.gz&quot;, subjects)) # output file name in MNI space subj_masks_mni &lt;- file.path(dirs$mask_dir, &quot;gray_matter&quot;, sprintf(&quot;%s_graymatter_mni.nii.gz&quot;, subjects)) # apply FSL flirt to move from wholebrain functional space to 1mm MNI space invisible(mapply(flirt_apply, infile = subj_masks, reffile = mni_fname(&quot;1&quot;), initmat = func2standard, outfile = subj_masks_mni, verbose = FALSE, retimg = FALSE)) # merge the files into a 4D file gm_4d &lt;- file.path(dirs$mask_dir, &quot;gray_matter&quot;, &quot;4d_graymatter_mni.nii.gz&quot;) fsl_merge(infiles = subj_masks_mni, direction = &quot;t&quot;, outfile = gm_4d, retimg = FALSE, verbose = FALSE) # create binary gray matter mask by thresholding and combining across subjects liberally mni_brain_mask &lt;- mni_fname(mm = &quot;1&quot;, brain=TRUE, mask=TRUE) gm_mask &lt;- file.path(dirs$mask_dir, &quot;gray_matter&quot;, &quot;gray_matter_mask.nii.gz&quot;) gm_mask_nii &lt;- fslmaths(gm_4d, outfile = gm_mask, opts = sprintf(&quot;-thr 0.3 -bin -Tmean -thr 0.3 -bin -mas %s&quot;, mni_brain_mask), verbose = FALSE, retimg = TRUE) Small Volume Correction Mask We corrected for multiple comparisons using a small volume correction mask including our a priori regions of interest, the anterior hippocampus and the anterior-lateral entorhinal cortex. The small volume correction mask consists of our a priori ROIs, the anterior hippocampus and the anterior-lateral entorhinal cortex. We use the subject-specific masks, moved back to MNI space for this. # folder for output if(!dir.exists(file.path(dirs$mask_dir, &quot;svc&quot;))){ dir.create(file.path(dirs$mask_dir, &quot;svc&quot;), recursive=TRUE)} # create the SVC mask by merging aHPC and alEC ROIs aHPC_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;aHPC_lr&quot;, sprintf(&quot;%s_group_prob_mni1mm.nii.gz&quot;, &quot;aHPC_lr&quot;)) alEC_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;alEC_lr&quot;, sprintf(&quot;%s_group_prob_mni1mm.nii.gz&quot;, &quot;alEC_lr&quot;)) svc_mask_fn &lt;- file.path(dirs$mask_dir, &quot;svc&quot;, &quot;svc_mask.nii.gz&quot;) fsl_add(file = aHPC_fn, file2 = alEC_fn, outfile = svc_mask_fn, retimg = FALSE) svc_nii &lt;- fslmaths(file = svc_mask_fn, outfile = svc_mask_fn, opts = sprintf(&quot;-thr 0.99 -bin -mas %s&quot;, gm_mask), retimg = TRUE) # let&#39;s have a look at the mask we created mni_nii &lt;- readNIfTI2(mni_fname(&quot;1&quot;)) ortho2(mni_nii, y = svc_nii, xyz = c(63, 113, 50)) ortho2(mni_nii, y = svc_nii, xyz = c(71, 127, 39)) 6.3 Quantify Representational Change Get multi-voxel patterns from ROIs We want to run RSA on the multi-voxel patterns corresponding to the object presentations during the picture viewing tasks. For this, we use the data as it was preprocessed by Lorena Deuker. We will do some further cleaning and extract the relevant volumes from the time series to eventually run RSA. Preprocessing Preprocessing was performed using FSL FEAT (version 6.00). Functional scans from the picture viewing tasks and the whole-brain functional scan were submitted to motion correction and high-pass filtering using FSL FEAT. For the two picture viewing tasks, data from each mini-block was preprocessed independently. For those participants with a field map scan, distortion correction was applied to the functional data sets. No spatial smoothing was performed. Functional images from the two picture viewing tasks were then registered to the preprocessed mean image of the whole-brain functional scan. The whole-brain functional images were registered to the individual structural scans. The structural scans were in turn normalized to the MNI template (1-mm resolution). Gray matter segmentation was done on the structural images, and the results were mapped back to the space of the whole-brain functional scan for later use in the analysis. Extract ROI time series and calculate residuals from motion parameter regression In this first section of the script we will create a dataframe that includes the relevant file names for the files that are needed to extract the clean time series. These files include the motion parameters from FSL FEAT, which was run on the functional data from the picture viewing task. As described above, these data were split into 10 blocks. We will use the preprocessed FEAT output images which were already co-registered to the analysis space (‘samespace’). Further, we will use a mask to only include voxels with data in all blocks. This mask has already been created and is available in the analysis space. Additionally, we will use the graymatter mask and ROI masks(both already co-registered to the analysis space). Representational similarity analysis (RSA) (Kriegeskorte et al., 2008) was first implemented separately for the pre- and post-learning picture viewing task. It was carried out in ROIs co-registered to the whole-brain functional image and in searchlight analyses (see below). For the ROI analyses, preprocessed data were intersected with the participant-specific anterior hippocampus and anterolateral entorhinal cortex ROI masks as well as a brain mask obtained during preprocessing (only voxels within the brain mask in all mini-blocks were analyzed) and the gray matter mask. For each voxel within the ROI mask, motion parameters from FSL MCFLIRT were used as predictors in a general linear model (GLM) with the voxel time series as the dependent variable. The residuals of this GLM (i.e. data that could not be explained by motion) were taken to the next analysis step. Build data frame with files and folders: #PREPARE DATA FRAME FOR CLEANING # create a tibble with filenames etc for data cleaning, start with subject IDs func_dat_df &lt;- tibble(subject = rep(as.numeric(subjects), each = n_runs * n_blocks)) # add run info (run 1 and 2 equal the pre and post PVT) func_dat_df &lt;- add_column(func_dat_df, run = rep(rep(1:n_runs, each = n_blocks), n_subs)) # add block information (blocks 1 to 10 are the PVT blocks preprocessed separately) func_dat_df &lt;- add_column(func_dat_df, block = rep(1:n_blocks, n_subs*n_runs)) # add the motion parameter file func_dat_df &lt;- add_column(func_dat_df, mc_par_fn = file.path(dirs$feat_dir, paste0(&quot;VIRTEM_P0&quot;, func_dat_df$subject), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, func_dat_df$run, func_dat_df$block), &quot;mc&quot;, &quot;prefiltered_func_data_mcf.par&quot;)) # add the functional data file func_dat_df &lt;- add_column(func_dat_df, filtered_func = file.path(dirs$samespace_dir, paste0(&quot;VIRTEM_P0&quot;, func_dat_df$subject), sprintf(&quot;VIRTEM_P%03d_RSA_%02d_Block%02d_func.nii.gz&quot;, func_dat_df$subject, func_dat_df$run, func_dat_df$block))) # add the brain mask data file func_dat_df &lt;- add_column(func_dat_df, brain_mask = file.path(dirs$samespace_dir, paste0(&quot;VIRTEM_P0&quot;, func_dat_df$subject), sprintf(&quot;VIRTEM_P%03d_common_mask_perBlock.nii.gz&quot;, func_dat_df$subject))) # add the graymatter mask file func_dat_df &lt;- add_column(func_dat_df, gray_mask = file.path(dirs$samespace_dir, paste0(&quot;VIRTEM_P0&quot;, func_dat_df$subject), sprintf(&quot;VIRTEM_P%03d_common_graymatter_mask.nii.gz&quot;, func_dat_df$subject))) # add output directory func_dat_df &lt;- add_column(func_dat_df, out_dir = list(dirs$rsa_roi_clean_timeseries_dirs)) # add list of ROIs func_dat_df &lt;- add_column(func_dat_df, rois = list(rois)) # add list of ROI file names func_dat_df &lt;- add_column(func_dat_df, roi_dirs = list(dirs$rois_ss_dirs)) Define the function used to clean functional data In this next section, we will first define a function and then run it on all datasets. In this function, the preprocessed data will be loaded and transformed to matrix format. A combined mask is generated from the respective ROI mask and the graymatter and brain masks. For every voxel within this mask, movement correction parameters are used as predictors in a GLM with the voxel time series as dependent variable. Finally, the residuals from this GLM (i.e. what could not be explained by motion) will be written to a text file. # DEFINE THE FUNCTION TO GET THE RESIDUAL TIME SERIES run_motion_glm &lt;- function(df = func_dat_df[1,]){ # load the motion params and convert to tibble mc_pars &lt;- read.table(df$mc_par_fn, header = FALSE) mp_df &lt;- as_tibble(mc_pars) colnames(mp_df) &lt;- paste0(&quot;mp&quot;, 1:6) # load the brain mask and linearize it brain_mask_nii &lt;- readNIfTI2(df$brain_mask) brain_mask_lin &lt;- c(brain_mask_nii) assertthat::assert_that(all.equal(unique(brain_mask_lin), c(0,1))) # load the graymatter mask and linearize it and threshold it at 0.7 gray_mask_nii &lt;- readNIfTI2(df$gray_mask) gray_mask_lin &lt;- c(gray_mask_nii) &gt; 0.7 # load the functional data func_nii &lt;- readNIfTI2(df$filtered_func) # create matrix from the functional data with voxels in rows and volumes in columns n_vols &lt;- dim(func_nii)[4] func_dat_mat &lt;- matrix(nrow = prod(dim(func_nii)[1:3]), ncol = n_vols) for(i_vol in 1:n_vols){ func_dat_mat[,i_vol] &lt;- c(func_nii[,,,i_vol]) } # define the roi files roi_files &lt;- file.path(df$roi_dirs[[1]], sprintf(&quot;P%03d_%s_ss_fs.nii.gz&quot;, df$subject, df$rois[[1]])) # loop over the ROIs for(i_roi in 1:length(roi_files)){ # load the current ROI and linearize roi_nii &lt;- readNIfTI2(roi_files[i_roi]) roi_lin &lt;- c(roi_nii) # make sure masks and functional data have the same number of voxels assertthat::assert_that(length(roi_lin) == length(gray_mask_lin)) assertthat::assert_that(length(roi_lin) == length(brain_mask_lin)) assertthat::assert_that(length(roi_lin) == dim(func_dat_mat)[1]) assertthat::assert_that(n_vols == nrow(mc_pars)) # create a mask combining the ROI, the graymatter, and the functional brain mask comb_mask &lt;- as.logical(roi_lin) #&amp; gray_mask_lin &amp; as.logical(brain_mask_lin) # run the GLM for each voxel in the combined mask roi_dat &lt;- func_dat_mat[comb_mask,] # initialize output for cleaned timeseries and then loop over all voxels roi_dat_clean &lt;- matrix(nrow = sum(comb_mask), ncol = n_vols) for(i_vox in 1:sum(comb_mask)){ # extract this voxel&#39;s data and merge with motion params vox_dat &lt;- roi_dat[i_vox,] vox_df &lt;- tibble(vox_dat) vox_df &lt;- cbind(vox_df, mp_df) # run the glm and store the residuals roi_dat_clean[i_vox,] &lt;- resid(glm(&#39;vox_dat~mp1+mp2+mp3+mp4+mp5+mp6&#39;, data = vox_df)) } # write clean timeseries data of this ROI to file fn &lt;- sprintf(&quot;%03d_run%02d_block%02d_%s_clean_timeseries.txt&quot;, df$subject, df$run, df$block, df$rois[[1]][i_roi]) out_dir_sub &lt;- file.path(df$out_dir[[1]][i_roi], sprintf(&quot;%03d&quot;,df$subject)) if(!dir.exists(out_dir_sub)){dir.create(out_dir_sub)} write.table(roi_dat_clean, file.path(out_dir_sub, fn), append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = FALSE) } } Apply cleaning function Now we are ready to apply the cleaning function to all blocks of all participants. Typically, this should be run in parallel to save time. # next step depends on whether we are in parallel or serial mode if (!run_parallel){ # run serially # run the function for each row of the data frame, # i.e. for each block in each run for each subject for(i in 1:nrow(func_dat_df)){ tic(sprintf(&quot;Subject %s, run %d, block %d&quot;, func_dat_df$subject[i], func_dat_df$run[i], func_dat_df$block[i])) run_motion_glm(df = func_dat_df[i,]) toc() } } else if (run_parallel){ # run in parallel, assumes CBS HTCondor is available # expand the data frame and write to file func_dat_df_long &lt;- unnest(func_dat_df, cols = c(rois, out_dir, roi_dirs)) fn &lt;- file.path(here(&quot;data&quot;,&quot;mri&quot;, &quot;rsa&quot;, &quot;clean_roi_timeseries&quot;), &quot;htc_config_clean_time_series.txt&quot;) fn_def &lt;- cat(sprintf(&#39;&quot;fn &lt;- &quot;%s&quot;&#39;,fn)) write.table(func_dat_df_long, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) # store the function definition as text func_def &lt;- capture.output(run_motion_glm) func_def[1] &lt;- paste0(&quot;run_motion_glm &lt;- &quot;,func_def[1]) #func_def &lt;- func_def[-length(func_def)] #write the Rscript that we want to run rscript_fn &lt;- here(&quot;data&quot;,&quot;mri&quot;, &quot;rsa&quot;, &quot;clean_roi_timeseries&quot;, &quot;run_clean_timeseries.R&quot;) con &lt;- file(rscript_fn) open(con, &quot;w&quot;) writeLines(c( &quot;\\n# handle input&quot;, &quot;args = commandArgs()&quot;, &quot;i &lt;- as.numeric(args[length(args)])&quot;, &quot;\\n#load required packages&quot;, noquote(sprintf(&#39;lib_dir &lt;- &quot;%s&quot;&#39;,&quot;/data/pt_02261/virtem/virtem_code/R3.6.1/library/Linux&quot;)), &#39;.libPaths(c(lib_dir,.libPaths()))&#39;, &#39;lapply(c(&quot;oro.nifti&quot;, &quot;assertthat&quot;, &quot;dplyr&quot;, &quot;neurobase&quot;), library, character.only = TRUE)&#39;, &quot;\\n# read the data and transform ROI column back to list&quot;, noquote(sprintf(&#39;fn &lt;- &quot;%s&quot;&#39;,fn)), &#39;func_dat_df &lt;- read.table(fn, sep = &quot;,&quot;, header = TRUE, stringsAsFactors = FALSE)&#39;, &quot;\\n#define the function to run motion GLM&quot;, func_def, &quot;\\n# run the function on one line of the data frame&quot;, &quot;run_motion_glm(df = func_dat_df[i,])&quot;),con) close(con) # folder for condor output htc_dir &lt;- here(&quot;htc_logs&quot;, &quot;clean_timeseries&quot;) if(!exists(htc_dir)){dir.create(htc_dir, recursive = TRUE)} # write the submit script fn &lt;- here(&quot;data&quot;,&quot;mri&quot;, &quot;rsa&quot;, &quot;clean_roi_timeseries&quot;, &quot;run_clean_timeseries.submit&quot;) con &lt;- file(fn) open(con, &quot;w&quot;) writeLines(c( &quot;universe = vanilla&quot;, &quot;executable = /afs/cbs.mpg.de/software/scripts/envwrap&quot;, &quot;request_memory = 9000&quot;, &quot;notification = error&quot; ),con) for (i in 1:nrow(func_dat_df_long)){ writeLines(c( sprintf(&quot;\\narguments = R+ --version 3.6.1 Rscript %s %d&quot;, rscript_fn, i), sprintf(&quot;log = %s/%d.log&quot;, htc_dir, i), sprintf(&quot;output = %s/%d.out&quot;, htc_dir, i), sprintf(&quot;error = %s/%d.err&quot;, htc_dir, i), sprintf(&quot;Queue\\n&quot;)),con) } close(con) # submit to condor #system(&quot;reset-memory-max&quot;) batch_id &lt;- system(paste(&quot;condor_submit&quot;, fn), intern = TRUE) batch_id &lt;- regmatches(batch_id[2], gregexpr(&quot;[[:digit:]]+&quot;, batch_id[2]))[[1]][2] #system(&quot;reset-memory-max&quot;) sprintf(&quot;submitted jobs (ID = %s) to clean time series in ROIs. Time to wait...&quot;, batch_id) pause_until_batch_done(batch_id = batch_id, wait_interval = 300) } Extract relevant volumes As the presentation of images in the PVT pre and post blocks was locked to the onset of a new volume (see above), the second volume after image onset was selected for every trial (effectively covering the time between 2270-4540 ms after stimulus onset). In this step, we split the presentation logfile from the picture viewing task into the 10 blocks. We reference the volume count to the first volume of each block and extract the relevant volumes from the cleaned ROI timeseries data, accounting for the temporal offset. This is done for each ROI and each block separately in both the pre and post runs. The data are written to file. These output files include the multi-voxel patterns (rows) for each image (columns) that was presented during the picture viewing tasks, including the catch images. The columns are sorted based on the presentation order during the picture viewing task. As the presentation of images in the picture viewing tasks was locked to the onset of a new volume (see above), the second volume after image onset was selected for every trial, effectively covering the time between 2270 and 4540 ms after stimulus onset. offset_tr = 2 for (i_sub in subjects){ for (i_run in 1:n_runs){ # load the logfile for this run (pre or post) log_fn &lt;- file.path(dirs$pvt_log_dir, sprintf(&#39;P%s_%svirtem.txt&#39;, i_sub, runs[i_run])) log &lt;- read.table(log_fn) colnames(log) &lt;- c(&quot;pic&quot;, &quot;fix_start&quot;, &quot;pic_start&quot;, &quot;volume&quot;, &quot;response&quot;, &quot;RT&quot;, &quot;trial_end&quot;) # split the log file into the 10 blocks log_split &lt;- split(log, rep(1:10, each = 21)) # reference volume numbers to the first volume of that block vol_block &lt;- lapply(log_split, function(x){x$volume - x$volume[1]}) log_split &lt;- mapply(cbind, log_split, &quot;vol_block&quot;=vol_block, SIMPLIFY=FALSE) for (i_roi in rois){ for (i_block in 1:10){ # load the ROI timeseries fn &lt;- sprintf(&quot;%s_run%02d_block%02d_%s_clean_timeseries.txt&quot;, i_sub, i_run, i_block, i_roi) dir &lt;- file.path(dirs$rsa_dir, &quot;clean_roi_timeseries&quot;, i_roi, i_sub) roi_dat &lt;- read.table(file.path(dir, fn), sep = &quot;,&quot;, dec = &quot;.&quot;) # index of relevant volumes when accounting for offset rel_vols &lt;- log_split[[i_block]]$vol_block + offset_tr # extract the relevant volumes from the ROI timeseries data if(i_block == 1){rel_dat &lt;- roi_dat[,rel_vols]} else{ rel_dat &lt;- cbind(rel_dat, roi_dat[,rel_vols])} } # save the relevant data out_dir &lt;- file.path(dirs$rsa_dir, &quot;relevant_roi_volumes&quot;, i_roi) if(!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} fn &lt;- sprintf(&quot;%s_%s_%s_relevant_volumes.txt&quot;, i_sub, i_roi, runs[i_run]) write.table(rel_dat, file.path(out_dir, fn), append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = FALSE) } } } Calculate correlation matrices Only data for the 20 event images that were shown in the learning task were analyzed; data for the target stimulus were discarded. The similarity between the multi-voxel activity pattern for every event image in every mini-block with the pattern of every other event in every other mini-block was quantified using Pearson correlation coefficients. Thus, comparisons of scenes from the same mini-block were excluded. Next, we calculated mean, Fisher z-transformed correlation coefficients for every pair of events, yielding separate matrices of pattern similarity estimates for the pre- and the post-learning picture viewing tasks (Figure 3). Next, we need to calculate pair-wise correlations between the multi-voxel patterns from the picture viewing tasks. We will restrict the analysis to the 20 scenes during the learning task and discard the data for the target scene. The correlation matrix will first be calculated for trial-by-trial comparisons of multi-voxel patterns. These will then be averaged excluding comparisons of patterns from the same block. The resulting correlation matrices that are saved are ordered according to picture identity (i.e. picture 1-20 x picture 1-20). # for all 10x10 comparisons we will be averaging all comparisons apart from the diagonal # to exclude same_block comparisons no_diag &lt;- matrix(data = TRUE, nrow=10, ncol = 10) diag(no_diag)&lt;- FALSE for (i_sub in subjects){ for (i_run in 1:n_runs){ for(i_roi in 1:length(rois)){ # load the logfile for this run (pre or post) (Note to self: moved here 6 March when hunting ghosts) log_fn &lt;- file.path(dirs$pvt_log_dir, sprintf(&#39;P%s_%svirtem.txt&#39;, i_sub, runs[i_run])) log &lt;- read.table(log_fn) colnames(log) &lt;- c(&quot;pic&quot;, &quot;fix_start&quot;, &quot;pic_start&quot;, &quot;volume&quot;, &quot;response&quot;, &quot;RT&quot;, &quot;trial_end&quot;) # load the relevant MRI volumes (i.e. multi-voxel patterns) fn &lt;- file.path(dirs$rsa_roi_rel_vol_dirs[i_roi], sprintf(&quot;%s_%s_%s_relevant_volumes.txt&quot;, i_sub, rois[i_roi], runs[i_run])) rel_dat &lt;- read.table(fn, sep = &quot;,&quot;, dec = &quot;.&quot;) # remove patterns &amp; log entries corresponding to the target picture (catch trials) rel_dat &lt;- rel_dat[, log$pic != 21] log &lt;- log[log$pic != 21,] # order the data according to picture identity rel_dat &lt;- rel_dat[,order(log$pic)] colnames(rel_dat) &lt;- log$pic[order(log$pic)] # write to file for diagnostic purposes fn &lt;- file.path(dirs$rsa_roi_rel_vol_dirs[i_roi], sprintf(&quot;%s_%s_%s_relevant_volumes_ordered.txt&quot;, i_sub, rois[i_roi], runs[i_run])) write.table(rel_dat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = FALSE) # calculate the correlation matrix (trial by trial correlations at this point) corr_mat_trial &lt;- cor(rel_dat) # save the correlation matrix fn &lt;- file.path(dirs$rsa_roi_corr_mat_dirs[i_roi], sprintf(&quot;%s_%s_%s_corr_mat_trial.txt&quot;, i_sub, rois[i_roi], runs[i_run])) write.table(corr_mat_trial, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = FALSE) # initialize correlation matrix condition by condition corr_mat &lt;- matrix(nrow = 20, ncol = 20, dimnames = c(list(1:20), list(1:20))) # loop over all picture comparisons for(i_pic1 in 1:20){ for(i_pic2 in 1:20){ # extract the current 10x10 correlation matrix i1 &lt;- (1+(i_pic1-1)*10):(i_pic1*10) i2 &lt;- (1+(i_pic2-1)*10):(i_pic2*10) curr_mat &lt;- corr_mat_trial[i1, i2] # average the corrlations while excluding diagonal (same block comparisons) corr_mat[i_pic1, i_pic2] &lt;- mean(curr_mat[no_diag]) } } # diagnostic plot #corrplot(corr_mat, method = &quot;color&quot;, is.corr=FALSE, # cl.lim = c(min(corr_mat),max(corr_mat)), addgrid.col = NA) # save the correlation matrix fn &lt;- file.path(dirs$rsa_roi_corr_mat_dirs[i_roi], sprintf(&quot;%s_%s_%s_corr_mat.txt&quot;, i_sub, rois[i_roi], runs[i_run])) write.table(corr_mat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = FALSE) } } } Calculate Pattern Similarity Change In the next step, we calculate how the correlation between patterns change from the first to the second picture viewing task, i.e. through the day learning task. To do this, we load both correlation matrices and reduce them to the upper triangle, excluding the diagonal. Then, the correlations are Fisher Z-transformed and the similarity values from the pre-learning picture viewing task are subtracted from those of the post-learning picture viewing task to isolate pattern similarity change. The correlations and their changes are then saved together with information about the pictures that were compared. In order to assess changes in representational similarity due to the day learning task, which took place in between the two picture viewing tasks, we quantified pattern similarity changes as the difference of the respective correlation coefficients for every pair of events between the post-learning picture viewing task and its pre-learning baseline equivalent (Figure 3). for(i_sub in subjects){ for(i_roi in 1:length(rois)){ # load pre correlation matrix fn &lt;- file.path(dirs$rsa_roi_corr_mat_dirs[i_roi], sprintf(&quot;%s_%s_pre_corr_mat.txt&quot;, i_sub, rois[i_roi])) corr_mat_pre &lt;- read.table(fn, sep = &quot;,&quot;, dec = &quot;.&quot;) # load post correlation matrix fn &lt;- file.path(dirs$rsa_roi_corr_mat_dirs[i_roi], sprintf(&quot;%s_%s_post_corr_mat.txt&quot;, i_sub, rois[i_roi])) corr_mat_post &lt;- read.table(fn, sep = &quot;,&quot;, dec = &quot;.&quot;) # reduce to upper triangle of correlations (without diagonal) pre_corrs &lt;- corr_mat_pre[upper.tri(corr_mat_pre, diag = FALSE)] post_corrs &lt;- corr_mat_post[upper.tri(corr_mat_post, diag = FALSE)] # create a tibble with the correlations pics &lt;- which(upper.tri(corr_mat_post), arr.ind=TRUE) corr_df &lt;- tibble(pic1 = pics[,1], pic2 = pics[,2], pre_corrs, post_corrs) # Fisher Z transform correlations and calculate pattern similarity change # by subtracting the pre-correlations from the post-correlations corr_df$ps_change &lt;- FisherZ(corr_df$post_corrs) - FisherZ(corr_df$pre_corrs) # write to file fn &lt;- file.path(dirs$rsa_roi_ps_change_dirs[i_roi], sprintf(&quot;%s_%s_pattern_similarity_change.txt&quot;, i_sub, rois[i_roi])) write.table(corr_df, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) } } 6.4 Combine behavioral and fMRI data for RSA To create input for the RSA the next step is to combine the similarity change data with the behavioral data, so that we can do meaningful analyses. First the behavioral data is loaded and brought into the same pair-wise format as the similarity change data. Both datasets are combined for each subject and then written to disk. for (i_sub in subjects){ # load the behavioral data fn &lt;- file.path(dirs$timeline_dat_dir, sprintf(&quot;%s_behavior_tbl_timeline.txt&quot;, i_sub)) col_types_list &lt;- cols_only(sub_id = col_character(), day = col_factor(), event = col_integer(), pic = col_integer(), virtual_time = col_double(), real_time = col_double(), memory_time = col_double(), memory_order = col_double(), sorted_day = col_integer()) beh_dat &lt;- read_csv(fn, col_types = col_types_list) # sort behavioral data according to picture identity beh_dat_ordered &lt;- beh_dat[order(beh_dat$pic),] # find the order of comparisons pairs &lt;- which(upper.tri(matrix(nrow = 20, ncol = 20)), arr.ind=TRUE) # extract the data for the first and second picture in each pair from the behavioral data pic1_dat &lt;- beh_dat_ordered[pairs[,1],] pic2_dat &lt;- beh_dat_ordered[pairs[,2],] colnames(pic1_dat) &lt;- paste0(colnames(pic1_dat), &quot;1&quot;) colnames(pic2_dat) &lt;- paste0(colnames(pic2_dat), &quot;2&quot;) # combine the two tibbles pair_dat &lt;- cbind(pic1_dat, pic2_dat) # reorder the tibble columns to make a bit more sense pair_dat &lt;- pair_dat %&gt;% select(sub_id1, day1, day2, pic1, pic2, event1, event2, virtual_time1, virtual_time2, real_time1, real_time2, memory_order1, memory_order2, memory_time1, memory_time2, sorted_day1, sorted_day2) %&gt;% rename(sub_id = sub_id1) rsa_dat &lt;- tibble() for (i_roi in 1:length(rois)){ # load the pattern similarity change data for this ROI fn &lt;- file.path(dirs$rsa_roi_ps_change_dirs[i_roi], sprintf(&quot;%s_%s_pattern_similarity_change.txt&quot;, i_sub, rois[i_roi])) ps_change_dat &lt;- read.csv(fn) # make sure files have the same order assertthat::are_equal(c(pair_dat$pic1, pair_dat$pic2), c(ps_change_dat$pic1, ps_change_dat$pic2)) # add column with ROI name ps_change_dat &lt;- add_column(ps_change_dat, roi = rois[i_roi]) # collect the data from this ROI and merge into long data frame roi_dat &lt;- cbind(pair_dat, ps_change_dat[,3:6]) rsa_dat &lt;- rbind(rsa_dat, roi_dat) } # write to file fn &lt;- file.path(dirs$rsa_dat_dir, sprintf(&quot;%s_data_for_rsa.txt&quot;,i_sub)) write.table(rsa_dat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) } Finally, the datasets are combined across subjects. # set up a dataframe to collect the data rsa_dat = tibble() for (i_sub in subjects){ # load data from CSV fn &lt;- file.path(dirs$rsa_dat_dir, sprintf(&quot;%s_data_for_rsa.txt&quot;,i_sub)) col_types_list &lt;- cols_only( sub_id = col_character(), day1 = col_integer(), day2 = col_integer(), event1 = col_integer(), event2 = col_integer(), pic1 = col_integer(), pic2 = col_integer(), virtual_time1 = col_double(), virtual_time2 = col_double(), real_time1 = col_double(), real_time2 = col_double(), memory_time1 = col_double(), memory_time2 = col_double(), memory_order1 = col_double(), memory_order2 = col_double(), sorted_day1 = col_integer(), sorted_day2 = col_integer(), pre_corrs = col_double(), post_corrs = col_double(), ps_change = col_double(), roi = col_factor() ) sub_dat &lt;- as_tibble(read_csv(fn, col_types = col_types_list)) # append to table with data from all subjects rsa_dat &lt;- bind_rows(sub_dat, rsa_dat) } # sort the data rsa_dat &lt;- rsa_dat[with(rsa_dat, order(sub_id, day1, day2, event1, event2)),] # write to file fn &lt;- file.path(dirs$data4analysis, &quot;rsa_data_rois.txt&quot;) write.table(rsa_dat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) "],
["run-rsa-searchlight.html", "7 Run RSA Searchlight 7.1 Prepare functional data 7.2 First-level RSA searchlight 7.3 Group-level searchlight analysis 7.4 Prepare RSA in searchlight peak", " 7 Run RSA Searchlight 7.1 Prepare functional data We want to run RSA on the multi-voxel patterns corresponding to the object presentations during the picture viewing tasks. For this, we use the data as it was preprocessed by Lorena Deuker. We will do some further cleaning and extract the relevant volumes from the time series to eventually run RSA searchlights. What will be done in the following sections has been done previously for the ROI data only. Now we do it for the whole field of view in preparation of the RSA searchlights. Data cleaning: Extract residuals from motion parameter regression In this first section of the script we will create a dataframe that includes the relevant file names for the files that are needed to calculate the clean time series. These files include the motion parameters from FSL FEAT, which was run on the functional data from the picture viewing task. As described above, these data were split into 10 blocks. We will use the preprocessed FEAT output images which were already co-registered to the analysis space (‘samespace’). Further, we will use a mask to only include voxels with data in all blocks. This mask has already been created and is available in the analysis space. #PREPARE DATA FRAME FOR CLEANING # create a tibble with filenames etc for data cleaning, start with subject IDs func_dat_df &lt;- tibble(subject = rep(as.numeric(subjects), each = n_runs * n_blocks)) # add run info (run 1 and 2 equal the pre and post PVT) func_dat_df &lt;- add_column(func_dat_df, run = rep(rep(1:n_runs, each = n_blocks), n_subs)) # add block information (blocks 1 to 10 are the PVT blocks preprocessed separately) func_dat_df &lt;- add_column(func_dat_df, block = rep(1:n_blocks, n_subs*n_runs)) # add the motion parameter file func_dat_df &lt;- add_column(func_dat_df, mc_par_fn = file.path(dirs$feat_dir, paste0(&quot;VIRTEM_P0&quot;, func_dat_df$subject), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, func_dat_df$run, func_dat_df$block), &quot;mc&quot;, &quot;prefiltered_func_data_mcf.par&quot;)) # add the functional data file func_dat_df &lt;- add_column(func_dat_df, filtered_func = file.path(dirs$samespace_dir, paste0(&quot;VIRTEM_P0&quot;, func_dat_df$subject), sprintf(&quot;VIRTEM_P%03d_RSA_%02d_Block%02d_func.nii.gz&quot;, func_dat_df$subject, func_dat_df$run, func_dat_df$block))) # add the brain mask data file func_dat_df &lt;- add_column(func_dat_df, brain_mask = file.path(dirs$samespace_dir, paste0(&quot;VIRTEM_P0&quot;, func_dat_df$subject), sprintf(&quot;VIRTEM_P%03d_common_mask_perBlock.nii.gz&quot;, func_dat_df$subject))) # add output directory func_dat_df &lt;- add_column(func_dat_df, out_dir = file.path(dirs$searchlight, &quot;clean_timeseries&quot;)) if(!dir.exists(file.path(dirs$searchlight, &quot;clean_timeseries&quot;))){ dir.create(file.path(dirs$searchlight, &quot;clean_timeseries&quot;)) } Define function for data cleaning In this next section, we will first define a function and then run it on all datasets. In this function, the preprocessed data will be loaded and transformed to matrix format. For every voxel within the brain mask, movement correction parameters are used as predictors in a GLM with the voxel time series as dependent variable. Finally, the residuals from this GLM (i.e. what could not be explained by motion) will be written to a text file. # DEFINE THE FUNCTION TO GET THE RESIDUAL TIME SERIES run_motion_glm &lt;- function(df = func_dat_df[1,]){ # load the motion params and convert to tibble mc_pars &lt;- read.table(df$mc_par_fn, header = FALSE) mp_df &lt;- as_tibble(mc_pars) colnames(mp_df) &lt;- paste0(&quot;mp&quot;, 1:6) # load the brain mask, check it&#39;s only zeros and ones, then convert to logical brain_mask_nii &lt;- readNIfTI2(df$brain_mask) assertthat::assert_that(all.equal(unique(c(img_data(brain_mask_nii))), c(0,1))) brain_mask &lt;- array(NA, dim(brain_mask_nii)) brain_mask[] &lt;- as.logical(img_data(brain_mask_nii)) # load the functional data func_nii &lt;- readNIfTI2(df$filtered_func) # initialize output clean_dat &lt;- array(0, dim(func_nii)) counter &lt;- 0 for (i_dim1 in 1:dim(func_nii)[1]){ for (i_dim2 in 1:dim(func_nii)[2]){ for (i_dim3 in 1:dim(func_nii)[3]){ if (brain_mask[i_dim1,i_dim2,i_dim3]){ # extract this voxel&#39;s data and merge with motion params mp_df$vox_dat &lt;- func_nii[i_dim1,i_dim2,i_dim3,] # run the glm and store the residuals clean_dat[i_dim1,i_dim2,i_dim3,] &lt;- resid(glm(&#39;vox_dat~mp1+mp2+mp3+mp4+mp5+mp6&#39;, data = mp_df)) } # print a message to show progress counter &lt;-counter+1 if (counter%%(prod(dim(func_nii)[1:3])/100) == 0) { print(paste0((counter/prod(dim(func_nii)[1:3])*100), &quot;% done&quot;))} } } } # create nifti with same header info as original data from clean time series clean_dat_nii = copyNIfTIHeader(img = func_nii, arr = clean_dat) # create output folder out_dir_sub &lt;- file.path(df$out_dir, sprintf(&quot;%03d&quot;,df$subject)) if(!dir.exists(out_dir_sub)){dir.create(out_dir_sub, recursive = TRUE)} # write cleaned nifti to file fn &lt;- sprintf(&quot;%03d_run%02d_block%02d_clean_func_data.nii.gz&quot;, df$subject, df$run, df$block) writenii(nim = clean_dat_nii, filename = file.path(out_dir_sub,fn)) } Run data cleaning Now actually run the function either in parallel or serially. # next step depends on whether we are in parallel or serial mode if (!run_parallel){ # run serially print(&quot;Attempting to run motion parameter cleaning serially. This will take a very long time if runnning for all subjects&quot;) # run the function for each row of the data frame, # i.e. for each block in each run for each subject for(i in 1:nrow(func_dat_df)){ tic(sprintf(&quot;Subject %s, run %d, block %d&quot;, func_dat_df$subject[i], func_dat_df$run[i], func_dat_df$block[i])) run_motion_glm(df = func_dat_df[i,]) toc() } } else if (run_parallel){ # run in parallel, assumes CBS HTCondor is available # write the data frame to file fn &lt;- file.path(here(&quot;data&quot;,&quot;mri&quot;, &quot;rsa&quot;, &quot;searchlight&quot;, &quot;clean_timeseries&quot;), &quot;htc_config_clean_time_series.txt&quot;) fn_def &lt;- cat(sprintf(&#39;&quot;fn &lt;- &quot;%s&quot;&#39;,fn)) write.table(func_dat_df, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) # store the function definition as text func_def &lt;- capture.output(run_motion_glm) func_def[1] &lt;- paste0(&quot;run_motion_glm &lt;- &quot;,func_def[1]) #func_def &lt;- func_def[-length(func_def)] #write the Rscript that we want to run rscript_fn &lt;- here(&quot;data&quot;,&quot;mri&quot;, &quot;rsa&quot;, &quot;searchlight&quot;, &quot;clean_timeseries&quot;, &quot;run_clean_timeseries.R&quot;) con &lt;- file(rscript_fn) open(con, &quot;w&quot;) writeLines(c( &quot;\\n# handle input&quot;, &quot;args = commandArgs()&quot;, &quot;i &lt;- as.numeric(args[length(args)])&quot;, &quot;\\n#load required packages&quot;, noquote(sprintf(&#39;lib_dir &lt;- &quot;%s&quot;&#39;,&quot;/data/pt_02261/virtem/virtem_code/R3.6.1/library/Linux&quot;)), &#39;.libPaths(c(lib_dir,.libPaths()))&#39;, &#39;lapply(c(&quot;oro.nifti&quot;, &quot;assertthat&quot;, &quot;dplyr&quot;, &quot;neurobase&quot;), library, character.only = TRUE)&#39;, &quot;\\n# read the data and transform ROI column back to list&quot;, noquote(sprintf(&#39;fn &lt;- &quot;%s&quot;&#39;,fn)), &#39;func_dat_df &lt;- read.table(fn, sep = &quot;,&quot;, header = TRUE, stringsAsFactors = FALSE)&#39;, &quot;\\n#define the function to run motion GLM&quot;, func_def, &quot;\\n# run the function on one line of the data frame&quot;, &quot;run_motion_glm(df = func_dat_df[i,])&quot;),con) close(con) # folder for condor output htc_dir &lt;- here(&quot;htc_logs&quot;, &quot;clean_timeseries&quot;) if(!exists(htc_dir)){dir.create(htc_dir, recursive = TRUE)} # write the submit script fn &lt;- here(&quot;data&quot;,&quot;mri&quot;, &quot;rsa&quot;, &quot;searchlight&quot;, &quot;clean_timeseries&quot;, &quot;run_clean_timeseries.submit&quot;) con &lt;- file(fn) open(con, &quot;w&quot;) writeLines(c( &quot;universe = vanilla&quot;, &quot;executable = /afs/cbs.mpg.de/software/scripts/envwrap&quot;, &quot;request_memory = 9000&quot;, &quot;notification = error&quot; ),con) for (i in 1:nrow(func_dat_df)){ writeLines(c( sprintf(&quot;\\narguments = R+ --version 3.6.1 Rscript %s %d&quot;, rscript_fn, i), sprintf(&quot;log = %s/%d.log&quot;, htc_dir, i), sprintf(&quot;output = %s/%d.out&quot;, htc_dir, i), sprintf(&quot;error = %s/%d.err&quot;, htc_dir, i), sprintf(&quot;Queue\\n&quot;)),con) } close(con) # submit to condor #system(&quot;reset-memory-max&quot;) batch_id &lt;- system(paste(&quot;condor_submit&quot;, fn), intern = TRUE) batch_id &lt;- regmatches(batch_id[2], gregexpr(&quot;[[:digit:]]+&quot;, batch_id[2]))[[1]][2] cat(sprintf(&quot;submitted jobs (ID = %s) to clean time series for searchlights. Time to wait...&quot;, batch_id)) pause_until_batch_done(batch_id = batch_id, wait_interval = 600) #system(&quot;reset-memory-max&quot;) #system(&quot;condor_q&quot;) } Extract relevant volumes As the presentation of images in the PVT pre and post blocks was locked to the onset of a new volume (see above), the second volume after image onset was selected for every trial (effectively covering the time between 2270-4540 ms after stimulus onset). In this step, we split the presentation logfile from the picture viewing task into the 10 blocks. We reference the volume count to the first volume of each block and extract the relevant volumes from the cleaned 4D timeseries data, accounting for the temporal offset. This is done for each block separately in both the pre and post runs. The data are written to 3D-niftis. offset_tr = 2 for (i_sub in subjects){ for (i_run in 1:n_runs){ # load the logfile for this run (pre or post) log_fn &lt;- file.path(dirs$pvt_log_dir, sprintf(&#39;P%s_%svirtem.txt&#39;, i_sub, runs[i_run])) log &lt;- read.table(log_fn) colnames(log) &lt;- c(&quot;pic&quot;, &quot;fix_start&quot;, &quot;pic_start&quot;, &quot;volume&quot;, &quot;response&quot;, &quot;RT&quot;, &quot;trial_end&quot;) # split the log file into the 10 blocks log_split &lt;- split(log, rep(1:10, each = 21)) # reference volume numbers to the first volume of that block vol_block &lt;- lapply(log_split, function(x){x$volume - x$volume[1]}) log_split &lt;- mapply(cbind, log_split, &quot;vol_block&quot;=vol_block, SIMPLIFY=FALSE) for (i_block in 1:n_blocks){ # define the full 4D file clean_func_fn &lt;- file.path(dirs$searchlight, &quot;clean_timeseries&quot;, i_sub, sprintf(&quot;%s_run%02d_block%02d_clean_func_data.nii.gz&quot;, i_sub, i_run, i_block)) # index of relevant volumes when accounting for offset rel_vols &lt;- log_split[[i_block]]$vol_block + offset_tr # extract the relevant volumes from the ROI timeseries data out_dir &lt;- file.path(dirs$searchlight, &quot;rel_vols_3D&quot;, i_sub) if(!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} for (i_vol in 1:length(rel_vols)){ # subtract 1 from volume number because of 0-based FSL indexing fslroi(file = clean_func_fn, tmin = rel_vols[i_vol]-1, tsize = 1, outfile = file.path(out_dir, sprintf(&quot;%s_run%02d_block%02d_pic%02d.nii.gz&quot;, i_sub, i_run, i_block, log_split[[i_block]]$pic[i_vol])), verbose = FALSE) } } } } Concatenate relevant volumes Next, we create 4D-files for both the pre and the post run. Each file will have consist of 200 (20 relevant pictures x 10 blocks) volumes. We take care to order the volumes according to picture identity as this is most convenient for later RSA. for (i_sub in subjects){ for (i_run in 1:n_runs){ # files to concatenate in_dir &lt;- file.path(dirs$searchlight, &quot;rel_vols_3D&quot;, i_sub) files &lt;- c(file.path(in_dir, sprintf(&quot;%s_run%02d_block%02d_pic%02d.nii.gz&quot;, i_sub, i_run, c(rep(1:10,n_pics)), c(rep(1:n_pics, each = n_blocks))))) # output file out_dir &lt;- file.path(dirs$searchlight, &quot;rel_vols_4D&quot;) if(!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} fn &lt;- file.path(out_dir, sprintf(&quot;%s_run%02d_rel_vols.nii.gz&quot;, i_sub, i_run)) # merge the files fslmerge(infiles = files, direction = &quot;t&quot;, outfile = fn, retimg = FALSE, verbose = FALSE) # change datatype to short to save space and memory for the cluster jobs fslmaths(file = fn, outfile = fn, retimg = FALSE, opts = &quot;-odt short&quot;, opts_after_outfile = TRUE, verbose = FALSE) } } 7.2 First-level RSA searchlight We further probed how temporal distances between events shaped representational change using searchlight analyses. Using the procedures described above, we calculated pattern similarity change values for search spheres with a radius of 3 voxels around the center voxel. Search spheres were centered on all brain voxels within our field of view. Within a given search sphere, only gray matter voxels were analyzed. Search spheres not containing more than 25 gray matter voxels were discarded. Building on the prepared data, we want to run the searchlight analysis. To speed things up, we split the brain into chunks that we analyze in parallel jobs. Set analysis parameters First, let’s set some analysis parameters. These include the radius of the search spheres. This is the radius around the center voxel, so that the diameter of the spheres is given by 2*radius + 1 the minimum number of valid voxels that a sphere has to contain to be analyzed the number of chunks into which we split the analysis to speed up the computations using the HPC cluster Further, we define a function that returns the voxels in the sphere of a given radius around. This canonical sphere definition will then be combined with the actual sphere center coordinates. It is based on the way the sphere definition is implemented in the fmri package for R. # how large should the searchlights be radius &lt;- 3 # radius around center voxel # how many voxels (counting the center) must be in a searchlight for it to be run; smaller ones are skipped. min_voxels &lt;- 25 # how many chunks to split the analysis up into (for speed reasons) n_chunks &lt;- 40 # function to get searchsphere of given radius (based on: https://rdrr.io/cran/fmri/src/R/searchlight.r) searchlight &lt;- function(radius){ rad &lt;- as.integer(radius) nr &lt;- 2*rad+1 indices &lt;- rbind(rep(-rad:rad,nr*nr), rep(-rad:rad,rep(nr,nr)), rep(-rad:rad,rep(nr*nr,nr))) indices[,apply(indices^2,2,sum)&lt;=radius^2] } Prepare RSA-predictions To relate pattern similarity change in each search sphere to the temporal structure of the sequences we need the relationships of the events in virtual time. We calculate these here analogously to the main ROI analyses, but save them for each subject separately. for (i_sub in subjects){ # set up the dataframe to use for RSA pics &lt;- which(upper.tri(matrix(TRUE, n_pics, n_pics), diag = FALSE), arr.ind=TRUE) # load the behavioral data fn &lt;- file.path(dirs$timeline_dat_dir, sprintf(&quot;%s_behavior_tbl_timeline.txt&quot;, i_sub)) col_types_list &lt;- cols_only(sub_id = col_character(), day = col_factor(), event = col_integer(), pic = col_integer(), virtual_time = col_double(), real_time = col_double(), memory_time = col_double(), memory_order = col_double(), sorted_day = col_integer()) beh_dat &lt;- read_csv(fn, col_types = col_types_list) # sort behavioral data according to picture identity beh_dat_ordered &lt;- beh_dat[order(beh_dat$pic),] # find the order of comparisons pics &lt;- which(upper.tri(matrix(nrow = 20, ncol = 20)), arr.ind=TRUE) # extract the data for the first and second picture in each pair from the behavioral data pic1_dat &lt;- beh_dat_ordered[pics[,1],] pic2_dat &lt;- beh_dat_ordered[pics[,2],] colnames(pic1_dat) &lt;- paste0(colnames(pic1_dat), &quot;1&quot;) colnames(pic2_dat) &lt;- paste0(colnames(pic2_dat), &quot;2&quot;) # combine the two tibbles pair_dat &lt;- cbind(pic1_dat, pic2_dat) # reorder the tibble columns to make a bit more sense pair_dat &lt;- pair_dat %&gt;% select(sub_id1, day1, day2, pic1, pic2, event1, event2, virtual_time1, virtual_time2, real_time1, real_time2, memory_order1, memory_order2, memory_time1, memory_time2, sorted_day1, sorted_day2) %&gt;% rename(sub_id = sub_id1) # prepare RSA distance measures pair_dat &lt;- pair_dat %&gt;% mutate( # pair of events from say or different day (dv = deviation code) same_day = day1 == day2, same_day_dv = plyr::mapvalues(same_day, from = c(FALSE, TRUE), to = c(-1, 1)), # absolute difference in time metrics vir_time_diff = abs(virtual_time1 - virtual_time2), order_diff = abs(event1 - event2), real_time_diff = abs(real_time1 - real_time2)) %&gt;% # z-score the time metric predictors mutate_at( c(&quot;vir_time_diff&quot;, &quot;order_diff&quot;, &quot;real_time_diff&quot;), scale) # write to file out_dir &lt;- file.path(dirs$searchlight, &quot;rsa_predictions&quot;) if(!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} fn &lt;- file.path(out_dir, sprintf(&quot;%s_rsa_info.txt&quot;, i_sub)) write.table(pair_dat, file = fn) } Create lookup table for searchlight As a first step, we prepare a table that - for each individual sphere - holds the coordinates of the voxels making up the search spheres. To find these voxels we rely on both a brain mask and a gray matter mask obtained during preprocessing. The brain mask is combined across the 10 preprocessing blocks and all voxels within this brain mask can be sphere centers. We get the coordinates of voxels around these sphere centers given the desired radius. Then, we exclude the non-gray matter voxels from the spheres. We check which spheres still contain the minimum number of voxels (do this here to speed up the subsequent computations) and discard the spheres that don’t hold enough voxels, e.g. because they are in white matter or on the edge of the brain/field of view. We split this table into chunks to speed up the calculations and save the chunked lookup-tables for later use. To be able to relate the rows of the table (each row is a sphere) back to the brain, we create a nifti image that numbers all voxels which are sphere centers. These numbers correspond to the row names of the lookup table and will later be used to map the RSA results of each sphere back to the brain. # Set up dataframe in_df &lt;- tibble(subject = subjects) # how many chunks to run the analysis in for each in_df$n_chunks &lt;- n_chunks # input files with relevant functional volumes in_df$rel_vol_pre_fn &lt;- file.path(dirs$searchlight, &quot;rel_vols_4D&quot;, sprintf(&quot;%s_run%02d_rel_vols.nii.gz&quot;, subjects, 1)) in_df$rel_vol_post_fn &lt;- file.path(dirs$searchlight, &quot;rel_vols_4D&quot;, sprintf(&quot;%s_run%02d_rel_vols.nii.gz&quot;, subjects, 2)) # which voxels should be center voxels of spheres --&gt; typically brain mask in_df$ctr_mask_fn &lt;- file.path(dirs$samespace_dir, paste0(&quot;VIRTEM_P&quot;, subjects), sprintf(&quot;VIRTEM_P%s_common_mask_perBlock.nii.gz&quot;, subjects)) # which voxels should be used for correlations --&gt; typically graymatter mask in_df$feat_mask_fn &lt;- file.path(dirs$samespace_dir, paste0(&quot;VIRTEM_P&quot;, subjects), sprintf(&quot;VIRTEM_P%s_common_graymatter_mask_brainmasked.nii.gz&quot;, subjects)) # radius of spheres in_df$radius &lt;- radius # where to store outputs in_df$out_path &lt;- file.path(dirs$searchlight, sprintf(&quot;first_level_%dvox_radius&quot;, radius), subjects) in_df$chunk_path &lt;- file.path(in_df$out_path, &quot;chunks&quot;) prepare_searchlight_lut &lt;- function(in_df = in_df[1,]){ if(!dir.exists(in_df$out_path)){dir.create(in_df$out_path, recursive = TRUE)} if(!dir.exists(in_df$chunk_path)){dir.create(in_df$chunk_path, recursive = TRUE)} # load the mask with voxels that will be sphere centers (typically brain mask) ctr_mask_nii &lt;- readNIfTI(in_df$ctr_mask_fn, reorient=FALSE) if (!all.equal(unique(c(img_data(ctr_mask_nii))), c(0,1))){ stop(&quot;center mask not binary!&quot;)} # load the mask with voxels that will be the features used for correlations feat_mask_nii &lt;- readNIfTI(in_df$feat_mask_fn, reorient=FALSE) feat_mask_array &lt;- array(0, dim = dim(feat_mask_nii)) feat_mask_array[feat_mask_nii &gt; 0.7 ] &lt;- 1 if (!all.equal(unique(c(feat_mask_array)), c(0,1))){ stop(&quot;make sure feature mask is binary!&quot;)} # make sure mask dimensions match functional images and each other if (!all.equal(dim(ctr_mask_nii), dim(feat_mask_array))){ stop(&quot;mask dimensions don&#39;t match each other!&quot;)} # find all the voxel coordinates that will be sphere centers ctr_inds &lt;- which(ctr_mask_nii != 0, arr.ind=TRUE) # inds has subscripts for 3D coords of center voxels if (ncol(ctr_inds) != 3) { stop(&#39;wrong dims on inds&#39;)} # 3D array with voxel numbers to save for future reference vox_num_array &lt;- array(0, dim(ctr_mask_nii)) # make a blank 3d matrix &#39;brain&#39;, all zeros vox_num_array[ctr_inds] &lt;- 1:nrow(ctr_inds) # and put integers into the voxel places # create nifti with same header info as input data vox_num_nii &lt;- copyNIfTIHeader(img = ctr_mask_nii, arr = vox_num_array) write_nifti(vox_num_nii, file.path(in_df$out_path, &quot;vox_num&quot;)) # write out as a NIfTI image # get the search sphere 3D subscripts sphere_coords &lt;- searchlight(radius) # actually fill up the lookup table. This goes through every voxel, so can take some time. # the table will hold the sphere indices for each center voxel lookup_tbl &lt;- array(NA, c(nrow(ctr_inds), ncol(sphere_coords))) rownames(lookup_tbl) &lt;- vox_num_nii[ctr_inds] for (i in 1:nrow(ctr_inds)) { # i &lt;- 1 # add the sphere coordinates to the current voxel coordinates curr_sphere &lt;- t(ctr_inds[i,] + sphere_coords) # remove voxels that are out of bounds (i.e. out of image dimensions) dims &lt;- dim(vox_num_array) curr_sphere[which(curr_sphere[,1]&gt;dims[1]),] &lt;- NA curr_sphere[which(curr_sphere[,2]&gt;dims[2]),] &lt;- NA curr_sphere[which(curr_sphere[,3]&gt;dims[3]),] &lt;- NA curr_sphere[which(curr_sphere[,1]&lt;1),] &lt;- NA curr_sphere[which(curr_sphere[,2]&lt;1),] &lt;- NA curr_sphere[which(curr_sphere[,3]&lt;1),] &lt;- NA # remove voxels that are not in the feature mask curr_sphere[!feat_mask_array[curr_sphere],] &lt;- NA # store the voxel numbers for this sphere vox.id &lt;- vox_num_array[ctr_inds[i,1], ctr_inds[i,2], ctr_inds[i,3]] lookup_tbl[vox.id,] &lt;- vox_num_array[curr_sphere] } # replace the zeroes with NA so can sort better at runtime to_remove &lt;- which(lookup_tbl == 0, arr.ind=TRUE) lookup_tbl[to_remove] &lt;- NA # remove rows that don&#39;t have the minimum amount of voxels (do this here because this speeds up all later computations) lookup_tbl &lt;- lookup_tbl[rowSums(!is.na(lookup_tbl)) &gt; min_voxels,] # write the table, gzipped to save file size write.table(lookup_tbl, gzfile(file.path(in_df$out_path, sprintf(&quot;lookup_radius%d&quot;, in_df$radius, &quot;.txt.gz&quot;))), row.names = TRUE) # vector to split lookupt table into chunks (last chunk will be slightly smaller) r &lt;- rep(1:in_df$n_chunks,each=ceiling(nrow(lookup_tbl)/in_df$n_chunks))[1: nrow(lookup_tbl)] for (i_chunk in 1:in_df$n_chunks){ # extract current chunk and write to file curr_chunk &lt;- lookup_tbl[r == i_chunk,] write.table(curr_chunk, gzfile(file.path(in_df$chunk_path, sprintf(&quot;lookup_radius%d_chunk_%02d.txt.gz&quot;, in_df$radius, i_chunk))), row.names = TRUE) } } # run for all datasets for (i in 1:nrow(in_df)){ prepare_searchlight_lut(in_df = in_df[i,]) } We want to create some diagnostic images of the spheres we created to make sure the resulting search spheres look as expected. For this a random subject is picked and a number of random spheres is plotted on the gray matter mask. # pick random subject rand_sub &lt;- sample(1:n_subs)[1] # read their lookup-table lookup_tbl&lt;-read.table(gzfile( file.path(in_df$out_path[rand_sub], sprintf(&quot;lookup_radius%d&quot;, in_df$radius[rand_sub], &quot;.txt.gz&quot;)))) # load their graymatter mask and the nifti linking indices gm_mask_nii &lt;- in_df[rand_sub,]$feat_mask_fn %&gt;% readNIfTI2() vox_num_nii &lt;- file.path(in_df$out_path[rand_sub], &quot;vox_num.nii.gz&quot;) %&gt;% readNIfTI2() # pick 10 searchlights to plot at random do.searchlights &lt;- sample(1:nrow(lookup_tbl))[1:10] # make a blank 3d matrix &#39;brain&#39; to put the searchlights into sphere_test_array &lt;- array(0, dim(gm_mask_nii)) for (i in 1:length(do.searchlights)) { # i &lt;- 1 # voxels of this example sphere voxs &lt;- unlist(lookup_tbl[do.searchlights[i],], use.names=FALSE) voxs &lt;- voxs[which(!is.na(voxs))] # get rid of NAs print(sprintf(&#39;sphere %02d: %d voxels&#39;, i, length(voxs))) # put integers into the sphere test array for each searchlight for (j in 1:length(voxs)) { # j &lt;- 1 coords &lt;- which(vox_num_nii == voxs[j], arr.ind=TRUE) # location of this voxel if (ncol(coords) != 3 | nrow(coords) != 1) { stop(&quot;wrong sized coords&quot;)} sphere_test_array[coords[1], coords[2], coords[3]] &lt;- i # assign this voxel the searchlight number } # visualize the sphere at the most frequent coordinate ortho2(gm_mask_nii, y=sphere_test_array==i, col.y = &quot;red&quot;, col.crosshairs = &quot;lightgrey&quot;, xyz = c(statip::mfv1(which(sphere_test_array==i, arr.ind = TRUE)[,1]), statip::mfv1(which(sphere_test_array==i, arr.ind = TRUE)[,2]), statip::mfv1(which(sphere_test_array==i, arr.ind = TRUE)[,3]))) } # create nifti with same header info as input data and save sphere_test_nii = copyNIfTIHeader(img = gm_mask_nii, arr = sphere_test_array) write_nifti(sphere_test_nii, file.path(dirs$searchlight, sprintf(&quot;first_level_%dvox_radius&quot;, radius), sprintf(&quot;sphere_test_sub%s&quot;, rand_sub))) Dataframe with input for analysis We start by preparing a data frame with one line for each chunk of each subject. The entries define the files to be used for this chunk and some basic analysis parameters. The function to be defined subsequently takes one row of this data frame as input. in_df &lt;- tibble(subject = rep(subjects, each=n_chunks)) in_df$chunk &lt;- rep(1:n_chunks, length(subjects)) in_df$lut_file &lt;- file.path(dirs$searchlight, sprintf(&quot;first_level_%dvox_radius&quot;, radius), rep(subjects, each=n_chunks), &quot;chunks&quot;, sprintf(&quot;lookup_radius%d_chunk_%02d.txt.gz&quot;, radius, rep(1:n_chunks, length(subjects)))) in_df$pred_file &lt;- file.path(dirs$searchlight, &quot;rsa_predictions&quot;, sprintf(&quot;%s_rsa_info.txt&quot;, rep(subjects, each=n_chunks))) # input files with relevant functional volumes in_df$rel_vol_pre_fn &lt;- rep(file.path(dirs$searchlight, &quot;rel_vols_4D&quot;, sprintf(&quot;%s_run%02d_rel_vols.nii.gz&quot;, subjects, 1)), each = n_chunks) in_df$rel_vol_post_fn &lt;- rep(file.path(dirs$searchlight, &quot;rel_vols_4D&quot;, sprintf(&quot;%s_run%02d_rel_vols.nii.gz&quot;, subjects, 2)), each = n_chunks) # file to use to bring data back into brain shape in_df$vox_num_fn &lt;- rep(file.path(dirs$searchlight, sprintf(&quot;first_level_%dvox_radius&quot;, radius), subjects, &quot;vox_num.nii.gz&quot;), each = n_chunks) # output directory in_df$out_path &lt;- rep(file.path(dirs$searchlight, sprintf(&quot;first_level_%dvox_radius&quot;, radius), subjects, &quot;chunks&quot;), each = n_chunks) # minimum number of features in_df$min_voxels &lt;- min_voxels head(in_df) Searchlight implementation Here, we define the function that implements RSA within each searchlight. The logic of running the searchlight analysis via the look-up table is inspired by and partly based on this blogpost by Joset A. Etzel and the code accompanying it. For each search sphere, we implemented linear models to quantify the relationship between representational change and the learned temporal structure. Specifically, we assessed the relationship of pattern similarity change and absolute virtual temporal distances, separately for event pairs from the same sequences and from pairs from different sequences. In a third model, we included all event pairs and tested for an interaction effect of a sequence (same or different) predictor and virtual temporal distances. The t-values of the respective regressors of interest were stored at the center voxel of a given search sphere. The function goes through the same steps as the ROI-based analysis. First, the data of the voxels in each sphere is extracted. The resulting multi-voxel patterns are correlated between picture presentations to yield a trial-by-trial (200-by-200) correlation matrix. For each pair of images, the trial-wise correlations (10-by-10) are averaged such that comparisons of trials from the same block are excluded (i.e. excluding the diagonal of the 10-by-10 squares). This results in the condition-by-condition similarity matrix (20-by-20), which is then subjected to further analysis based on the temporal structure of events. Specifically, linear models are implemented for the different analyses. The resulting t-values are stored at the location of the center voxels of the respective sphere. Thus, for each model we test, we end up with a nifti image that has t-values at the voxel locations of the current chunk. # FUNCTION DEFINITION run_searchlight &lt;- function(in_df){ #in_df &lt;- in_df[1,] ########## SET UP A FEW PARAMS sprintf(&quot;working on: %s&quot;, in_df$lut_file) n_pics &lt;- 20 n_blocks &lt;- 10 # for all 10x10 comparisons we will be averaging all comparisons apart from the diagonal # to exclude same_block comparisons no_diag &lt;- matrix(data = TRUE, nrow=n_blocks, ncol=n_blocks) diag(no_diag)&lt;- FALSE # indices to extract upper triangle excluding diagonal from pic-by-pic matrices triu_idx &lt;- which(upper.tri(matrix(TRUE, n_pics, n_pics), diag = FALSE), arr.ind = TRUE) ########## LOAD THE DATA TO BE USED # load the data about the learned temporal relationships rsa_info &lt;- read.table(in_df$pred_file) # initialize dataframe to fill (later ps-change data will be added) pics &lt;- which(upper.tri(matrix(TRUE, n_pics, n_pics), diag = FALSE), arr.ind=TRUE) rsa_df &lt;- tibble(pic1 = pics[,1], pic2 = pics[,2]) # make sure the data frames have the same order if(!(all.equal(rsa_df$pic1, rsa_info$pic1) &amp; all.equal(rsa_df$pic2, rsa_info$pic2))){ stop(&#39;order of data frames does not match [RSA preparations]&#39;)} # store the columns we need for RSA in the data frame rsa_df &lt;- cbind(rsa_df, rsa_info[names(rsa_info) %in% c(&quot;same_day&quot;, &quot;same_day_dv&quot;, &quot;vir_time_diff&quot;, &quot;order_diff&quot;, &quot;real_time_diff&quot;)]) # read the look up table lookup_tbl &lt;- read.table(gzfile(in_df$lut_file), row.names = 1) # figure out which voxels to run in this chunk n_voxels &lt;- nrow(lookup_tbl) # read the functional images (takes ~ 2 mins) print(&quot;starting to load functional images&quot;) func_nii_pre &lt;- readNIfTI(in_df$rel_vol_pre_fn, reorient=FALSE) func_nii_post &lt;- readNIfTI(in_df$rel_vol_post_fn, reorient=FALSE) if(!all.equal(dim(func_nii_pre),dim(func_nii_post))){ stop(&quot;functional images of different size!&quot;)}# make sure inputs have same dimensions # load the image with voxel numbers (linear subscripts) that will be used to sort back into brain shape vox_num_nii &lt;- readNIfTI(in_df$vox_num_fn, reorient=FALSE) if (!all.equal(dim(func_nii_pre)[1:3], dim(vox_num_nii))){ stop(&quot;voxel number image dimensions don&#39;t match functional data!&quot;)} print(&quot;finished loading images&quot;) ########## BEGIN THE ACTUAL ANALYSIS # initialize the output as all 0 (because FSL does not like NAs) rsa_out_array_same_day_vir_time &lt;- array(0, dim(vox_num_nii)) rsa_out_array_diff_day_vir_time &lt;- array(0, dim(vox_num_nii)) rsa_out_array_all_pairs_vir_time &lt;- array(0, dim(vox_num_nii)) rsa_out_array_interaction &lt;- array(0, dim(vox_num_nii)) #rsa_out_array_n_in_sphere &lt;- array(0, dim(vox_num_nii)) # loop over all voxels (in this chunk) for (v in 1:n_voxels) { # v &lt;- do.centers[500] # print a message to show progress if (v%%100 == 0) { print(paste(&quot;at&quot;, v, &quot;of&quot;, n_voxels))} # find which voxels belong in this center voxel&#39;s searchlight voxs &lt;- unlist(lookup_tbl[v,], use.names=FALSE) voxs &lt;- voxs[which(!is.na(voxs))]; # get rid of NAs. There will be NA entries if some surrounding voxels not in gray matter or brain # how many surrounding voxels must be in the searchlight? Smaller ones (edge of brain) will be skipped. if (length(voxs) &gt; in_df$min_voxels) { # initialize arrays to store data of current sphere for the pre and post runs # images in the rows (voxels in this searchlight only), voxels in the columns curr_dat_pre &lt;- array(NA, c(n_pics*n_blocks, length(voxs))) curr_dat_post &lt;- array(NA, c(n_pics*n_blocks, length(voxs))) # put the data into a matrix to run analysis for (i in 1:length(voxs)) { # i &lt;- 1 # for the current voxel, take the data from all conditions and store it (separately for pre and post) coords &lt;- which(vox_num_nii == voxs[i], arr.ind=TRUE) if (ncol(coords) != 3 | nrow(coords) != 1) { stop(&quot;wrong sized coords&quot;)} curr_vox &lt;- func_nii_pre[coords[1], coords[2], coords[3],] # pre if (sd(curr_vox) &gt; 0) {curr_dat_pre[,i] &lt;- curr_vox} else { stop(&quot;zero variance voxel&quot;)} curr_vox &lt;- func_nii_post[coords[1], coords[2], coords[3],] # post if (sd(curr_vox) &gt; 0) {curr_dat_post[,i] &lt;- curr_vox} else { stop(&quot;zero variance voxel&quot;)} } # data is in repetition (row) by voxel (col) format, so we transpose # to get a voxel x repetition format curr_dat_pre &lt;- t(curr_dat_pre) curr_dat_post &lt;- t(curr_dat_post) # calculate correlation matrix (trial by trial) for pre and post run cor_mat_pre_trial &lt;- cor(curr_dat_pre, curr_dat_pre) cor_mat_post_trial &lt;- cor(curr_dat_post, curr_dat_post) # initialize condition by condition correlation matrix for pre and post run corr_mat_pre &lt;- matrix(nrow = 20, ncol = 20) corr_mat_post &lt;- matrix(nrow = 20, ncol = 20) # loop over all picture comparisons for(i_pic1 in 1:20){ for(i_pic2 in 1:20){ # extract the current 10x10 correlation matrix i1 &lt;- (1+(i_pic1-1)*10):(i_pic1*10) i2 &lt;- (1+(i_pic2-1)*10):(i_pic2*10) curr_mat_pre &lt;- cor_mat_pre_trial[i1, i2] curr_mat_post &lt;- cor_mat_post_trial[i1, i2] # average the correlations while excluding diagonal (same block comparisons) corr_mat_pre[i_pic1, i_pic2] &lt;- mean(curr_mat_pre[no_diag]) corr_mat_post[i_pic1, i_pic2] &lt;- mean(curr_mat_post[no_diag]) } } # calculate pattern similarity change based on FisherZ-transformed upper # triangles of the correlation matrices rsa_df$ps_change &lt;- FisherZ(corr_mat_post[triu_idx]) - FisherZ(corr_mat_pre[triu_idx]) # find 3D-coordinates of this searchlight center to save output coords &lt;- which(vox_num_nii == as.numeric(rownames(lookup_tbl[v,])), arr.ind=TRUE) if (ncol(coords) != 3 | nrow(coords) != 1) { stop(&quot;wrong sized coords&quot;); } ########## RUN RSA FOR THIS SPHERE # same day virtual time fit &lt;- lm(ps_change ~ vir_time_diff, rsa_df[rsa_df$same_day,]) rsa_out_array_same_day_vir_time[coords[1], coords[2], coords[3]] &lt;- coef(summary(fit))[, &quot;t value&quot;][2] # different day virtual time fit &lt;- lm(ps_change ~ vir_time_diff, rsa_df[!rsa_df$same_day,]) rsa_out_array_diff_day_vir_time[coords[1], coords[2], coords[3]] &lt;- coef(summary(fit))[, &quot;t value&quot;][2] # all pairs virtual time fit &lt;- lm(ps_change ~ vir_time_diff, rsa_df) rsa_out_array_all_pairs_vir_time[coords[1], coords[2], coords[3]] &lt;- coef(summary(fit))[, &quot;t value&quot;][2] # day*time interaction fit &lt;- lm(ps_change ~ vir_time_diff*same_day_dv, rsa_df) rsa_out_array_interaction[coords[1], coords[2], coords[3]] &lt;- coef(summary(fit))[, &quot;t value&quot;][4] # number of features #rsa_out_array_n_in_sphere[coords[1], coords[2], coords[3]] &lt;- length(voxs) } else{ stop(&quot;number of features too small!&quot;) } } ########## SAVE RESULTS # create nifti with same header info as input data and write to file # same day virtual time rsa_out_nii = copyNIfTIHeader(img = vox_num_nii, arr = rsa_out_array_same_day_vir_time) write_nifti(rsa_out_nii, file.path(in_df$out_path, sprintf(&quot;%s_searchlight_same_day_vir_time_chunk%02d&quot;, in_df$subject, in_df$chunk))) # different day virtual time rsa_out_nii = copyNIfTIHeader(img = vox_num_nii, arr = rsa_out_array_diff_day_vir_time) write_nifti(rsa_out_nii, file.path(in_df$out_path, sprintf(&quot;%s_searchlight_diff_day_vir_time_chunk%02d&quot;, in_df$subject, in_df$chunk))) # all pairs virtual time rsa_out_nii = copyNIfTIHeader(img = vox_num_nii, arr = rsa_out_array_all_pairs_vir_time) write_nifti(rsa_out_nii, file.path(in_df$out_path, sprintf(&quot;%s_searchlight_all_pairs_vir_time_chunk%02d&quot;, in_df$subject, in_df$chunk))) # day*time interaction rsa_out_nii = copyNIfTIHeader(img = vox_num_nii, arr = rsa_out_array_interaction) write_nifti(rsa_out_nii, file.path(in_df$out_path, sprintf(&quot;%s_searchlight_day_time_interaction_chunk%02d&quot;, in_df$subject, in_df$chunk))) # no. voxels in sphere #rsa_out_nii = copyNIfTIHeader(img = vox_num_nii, arr = rsa_out_array_n_in_sphere) #write_nifti(rsa_out_nii, file.path(in_df$out_path, sprintf(&quot;%s_searchlight_n_vox_sphere_chunk%02d&quot;, in_df$subject, in_df$chunk))) } Now actually run the function for each chunk; either in parallel or serially. Because the analysis is slow (several hours per chunk when using 50 chunks), it is not feasible to run it serially. # next step depends on whether we are in parallel or serial mode if (!run_parallel){ # run serially print(&quot;Running the searchlight analysis serially. Do this only for testing purposes because the code will run forever&quot;) # run the function for each row of the data frame, # i.e. for each block in each run for each subject for(i in 1:nrow(in_df)){ tic(sprintf(&quot;Subject %s, chunk %d&quot;, in_df$subject[i], in_df$chunk[i])) run_searchlight(in_df = in_df[i,]) toc() } } else if (run_parallel){ # run in parallel, assumes CBS HTCondor is available # write the data frame to file fn &lt;- file.path(here(&quot;data&quot;,&quot;mri&quot;, &quot;rsa&quot;, &quot;searchlight&quot;, sprintf(&quot;first_level_%dvox_radius&quot;, radius)), &quot;htc_config_run_searchlight.txt&quot;) fn_def &lt;- cat(sprintf(&#39;&quot;fn &lt;- &quot;%s&quot;&#39;,fn)) write.table(in_df, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) # store the function definition as text func_def &lt;- capture.output(run_searchlight) func_def[1] &lt;- paste0(&quot;run_searchlight &lt;- &quot;,func_def[1]) #func_def &lt;- func_def[-length(func_def)] #write the Rscript that we want to run rscript_fn &lt;- here(&quot;data&quot;,&quot;mri&quot;, &quot;rsa&quot;, &quot;searchlight&quot;, sprintf(&quot;first_level_%dvox_radius&quot;, radius), &quot;run_searchlight.R&quot;) con &lt;- file(rscript_fn) open(con, &quot;w&quot;) writeLines(c( &quot;\\n# handle input&quot;, &quot;args = commandArgs()&quot;, &quot;i &lt;- as.numeric(args[length(args)])&quot;, &quot;\\n#load required packages&quot;, noquote(sprintf(&#39;lib_dir &lt;- &quot;%s&quot;&#39;,&quot;/data/pt_02261/virtem/virtem_code/R3.6.1/library/Linux&quot;)), &#39;.libPaths(c(lib_dir,.libPaths()))&#39;, &#39;lapply(c(&quot;oro.nifti&quot;, &quot;assertthat&quot;, &quot;dplyr&quot;, &quot;neurobase&quot;, &quot;DescTools&quot;), library, character.only = TRUE)&#39;, &quot;\\n# read the data and transform ROI column back to list&quot;, noquote(sprintf(&#39;fn &lt;- &quot;%s&quot;&#39;,fn)), &#39;func_dat_df &lt;- read.table(fn, sep = &quot;,&quot;, header = TRUE, stringsAsFactors = FALSE)&#39;, &quot;\\n#define the function to run motion GLM&quot;, func_def, &quot;\\n# run the function on one line of the data frame&quot;, &quot;run_searchlight(in_df = func_dat_df[i,])&quot;),con) close(con) # folder for condor output htc_dir &lt;- here(&quot;htc_logs&quot;, &quot;first_level_searchlight&quot;) if(!dir.exists(htc_dir)){dir.create(htc_dir, recursive = TRUE)} # write the submit script fn &lt;- here(&quot;data&quot;,&quot;mri&quot;, &quot;rsa&quot;, &quot;searchlight&quot;, sprintf(&quot;first_level_%dvox_radius&quot;, radius), &quot;run_searchlight.submit&quot;) con &lt;- file(fn) open(con, &quot;w&quot;) writeLines(c( &quot;universe = vanilla&quot;, &quot;executable = /afs/cbs.mpg.de/software/scripts/envwrap&quot;, &quot;request_memory = 13500&quot;, # &quot;request_memory = 13500&quot;, # works for 3 voxel radius &amp; 40 chunks &quot;notification = error&quot; ),con) c &lt;- 0 for (i in 1:nrow(in_df)){ if (!file.exists(file.path(in_df[i,]$out_path, sprintf(&quot;%d_searchlight_all_pairs_vir_time_chunk%02d.nii.gz&quot;, as.numeric(in_df[i,]$subject), in_df[i,]$chunk)))){ c &lt;- c + 1 writeLines(c( sprintf(&quot;\\narguments = R+ --version 3.6.1 Rscript %s %d&quot;, rscript_fn, i), sprintf(&quot;log = %s/%d.log&quot;, htc_dir, i), sprintf(&quot;output = %s/%d.out&quot;, htc_dir, i), sprintf(&quot;error = %s/%d.err&quot;, htc_dir, i), sprintf(&quot;Queue\\n&quot;)),con) } } close(con) # submit to condor and play the waiting game batch_id &lt;- system(paste(&quot;condor_submit&quot;, fn), intern = TRUE) batch_id &lt;- regmatches(batch_id[2], gregexpr(&quot;[[:digit:]]+&quot;, batch_id[2]))[[1]][2] cat(sprintf(&quot;submitted jobs (ID = %s) for 1st-level searchlights. Time to wait...&quot;, batch_id)) pause_until_batch_done(batch_id = batch_id, wait_interval = 1800) # check every half hour if done #system(&quot;condor_q&quot;) } Combine the chunks Lastly, we need to combine the results across chunks to get the whole picture. # analyses that we ran the searchlights for searchlights &lt;- c(&quot;same_day_vir_time&quot;, &quot;diff_day_vir_time&quot;, &quot;day_time_interaction&quot;, &quot;all_pairs_vir_time&quot;#, #&quot;n_vox_sphere&quot; ) for (i_sub in subjects){ # where are the chinks in_dir &lt;- file.path(dirs$searchlight, sprintf(&quot;first_level_%dvox_radius&quot;, radius), i_sub, &quot;chunks&quot;) for (i_srchlght in searchlights){ # file names of the individual chunks in_files &lt;- file.path(in_dir, sprintf(&quot;%d_searchlight_%s_chunk%02d.nii.gz&quot;, as.numeric(i_sub), i_srchlght, 1:n_chunks)) # load the first chunk image, this will serve as the basis for combining all chunks comb_nii &lt;- readNIfTI(in_files[1], reorient = FALSE) for (i_chunk in 2:n_chunks){ # load this chunk&#39;s nifti new_nii &lt;- readNIfTI(in_files[i_chunk], reorient = FALSE) # find where the data is (i.e. where the image is non-zero) #new_nii[is.na(new_nii)] &lt;- 0 coords &lt;- which(new_nii !=0, arr.ind = TRUE) # the combined image should be 0 at all these coordinates if(!(sum(comb_nii[coords]==0) == nrow(coords))){ stop(&quot;chunks overlap!&quot;)} # add this chunk&#39;s data to the combined nifti comb_nii[coords] &lt;- new_nii[coords] } # make a simple plot to check that the output is shaped like a brain fn &lt;- file.path(dirs$searchlight, sprintf(&quot;first_level_%dvox_radius&quot;, radius), i_sub, sprintf(&quot;%s_%s&quot;, i_sub, i_srchlght)) jpeg(file = paste0(fn,&quot;.jpg&quot;), width = 1000, height = 1000, units = &quot;px&quot;) ortho2(comb_nii, NA.x=TRUE) dev.off() # write nifti to file write_nifti(comb_nii, fn) } } 7.3 Group-level searchlight analysis After running the first level analysis, we want to run group-level statistics. Group-level stats will be run using permutation-based one sample t-tests as implemented by the sign-flipping procedure in FSL Randomise. This test will be run for all voxels in our field of view as well as in a mask comprising the aHPC and alEC to correct for multiple comparisons within our a priori regions of interest. First, let’s define some parameters and folders for these analyses. # searchlight radius (used in file names) radius &lt;- 3 # analyses that we ran the searchlights for searchlights &lt;- c(&quot;same_day_vir_time&quot;, &quot;diff_day_vir_time&quot;, &quot;day_time_interaction&quot;, &quot;all_pairs_vir_time&quot; ) # what smoothing to apply FWHM = 3 # in mm sigma = FWHM/2.3548 # fslmaths needs sigma not FWHM of kernel # Output folder invisible(lapply(file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights), function(x) if(!dir.exists(x)) dir.create(x, recursive=TRUE))) # folder for condor output htc_dir &lt;- here(&quot;htc_logs&quot;, &quot;srchlght&quot;) if(!exists(htc_dir)){dir.create(htc_dir, recursive = TRUE)} Move first-level images to MNI space The resulting t-maps were registered to MNI space for group level statistics and spatially smoothed (FWHM 3mm). The first level searchlight output is in each participant’s common functional space. For group-level stats we need to register the searchlight outputs to MNI (1mm) space. # name of transformation matrix file to move from the shared functional space to MNI 1mm func2standard &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;wholebrain&quot;, paste0(&quot;VIRTEM_P&quot;, rep(subjects, each = length(searchlights)), &quot;.feat&quot;), &quot;reg&quot;, &quot;example_func2standard.mat&quot;) # searchlight result in whole-brain functional space in_nii_fn &lt;- file.path(dirs$searchlight, sprintf(&quot;first_level_%dvox_radius&quot;, radius), rep(subjects, each = length(searchlights)), sprintf(&quot;%s_%s.nii.gz&quot;, rep(subjects, each = length(searchlights)), searchlights)) # output folder &amp; file name in MNI space invisible(lapply(file.path(dirs$searchlight, sprintf(&quot;mni_%dvox_radius&quot;, radius), searchlights, &quot;3d&quot;), function(x) if(!dir.exists(x)) dir.create(x, recursive=TRUE))) out_nii_fn &lt;- file.path(dirs$searchlight, sprintf(&quot;mni_%dvox_radius&quot;, radius), rep(searchlights,n_subs), &quot;3d&quot;, sprintf(&quot;%s_%s.nii.gz&quot;, rep(subjects, each = length(searchlights)), searchlights)) # apply FSL flirt to move from whole-brain functional space to 1mm MNI space invisible(mapply(flirt_apply, infile = in_nii_fn, reffile = mni_fname(&quot;1&quot;), initmat = func2standard, outfile = out_nii_fn, verbose = FALSE, retimg = FALSE)) Merge and smooth first-level images Now that all first-level images are in MNI space, we are ready to concatenate the images across participants. Then, we subject them to Gaussian smoothing. # field of view mask fov_mask &lt;- file.path(dirs$mask_dir, &quot;fov&quot;, &quot;fov_mask_mni.nii.gz&quot;) # open the task list fn &lt;- file.path(htc_dir, &quot;smooth_searchlights_tasklist.txt&quot;) con &lt;- file(fn) open(con, &quot;w&quot;) for (i_srchlght in 1:length(searchlights)){ # file names of files to merge in_files &lt;- file.path(dirs$searchlight, sprintf(&quot;mni_%dvox_radius&quot;, radius), searchlights[i_srchlght], &quot;3d&quot;, sprintf(&quot;%s_%s.nii.gz&quot;, subjects,searchlights[i_srchlght])) # file name of 4D output file fn_4d &lt;- file.path(dirs$searchlight, sprintf(&quot;mni_%dvox_radius&quot;, radius), searchlights[i_srchlght], sprintf(&quot;%s_4d.nii.gz&quot;, searchlights[i_srchlght])) # concatenate the images fslmerge(infiles = in_files, direction = &quot;t&quot;, outfile = fn_4d, retimg = FALSE, verbose = FALSE) # name of smoothed output file fn_4d_smooth &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;%s_4d_smooth_fwhm%d.nii.gz&quot;, searchlights[i_srchlght], FWHM)) # write smoothing command to file writeLines(sprintf(&quot;fslmaths %s -s %f -mas %s %s&quot;, fn_4d, sigma, fov_mask, fn_4d_smooth), con) } close(con) # submit to cluster cmd &lt;- sprintf(&quot;fsl_sub -T 30 -t %s -l %s -M bellmund@cbs.mpg.de -N smooth_srchlghts&quot;, fn, htc_dir) batch_id &lt;- system(cmd, intern = TRUE) pause_until_batch_done(batch_id = batch_id, wait_interval = 300) Run FSL Randomise Now we are ready to run FSL Randomise. Group level statistics were carried out using random sign flipping implemented with FSL Randomise and threshold-free cluster enhancement. We corrected for multiple comparisons using a small volume correction mask including our a priori regions of interest, the anterior hippocampus and the anterior-lateral entorhinal cortex. # masks to use gm_mask_fn &lt;- file.path(dirs$mask_dir, &quot;gray_matter&quot;, &quot;gray_matter_mask.nii.gz&quot;) svc_mask_fn &lt;- file.path(dirs$mask_dir, &quot;svc&quot;, &quot;svc_mask.nii.gz&quot;) # open the tasklist fn &lt;- file.path(htc_dir, &quot;randomise_searchlights_tasklist.txt&quot;) con &lt;- file(fn) open(con, &quot;w&quot;) for (i_srchlght in 1:length(searchlights)){ # do we want to look at the positive or negative contrast? if (any(searchlights[i_srchlght] == c(&quot;same_day_vir_time&quot;, &quot;day_time_interaction&quot;))){ test_side = &quot;&quot; } else { # we want to test for a negative effect for these searchlights test_side = &quot;_neg&quot; # multiply by -1 to get the negative contrast orig_file &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;%s_4d_smooth_fwhm%d.nii.gz&quot;, searchlights[i_srchlght], FWHM)) mul_neg1_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;%s%s_4d_smooth_fwhm%d.nii.gz&quot;, searchlights[i_srchlght], test_side, FWHM)) fslmaths(file = orig_file, outfile = mul_neg1_fn, opts = &quot;-mul -1&quot;) } # 4D input image to run randomise on in_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;%s%s_4d_smooth_fwhm%d.nii.gz&quot;, searchlights[i_srchlght], test_side, FWHM)) # output file name for FOV out_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;%s%s_randomise_fov_fwhm%d&quot;, searchlights[i_srchlght], test_side, FWHM)) # define randomise command for full FOV and write to file writeLines(sprintf(&quot;randomise -i %s -o %s -1 -T --uncorrp -m %s -n 5000&quot;, in_fn, out_fn, gm_mask_fn),con) # output file name for SVC out_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;%s%s_randomise_svc_fwhm%d&quot;, searchlights[i_srchlght], test_side, FWHM)) # define randomise command for small-volume correction and and write to file writeLines(sprintf(&quot;randomise -i %s -o %s -1 -T --uncorrp -m %s -n 5000&quot;, in_fn, out_fn, svc_mask_fn),con) } close(con) # submit to cluster cmd &lt;- sprintf(&quot;fsl_sub -T 300 -t %s -l %s -M bellmund@cbs.mpg.de -N randomise_srchlghts&quot;, fn, htc_dir) batch_id &lt;- system(cmd, intern = TRUE) pause_until_batch_done(batch_id = batch_id, wait_interval = 600) Find searchlight clusters and atlas labels Next, we do an automated search in the Randomise results. This is done via FSL cluster and the result is stored in a text file. Then, we add atlas labels based on the Harvard-Oxford Cortical/Subcortial Structural Atlas using FSL atlasquery. Here is a function that runs the atlasquery command and removes some excess characters around the return string. find_FSL_atlas_label &lt;- function(atlas = &quot;Harvard-Oxford Cortical Structural Atlas&quot;, x=-20, y=-3, z=-26){ # build FSL atlasquery command and send to command line cmd&lt;- sprintf(&#39;atlasquery -a &quot;%s&quot; -c %s&#39;, atlas,sprintf(&quot;%f,%f,%f&quot;,x,y,z)) cmd&lt;-sprintf(&quot;%s | sed &#39;s/&lt;b&gt;//g&#39; | sed &#39;s/&lt;\\\\/b&gt;&lt;br&gt;/\\\\,/g&#39;&quot;, cmd) # based on FSL autoaq (l.106) string &lt;- system(cmd,intern=TRUE) # remove atlas name label &lt;- stringr::str_remove(string, sprintf(&quot;%s,&quot;, atlas)) return(label) } We extract clusters that are significant at p&lt;0.05 after correcting for multiple comparisons using small volume correction. To explore the searchlight results beyond the hippocampal-entorhinal region, we look for clusters at a threshold of p&lt;0.001 uncorrected with a minimum extent of 30 voxels. We corrected for multiple comparisons using a small volume correction mask including our a priori regions of interest, the anterior hippocampus and the anterior-lateral entorhinal cortex. Further, we used a liberal threshold of puncorrected&lt;0.001 to explore the data for additional effects within our field of view. Exploratory searchlight results are shown in Supplemental Figure 9 and clusters with a minimum extent of 30 voxels are listed in Supplemental Tables 9, 11 and 12. for (i_srchlght in 1:length(searchlights)){ # searchlight with positive or negative contrast? if (any(searchlights[i_srchlght] == c(&quot;same_day_vir_time&quot;, &quot;day_time_interaction&quot;))){ test_side &lt;- &quot;&quot; } else { test_side &lt;- &quot;_neg&quot; } # file with t-values t_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;%s%s_randomise_fov_fwhm%d_tstat1.nii.gz&quot;, searchlights[i_srchlght], test_side, FWHM)) # SVC-CORRECTED CLUSTERS # file with corrected p-values based on SVC corrp_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;%s%s_randomise_svc_fwhm%d_tfce_corrp_tstat1.nii.gz&quot;, searchlights[i_srchlght], test_side, FWHM)) # output text file for clusters significant after small volume correction cluster_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;cluster_%s%s_svc_corrp.txt&quot;, searchlights[i_srchlght], test_side, FWHM)) cmd &lt;- sprintf(&#39;cluster -i %s -t 0.95 -c %s --mm &gt; %s&#39;, corrp_fn, t_fn, cluster_fn) system(cmd) # read the cluster file cluster_df &lt;- readr::read_tsv(cluster_fn, col_types = c(&quot;dddddddddd____&quot;)) colnames(cluster_df) &lt;- c(&quot;cluster_index&quot;, &quot;n_voxel&quot;, &quot;max_1minusp&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;cog_x&quot;, &quot;cog_y&quot;, &quot;cog_z&quot;, &quot;t_extr&quot;) # add columns for atlas label info cluster_df &lt;- cluster_df %&gt;% tibble::add_column(Harvard_Oxford_Cortical = NA, .before = &quot;cluster_index&quot;) %&gt;% tibble::add_column(Harvard_Oxford_Subcortical = NA, .before = &quot;cluster_index&quot;) # get atlas label info if (nrow(cluster_df)&gt;0){ for (i in 1:nrow(cluster_df)){ cluster_df$Harvard_Oxford_Cortical[i] &lt;- find_FSL_atlas_label( atlas = &quot;Harvard-Oxford Cortical Structural Atlas&quot;, x=cluster_df$x[i], y=cluster_df$y[i], z=cluster_df$z[i]) cluster_df$Harvard_Oxford_Subcortical[i] &lt;- find_FSL_atlas_label( atlas = &quot;Harvard-Oxford Subcortical Structural Atlas&quot;, x=cluster_df$x[i], y=cluster_df$y[i], z=cluster_df$z[i]) } } # write to new file fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;cluster_%s%s_svc_corrp_atlas.txt&quot;, searchlights[i_srchlght], test_side, FWHM)) write_tsv(cluster_df, path=fn) # FOV CLUSTERS AT p&lt;0.001 UNCORRECTED # file with uncorrected p-values in entire FOV uncorrp_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;%s%s_randomise_fov_fwhm%d_tfce_p_tstat1.nii.gz&quot;, searchlights[i_srchlght], test_side, FWHM)) # output text file for clusters significant after small volume correction cluster_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;cluster_%s%s_fov_uncorrp.txt&quot;, searchlights[i_srchlght], test_side, FWHM)) cmd &lt;- sprintf(&#39;cluster -i %s -t 0.999 -c %s --mm --minextent=30 &gt; %s&#39;, uncorrp_fn, t_fn, cluster_fn) system(cmd) # read the cluster file cluster_df &lt;- readr::read_tsv(cluster_fn, col_types = c(&quot;dddddddddd____&quot;)) colnames(cluster_df) &lt;- c(&quot;cluster_index&quot;, &quot;n_voxel&quot;, &quot;max_1minusp&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;cog_x&quot;, &quot;cog_y&quot;, &quot;cog_z&quot;, &quot;t_extr&quot;) # add columns for atlas label info cluster_df &lt;- cluster_df %&gt;% tibble::add_column(Harvard_Oxford_Cortical = NA, .before = &quot;cluster_index&quot;) %&gt;% tibble::add_column(Harvard_Oxford_Subcortical = NA, .before = &quot;cluster_index&quot;) # get atlas label info for (i in 1:nrow(cluster_df)){ cluster_df$Harvard_Oxford_Cortical[i] &lt;- find_FSL_atlas_label( atlas = &quot;Harvard-Oxford Cortical Structural Atlas&quot;, x=cluster_df$cog_x[i], y=cluster_df$cog_y[i], z=cluster_df$cog_z[i]) cluster_df$Harvard_Oxford_Subcortical[i] &lt;- find_FSL_atlas_label( atlas = &quot;Harvard-Oxford Subcortical Structural Atlas&quot;, x=cluster_df$cog_x[i], y=cluster_df$cog_y[i], z=cluster_df$cog_z[i]) } # write to new file fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, searchlights[i_srchlght], sprintf(&quot;cluster_%s%s_fov_uncorrp_atlas.txt&quot;, searchlights[i_srchlght], test_side, FWHM)) write_tsv(cluster_df, path=fn) } Create outlines of significant clusters To later show which voxels survive corrections for multiple comparisons based on our small-volume correction, we create an outline of the clusters surviving at corrected p&lt;0.05. We do this by dilating a binarized mask of this effect with a spherical kernel with a radius of 2mm. voxels within black outline are significant after correction for multiple comparisons using small volume correction Same Day Event Pairs # to create an outline of the significant cluster, we threshold the small-volume corrected p-image # at 0.95 (i.e. 1-0.05) and binarize it. Then we dilate it using a spherical kernel. corrpsvc_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_svc_fwhm3_tfce_corrp_tstat1.nii.gz&quot;) outl_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_svc_fwhm3_tfce_corrp_outline.nii.gz&quot;) fslmaths(file=corrpsvc_fn,outfile = outl_fn, opts = &quot;-thr 0.95 -bin&quot;, retimg = FALSE, verbose = FALSE) outl_nii &lt;- fslmaths(file=outl_fn, outfile = outl_fn, opts = sprintf(&quot;-kernel sphere 2 -dilM -sub %s&quot;, outl_fn), verbose = FALSE) Sequence-Time Interaction # to create an outline of the significant cluster, we threshold the small-volume corrected p-image # at 0.95 (i.e. 1-0.05) and binarize it. Then we dilate it using a spherical kernel. corrpsvc_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_svc_fwhm3_tfce_corrp_tstat1.nii.gz&quot;) outl_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_svc_fwhm3_tfce_corrp_outline.nii.gz&quot;) fslmaths(file=corrpsvc_fn,outfile = outl_fn, opts = &quot;-thr 0.95 -bin&quot;, retimg = FALSE, verbose = FALSE) outl_nii &lt;- fslmaths(file=outl_fn, outfile = outl_fn, opts = sprintf(&quot;-kernel sphere 2 -dilM -sub %s&quot;, outl_fn), verbose = FALSE) 7.4 Prepare RSA in searchlight peak To show that overlapping clusters of voxels drive both the within- and across sequence effects, we will run the across-sequence analysis in the cluster of voxels that show the within-day effect. Because these are independent comparisons, we can use the within-day searchlight to define a region of interest for the across-day analysis. To test whether within- and across-sequence representations overlap, we defined an ROI based on the within-sequence searchlight analysis. Specifically, voxels belonging to the cluster around the peak voxel, thresholded at p&lt;0.01 uncorrected within our small volume correction mask, were included. The analysis of representational change was then carried out as described for the other ROIs above. Create the ROI mask The first steps are to prepare the ROI mask we want to use. Thus, we need to threshold the ROI from the main analysis in MNI space and move the resulting mask to each participant’s functional space for the analysis. # threshold the searchlight results in_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_svc_fwhm3_tfce_p_tstat1.nii.gz&quot;) svc_fn &lt;- file.path(dirs$mask_dir, &quot;svc&quot;, &quot;svc_mask.nii.gz&quot;) bin_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, &quot;searchlight_same-day_svc.nii.gz&quot;) fsl_thresh(file = in_fn, outfile = bin_fn, thresh = 0.99, opts = sprintf(&quot;-bin -mas %s&quot;, svc_fn), verbose = FALSE, retimg = FALSE) # peak cluster is in left hemisphere, so don&#39;t include any voxels in right hemisphere lh_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, &quot;left_hemi.nii.gz&quot;) fsl_maths(file=bin_fn, outfile = lh_fn, opts = &quot;-mul 0 -add 1 -roi 91 182 0 218 0 182 0 1&quot;, retimg = FALSE, verbose = FALSE) fsl_mul(file=bin_fn, file2=lh_fn, outfile=bin_fn, retimg = FALSE, verbose = FALSE) # let&#39;s have a look at the mask we created mni_nii &lt;- readNIfTI2(mni_fname(&quot;1&quot;)) roi_nii &lt;- readNIfTI2(bin_fn) coords &lt;- c(statip::mfv1(which(roi_nii==1, arr.ind = TRUE)[,1]), statip::mfv1(which(roi_nii==1, arr.ind = TRUE)[,2]), statip::mfv1(which(roi_nii==1, arr.ind = TRUE)[,3])) ortho2(mni_nii, y = roi_nii, xyz = coords, add.orient = TRUE) # check the number of voxels in this ROI sum(c(roi_nii)) The resulting ROI mask is now coregistered from MNI 1mm space to the analysis space of the wholebrain functional sequence. Finally, it is thresholded at a probability of 0.5. samespace_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;same-day_searchlight&quot;, &quot;samespace&quot;) if (!dir.exists(samespace_dir)){dir.create(samespace_dir, recursive = TRUE)} # name of transformation matrix file to move from highres to functional space standard2func &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;wholebrain&quot;, paste0(&quot;VIRTEM_P&quot;, subjects, &quot;.feat&quot;), &quot;reg&quot;, &quot;standard2example_func.mat&quot;) # use the mean EPI of wholebrain image as a reference mean_epi &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;samespace&quot;, paste0(&quot;VIRTEM_P&quot;, subjects), paste0(&quot;VIRTEM_P&quot;, subjects, &quot;_wholebrain.nii.gz&quot;)) # define output files in samespace (ss) = analysis space based on the wholebrain EPI roi_ss &lt;- file.path(samespace_dir, sprintf(&quot;P%s_%s_ss.nii.gz&quot;, subjects, &quot;same-day_searchlight&quot;)) # apply FSL flirt to move ROI from standard to wholebrain functional space invisible(mapply(flirt_apply, infile = bin_fn, reffile = mean_epi, initmat = standard2func, outfile = roi_ss, verbose = FALSE, retimg = FALSE)) # use fslmaths to binarize the masked ROIs using a threshold of 0.5 out &lt;- mapply(fsl_thresh, file = roi_ss, outfile = roi_ss, thresh = 0.5, opts = &quot;-bin&quot;, verbose = FALSE, retimg = FALSE) Calculating the correlation matrices The following steps are analogous to the preparation of the functional data in the ROI and searchlight analyses. For the main searchlight analyses we already cleaned the voxel-wise time series and extracted the volumes relevant to RSA. Thus, to calculate the correlation matrices and to calculate pattern similarity changes, we fall back onto the scripts from the main ROI analyses and the searchlight. # for all 10x10 comparisons we will be averaging all comparisons apart from the diagonal # to exclude same_block comparisons no_diag &lt;- matrix(data = TRUE, nrow=10, ncol = 10) diag(no_diag)&lt;- FALSE out_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;correlation_matrices&quot;, &quot;same-day_searchlight&quot;) if (!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} for (i_sub in subjects){ # load the ROI based on the searchlight roi_fn &lt;-file.path(samespace_dir, sprintf(&quot;P%s_%s_ss.nii.gz&quot;, i_sub, &quot;same-day_searchlight&quot;)) roi_nii &lt;- readNIfTI(roi_fn, reorient=FALSE) # array indices of ROI voxels roi_vox &lt;- which(roi_nii == 1, arr.ind=TRUE) sprintf(&quot;%s: %d voxels\\n&quot;, i_sub, nrow(roi_vox)) for (i_run in 1:n_runs){ # load the relevant functional volumes rel_vol_fn &lt;- file.path(dirs$searchlight, &quot;rel_vols_4D&quot;, sprintf(&quot;%s_run%02d_rel_vols.nii.gz&quot;, i_sub, i_run)) func_nii &lt;- readNIfTI(rel_vol_fn, reorient=FALSE) # get the ROI voxels rel_dat &lt;- array(NA, c(n_pics*n_blocks, nrow(roi_vox))); # images in rows (ROI voxels), voxels in columns for (i in 1:nrow(roi_vox)) { # i &lt;- 1 curr_vox &lt;- func_nii[roi_vox[i,1], roi_vox[i,2], roi_vox[i,3],] if (sd(curr_vox) &gt; 0) {rel_dat[,i] &lt;- curr_vox} else { stop(&quot;zero variance voxel&quot;)} } # data is in repetition (row) by voxel (col) format, so we transpose # to get a voxel x repetition format rel_dat &lt;- t(rel_dat) # calculate correlation matrix (trial by trial) for pre and post run cor_mat_trial &lt;- cor(rel_dat, rel_dat) # initialize condition by condition correlation matrix for pre and post run corr_mat &lt;- matrix(nrow = 20, ncol = 20) # loop over all picture comparisons for(i_pic1 in 1:20){ for(i_pic2 in 1:20){ # extract the current 10x10 correlation matrix i1 &lt;- (1+(i_pic1-1)*10):(i_pic1*10) i2 &lt;- (1+(i_pic2-1)*10):(i_pic2*10) curr_mat &lt;- cor_mat_trial[i1, i2] # average the correlations while excluding diagonal (same block comparisons) corr_mat[i_pic1, i_pic2] &lt;- mean(curr_mat[no_diag]) } } # save the correlation matrix fn &lt;- file.path(out_dir, sprintf(&quot;%s_%s_%s_corr_mat.txt&quot;, i_sub, &quot;same-day_searchlight&quot;, runs[i_run])) write.table(corr_mat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = FALSE) } } Calculate Pattern Similarity Change In the next step, we calculate how the correlation between patterns change from the first to the second picture viewing task, i.e. through the day learning task. To do this, we load both correlation matrices and reduce them to the upper triangle, excluding the diagonal. Then, the correlations are Fisher Z-transformed and the similarity values from the pre-learning picture viewing task are subtracted from those of the post-learning picture viewing task to isolate pattern similarity change. The correlations and their changes are then saved together with information about the pictures that were compared. This part is based on the corresponding script from the main ROI analyses. in_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;correlation_matrices&quot;, &quot;same-day_searchlight&quot;) out_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;pattern_similarity_change&quot;, &quot;same-day_searchlight&quot;) if (!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} for(i_sub in subjects){ # load pre correlation matrix fn &lt;- file.path(in_dir, sprintf(&quot;%s_%s_pre_corr_mat.txt&quot;, i_sub, &quot;same-day_searchlight&quot;)) corr_mat_pre &lt;- read.table(fn, sep = &quot;,&quot;, dec = &quot;.&quot;) # load post correlation matrix fn &lt;- file.path(in_dir, sprintf(&quot;%s_%s_post_corr_mat.txt&quot;, i_sub, &quot;same-day_searchlight&quot;)) corr_mat_post &lt;- read.table(fn, sep = &quot;,&quot;, dec = &quot;.&quot;) # reduce to upper triangle of correlations (without diagonal) pre_corrs &lt;- corr_mat_pre[upper.tri(corr_mat_pre, diag = FALSE)] post_corrs &lt;- corr_mat_post[upper.tri(corr_mat_post, diag = FALSE)] # create a tibble with the correlations pics &lt;- which(upper.tri(corr_mat_post), arr.ind=TRUE) corr_df &lt;- tibble(pic1 = pics[,1], pic2 = pics[,2], pre_corrs, post_corrs) # Fisher Z transform correlations and calculate pattern similarity change # by subtracting the pre-correlations from the post-correlations corr_df$ps_change &lt;- FisherZ(corr_df$post_corrs) - FisherZ(corr_df$pre_corrs) # write to file fn &lt;- file.path(out_dir, sprintf(&quot;%s_%s_pattern_similarity_change.txt&quot;, i_sub, &quot;same-day_searchlight&quot;)) write.table(corr_df, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) } Combine behavioral and fMRI data for RSA To create input for the RSA the next step is to combine the similarity change data with the behavioral data, so that we can do meaningful analyses. First the behavioral data is loaded and brought into the same pair-wise format as the similarity change data. Both datasets are combined for each subject and then written to disk. This part is based on the corresponding script from the main ROI analyses. in_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;pattern_similarity_change&quot;, &quot;same-day_searchlight&quot;) out_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;data_for_rsa&quot;) if (!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} for (i_sub in subjects){ # load the behavioral data fn &lt;- file.path(dirs$timeline_dat_dir, sprintf(&quot;%s_behavior_tbl_timeline.txt&quot;, i_sub)) col_types_list &lt;- cols_only(sub_id = col_character(), day = col_factor(), event = col_integer(), pic = col_integer(), virtual_time = col_double(), real_time = col_double(), memory_time = col_double(), memory_order = col_double(), sorted_day = col_integer()) beh_dat &lt;- read_csv(fn, col_types = col_types_list) # sort behavioral data according to picture identity beh_dat_ordered &lt;- beh_dat[order(beh_dat$pic),] # find the order of comparisons pairs &lt;- which(upper.tri(matrix(nrow = 20, ncol = 20)), arr.ind=TRUE) # extract the data for the first and second picture in each pair from the behavioral data pic1_dat &lt;- beh_dat_ordered[pairs[,1],] pic2_dat &lt;- beh_dat_ordered[pairs[,2],] colnames(pic1_dat) &lt;- paste0(colnames(pic1_dat), &quot;1&quot;) colnames(pic2_dat) &lt;- paste0(colnames(pic2_dat), &quot;2&quot;) # combine the two tibbles pair_dat &lt;- cbind(pic1_dat, pic2_dat) # reorder the tibble columns to make a bit more sense pair_dat &lt;- pair_dat %&gt;% select(sub_id1, day1, day2, pic1, pic2, event1, event2, virtual_time1, virtual_time2, real_time1, real_time2, memory_order1, memory_order2, memory_time1, memory_time2, sorted_day1, sorted_day2) %&gt;% rename(sub_id = sub_id1) rsa_dat &lt;- tibble() # load the pattern similarity change data for this ROI fn &lt;- file.path(in_dir, sprintf(&quot;%s_%s_pattern_similarity_change.txt&quot;, i_sub, &quot;same-day_searchlight&quot;)) ps_change_dat &lt;- read.csv(fn) # make sure files have the same order assertthat::are_equal(c(pair_dat$pic1, pair_dat$pic2), c(ps_change_dat$pic1, ps_change_dat$pic2)) # add column with ROI name ps_change_dat &lt;- add_column(ps_change_dat, roi = &quot;same-day_searchlight&quot;) # collect the data from this ROI and merge into long data frame roi_dat &lt;- cbind(pair_dat, ps_change_dat[,3:6]) rsa_dat &lt;- rbind(rsa_dat, roi_dat) # write to file fn &lt;- file.path(dirs$rsa_dat_dir, sprintf(&quot;%s_data_for_rsa_same-day_searchlight.txt&quot;,i_sub)) write.table(rsa_dat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) } Finally, the datasets are combined across subjects. # set up a dataframe to collect the data rsa_dat = tibble() for (i_sub in subjects){ # load data from CSV fn &lt;- file.path(out_dir, sprintf(&quot;%s_data_for_rsa_same-day_searchlight.txt&quot;,i_sub)) col_types_list &lt;- cols_only( sub_id = col_character(), day1 = col_integer(), day2 = col_integer(), event1 = col_integer(), event2 = col_integer(), pic1 = col_integer(), pic2 = col_integer(), virtual_time1 = col_double(), virtual_time2 = col_double(), real_time1 = col_double(), real_time2 = col_double(), memory_time1 = col_double(), memory_time2 = col_double(), memory_order1 = col_double(), memory_order2 = col_double(), sorted_day1 = col_integer(), sorted_day2 = col_integer(), pre_corrs = col_double(), post_corrs = col_double(), ps_change = col_double(), roi = col_factor() ) sub_dat &lt;- as_tibble(read_csv(fn, col_types = col_types_list)) # append to table with data from all subjects rsa_dat &lt;- bind_rows(sub_dat, rsa_dat) } # sort the data rsa_dat &lt;- rsa_dat[with(rsa_dat, order(sub_id, day1, day2, event1, event2)),] # write to file fn &lt;- file.path(dirs$data4analysis, &quot;rsa_data_in_same-seq_searchlight_peak.txt&quot;) write.table(rsa_dat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) "],
["rsa-on-pattern-similarity-change.html", "8 RSA on Pattern Similarity Change 8.1 Time metrics to quantify event relations 8.2 Visualize Regions of Interests 8.3 First-level RSA 8.4 aHPC: virtual time within sequence 8.5 aHPC: virtual time within sequence when including order &amp; real time 8.6 aHPC: virtual time across sequences 8.7 aHPC: virtual time across sequence when including order &amp; real time 8.8 aHPC: virtual time within vs. across sequences 8.9 alEC 8.10 aHPC vs. alEC 8.11 Searchlight Results 8.12 Correlation of behavioral generalization bias with searchlight effects 8.13 Mixed Model Summaries", " 8 RSA on Pattern Similarity Change In order to assess changes in representational similarity between the two picture viewing tasks, we quantified pattern similarity changes as the difference of the respective correlation coefficients for every pair of events between the post-learning picture viewing task and its pre-learning baseline equivalent (Figure 3). Then, we analyzed how these difference values related to temporal relations between events, which we quantified using the absolute distances in virtual time (“virtual time”) between events (Figure 1C, bottom right). We further tested whether the effect of virtual time on anterior hippocampal pattern similarity change persisted when including the absolute difference between sequence positions (“order”) and the interval in seconds between events (“real time”) as control predictors of no interest in the model. Time metrics were z-scored within each participant prior to analysis. We separately tested the effect of virtual time for event pairs from the same or different sequences and used a Bonferroni-corrected α-level of 0.025 for these tests. Time metrics were z-scored within each participant prior to analysis. Next, we can run our analyses of representational change in the ROIs. Let’s load the data. # load the data col_types_list &lt;- cols_only( sub_id = col_character(), day1 = col_integer(), day2 = col_integer(), event1 = col_integer(), event2 = col_integer(), pic1 = col_integer(), pic2 = col_integer(), virtual_time1 = col_double(), virtual_time2 = col_double(), real_time1 = col_double(), real_time2 = col_double(), memory_time1 = col_double(), memory_time2 = col_double(), memory_order1 = col_double(), memory_order2 = col_double(), sorted_day1 = col_integer(), sorted_day2 = col_integer(), pre_corrs = col_double(), post_corrs = col_double(), ps_change = col_double(), roi = col_factor()) fn &lt;- file.path(dirs$data4analysis, &quot;rsa_data_rois.txt&quot;) rsa_dat &lt;- as_tibble(read_csv(fn, col_types = col_types_list)) head(rsa_dat) sub_idday1day2pic1pic2event1event2virtual_time1virtual_time2real_time1real_time2memory_order1memory_order2memory_time1memory_time2sorted_day1sorted_day2pre_corrspost_corrsps_changeroi 031111220129.2511.8&nbsp;7.8123.4&nbsp;129.4511.5&nbsp;110.00116&nbsp;-0.00297&nbsp;-0.00413aHPC_lr 031111220129.2511.8&nbsp;7.8123.4&nbsp;129.4511.5&nbsp;110.0005290.0265&nbsp;&nbsp;0.026&nbsp;&nbsp;alEC_lr 031111219159.2518.2&nbsp;7.8164.1&nbsp;159.4519&nbsp;&nbsp;&nbsp;11-0.0004240.0009380.00136aHPC_lr 031111219159.2518.2&nbsp;7.8164.1&nbsp;159.4519&nbsp;&nbsp;&nbsp;11-0.00318&nbsp;-0.0005910.00259alEC_lr 031112123114.8&nbsp;9.2542.2&nbsp;7.813114.5&nbsp;9.45110.00164&nbsp;-0.00198&nbsp;-0.00362aHPC_lr 031112123114.8&nbsp;9.2542.2&nbsp;7.813114.5&nbsp;9.4511-0.0102&nbsp;&nbsp;-0.0121&nbsp;&nbsp;-0.00184alEC_lr 8.1 Time metrics to quantify event relations After loading the data, we first calculate some temporal distance measures for the comparisons of event pairs. These include: virtual time difference order difference scanner time difference These temporal distance measures will be used as predictor variables for model-based representational similarity analysis. They are z-scored within each participant. Importantly, we will analyze event pairs from the same virtual day or from different virtual days in separate analyses. Thus, we also create a corresponding variable that marks whether comparisons are from the same sequence or not. # create predictors for RSA rsa_dat &lt;- rsa_dat %&gt;% mutate( # pair of events from say or different day (dv = deviation code) same_day = day1 == day2, same_day_dv = plyr::mapvalues(same_day, from = c(FALSE, TRUE), to = c(-1, 1)), # absolute difference in time metrics vir_time_diff = abs(virtual_time1 - virtual_time2), #vir_time_diff = abs(memory_time1 - memory_time2), order_diff = abs(event1 - event2), #order_diff = abs(memory_order1 - memory_order2), real_time_diff = abs(real_time1 - real_time2), group = factor(1)) %&gt;% # z-score the time metric predictors (within each subject) group_by(sub_id) %&gt;% mutate_at( c(&quot;vir_time_diff&quot;, &quot;order_diff&quot;, &quot;real_time_diff&quot;), scale, scale = TRUE) %&gt;% ungroup() 8.2 Visualize Regions of Interests Template background image with FOV As a background image we use the 1mm MNI template together with a visualization of the field of view of our scans. # 1 mm MNI template as background image mni_fn &lt;- file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;, &quot;MNI152_T1_1mm_brain.nii.gz&quot;) mni_nii &lt;- readNIfTI2(mni_fn) # load FOV mask and binarize it fov_fn &lt;- file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;, &quot;fov_mask_mni.nii.gz&quot;) fov_nii &lt;- readNIfTI2(fov_fn) fov_nii[fov_nii &gt;0] &lt;- 1 fov_nii[fov_nii !=1 ] &lt;- 0 # make a mask for the brain area outside our FOV out_fov &lt;- (fov_nii == 0) &amp; (mni_nii&gt;0) mni_nii[out_fov] &lt;- scales::rescale(mni_nii[out_fov], from=range(mni_nii[out_fov]), to=c(6000, 8207)) mni_nii[mni_nii == 0] &lt;- NA Anterior hippocampus ROI # load the ROI roi_fn &lt;- file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;, &quot;aHPC_lr_group_prob_mni1mm.nii.gz&quot;) roi_nii &lt;- readNIfTI2(roi_fn) roi_nii[roi_nii == 0] &lt;- NA # choose coordinates (these are chosen manually now) coords &lt;- mni2vox(c(-24,-15,-20)) # f4a # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,2,3), mar_ind=coords, col_ind = c(1,1,1), row_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), center_coords=TRUE) # get data frame for the ROI image roi_df &lt;- getBrainFrame(brains=roi_nii, mar=c(1,2,3), row_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), #row_ind = c(1,2,3), col_ind = c(1,1,1), mar_ind=coords, mask=NULL, center_coords=TRUE) label_df &lt;- data.frame(row_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords))) f4a &lt;- ggTemplate + geom_tile(data=roi_df, aes(x=row,y=col,fill=value))+ facet_wrap(~row_ind, nrow=3, ncol=1, strip.position = &quot;top&quot;, scales = &quot;free&quot;) + scico::scale_fill_scico(palette = &quot;roma&quot;, begin = 0, end=1, name = &quot;probability&quot;, limits = round(range(roi_df$value), digits=2), breaks = round(range(roi_df$value), digits=2)) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), text = element_text(size=10), axis.text = element_text(size=8), legend.key.size = unit(0.015, &quot;npc&quot;), legend.position = &quot;bottom&quot;, legend.justification = c(0.5, 0.5), aspect.ratio = 1)+ guides(fill = guide_colorbar( direction = &quot;horizontal&quot;, title.position = &quot;top&quot;, title.hjust = 0.5, label.position = &quot;bottom&quot; ))+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=c(0,0,0), y=c(-84.7,-67.25,-84.45), aes(label = label), size = 8/.pt, family=font2use, vjust=1) + # y coords found via command below (to keep text aligned across panels): # ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range[1]-diff(ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range)/100*3 theme_no_ticks() print(f4a) # save source data file.copy(from = roi_fn, to = file.path(dirs$source_dat_dir, &quot;f4a.nii.gz&quot;)) ## [1] TRUE Anterior-lateral entorhinal cortex # load the ROI roi_fn &lt;- file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;, &quot;alEC_lr_group_prob_mni1mm.nii.gz&quot;) roi_nii &lt;- readNIfTI2(roi_fn) roi_nii[roi_nii == 0] &lt;- NA # choose coordinates (these are chosen manually now) coords &lt;- mni2vox(c(18,1,-35)) # f6a # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,2,3), mar_ind=coords, col_ind = c(1,1,1), row_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), center_coords=TRUE) #row_ind=c(1,2,3), center_coords=TRUE) # get data frame for the ROI image roi_df &lt;- getBrainFrame(brains=roi_nii, mar=c(1,2,3), row_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), #row_ind = c(1,2,3), col_ind = c(1,1,1), mar_ind=coords, mask=NULL, center_coords=TRUE) label_df &lt;- data.frame(row_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords))) f6a &lt;- ggTemplate + geom_tile(data=roi_df, aes(x=row,y=col,fill=value))+ facet_wrap(~row_ind, nrow=1, ncol=3, strip.position = &quot;top&quot;, scales = &quot;free&quot;) + scico::scale_fill_scico(palette = &quot;roma&quot;, begin = 0, end=1, name = element_blank(), limits = round(range(roi_df$value), digits=2), breaks = round(range(roi_df$value), digits=2)) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), text = element_text(size=10), axis.text = element_text(size=8), legend.key.size = unit(0.015, &quot;npc&quot;), legend.position = &quot;left&quot;, legend.justification = c(0.5, 0.5), aspect.ratio = 1 )+ guides(fill = guide_colorbar( direction = &quot;vertical&quot;, title.position = &quot;top&quot;, title.hjust = 0.5, label.position = &quot;right&quot;))+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=c(0,0,0), y=c(-85.82,-72.31,-78.1), aes(label = label), size = 8/.pt, family=font2use, vjust=1) + # y coords found via command below (to keep text aligned across panels): # ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range[1]-diff(ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range)/100*3 theme_no_ticks() print(f6a) # save source data file.copy(from = roi_fn, to = file.path(dirs$source_dat_dir, &quot;f6a.nii.gz&quot;)) ## [1] TRUE 8.3 First-level RSA In the summary statistics approach, we used the different time metrics as predictors for pattern similarity change. We set up a GLM with the given variable from the day learning task as a predictor and the pairwise representational change values as the criterion for every participant. The t-values of the resulting model coefficients were then compared to a null distribution obtained from shuffling the dependent variable of the linear model (i.e. pattern similarity change) 10,000 times. This approach to permutation-testing of regression coefficients controls Type I errors even under situations of collinear regressors106. Resulting p-values for each coefficient were transformed to a Z-score. The Z-scores were then used for group-level inferential statistics. Group-level statistics were carried out using permutation-based procedures. For t-tests, we compared the observed t-values against a surrogate distribution obtained from 10,000 random sign-flips to non-parametrically test against 0 or to assess within-participant differences between conditions (two-sided tests; α=0.05 unless stated otherwise). We report Cohen’s d with Hedges’ correction and its 95% confidence interval as computed using the effsize-package for R. For paired tests, Cohen’s d was calculated using pooled standard deviations and confidence intervals are based on the non-central t-distribution. Permutation-based repeated measures ANOVAs were carried out using the permuco-package107 and we report generalized η2 as effect sizes computed using the afex-package(108). For the summary statistics approach, let’s run the first-level analysis using virtual time as a predictor of pattern similarity change separately for each participant and each ROI. For simplicity we run these analysis together here and will run separate group-level stats below. set.seed(100) # set seed for reproducibility # select the data from alEC and HPC rsa_dat_mtl &lt;- rsa_dat %&gt;% filter(roi == &quot;alEC_lr&quot; | roi == &quot;aHPC_lr&quot;) # do RSA using linear model and calculate z-score for model fit based on permutations rsa_fit &lt;- rsa_dat_mtl %&gt;% group_by(sub_id, roi, same_day) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;ps_change ~ vir_time_diff&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;)))) %&gt;% unnest_wider(z) # add group column used for plotting rsa_fit$group &lt;- factor(1) # add a factor with character labels for within/across days and one to later color control in facets rsa_fit &lt;- rsa_fit %&gt;% mutate(same_day_char = plyr::mapvalues(same_day, from = c(0, 1), to = c(&quot;across days&quot;, &quot;within days&quot;), warn_missing = FALSE), same_day_char = factor(same_day_char, levels = c(&quot;within days&quot;, &quot;across days&quot;))) %&gt;% unite(roi_same_day, c(&quot;roi&quot;, &quot;same_day_char&quot;), remove = FALSE, sep = &quot; &quot;) %&gt;% mutate(roi_same_day = str_replace(roi_same_day, &quot;_lr&quot;, &quot;:&quot;), roi_same_day = factor(roi_same_day, levels = c(&quot;aHPC: within days&quot;, &quot;aHPC: across days&quot;, &quot;alEC: within days&quot;, &quot;alEC: across days&quot;)) ) 8.4 aHPC: virtual time within sequence We first test, whether virtual time explains pattern similarity change within the aHPC for event pairs from the same sequence. Summary statistics set.seed(101) # set seed for reproducibility # run a group-level t-test on the RSA fits from the first level in aHPC for within-day stats &lt;- rsa_fit %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE) %&gt;% select(z_virtual_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE))$z_virtual_time, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 0.6183.070.00480.006270.2051.03One Sample t-testtwo.sided0.5640.1780.995 Summary Statistics: t-test against 0 for virtual time within sequence in aHPC t27=3.07, p=0.006, d=0.56, 95% CI [0.18, 1.00] # select the data to plot plot_dat &lt;- rsa_fit %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE) # raincloud plot f4b &lt;- ggplot(plot_dat, aes(x=1, y=z_virtual_time, fill = roi, color = roi)) + gghalves::geom_half_violin(position=position_nudge(-0.1), side = &quot;l&quot;, color = NA) + geom_point(aes(x = 1.2, y = z_virtual_time), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(-0.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(-0.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ scale_color_manual(values=unname(aHPC_colors[&quot;within_main&quot;])) + scale_fill_manual(values=unname(aHPC_colors[&quot;within_main&quot;])) + ylab(&#39;z RSA model fit&#39;) + xlab(&quot;virtual time&quot;) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;**&quot;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;, axis.ticks.x = element_blank(), axis.text.x = element_blank()) f4b # save source data source_dat &lt;-ggplot_build(f4b)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f4b.txt&quot;)) Linear Mixed Effects Likewise, we want to test whether similarity change reflects virtual time differences between elements within a sequence using a mized model. Thus, we use virtual time differences between scenes as the only fixed effect. Following the recommendation for maximal random effect structures by Barr et al. (JML, 2013), we first include random intercepts and random slopes for each subject. This results in a singular fit (random intercept variance estimated as 0). To avoid anti-conservativity we thus drop the correlation between random intercepts and random slopes as suggested by Barr et al., who demonstrate that “LMEMs with maximal random slopes, but missing either random correlations or within-unit random intercepts, performed nearly as well as “fully” maximal LMEMs, with the exception of the case where p-values were determined by MCMC sampling” (p. 267). This model converges without warnings. Below are the model summaries. set.seed(56) # set seed for reproducibility # extract the comparisons from the same day rsa_dat_same_day &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE) # define the full model with virtual time difference as # fixed effect and random intercepts for subject # maximal random effects structure with random slopes and intercepts --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff + (1 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE, control=lmerControl(optCtrl=list(maxfun=20000))) ## boundary (singular) fit: see ?isSingular # thus we reduce the random effects structure. Following Barr et al. (2013), we drop the correlation between random intercepts and random slopes set.seed(345) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff + (1 + vir_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE, control=lmerControl(optCtrl=list(maxfun=20000))) summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff + ((1 | sub_id) + (0 + vir_time_diff | sub_id)) ## Data: rsa_dat_same_day ## Control: lmerControl(optCtrl = list(maxfun = 20000)) ## ## AIC BIC logLik deviance df.resid ## -7951.4 -7926.3 3980.7 -7961.4 1115 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.4983 -0.6232 -0.0284 0.6645 3.8269 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id (Intercept) 6.434e-13 8.021e-07 ## sub_id.1 vir_time_diff 6.598e-08 2.569e-04 ## Residual 4.784e-05 6.917e-03 ## Number of obs: 1120, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -0.0003261 0.0002112 -1.544 ## vir_time_diff 0.0007513 0.0002197 3.420 # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) We assess the statistical significance of the fixed effect of virtual time using a likelihood ratio test against a reduced (nested) model that does not have this fixed effect. # one way of testing for significance is by comparing the likelihood against a simpler model. # Here, we drop the effect of virtual time difference and run an ANOVA. See e.g. Bodo Winter tutorial set.seed(348) # set seed for reproducibility formula &lt;- &quot;ps_change ~ 1 + (1 + vir_time_diff || sub_id)&quot; lmm_no_vir_time &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular lmm_aov &lt;- anova(lmm_no_vir_time, lmm_full) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 4-7.94e+03-7.92e+033.98e+03-7.95e+03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5-7.95e+03-7.93e+033.98e+03-7.96e+039.8710.00168 Mixed Model: Fixed effect of virtual time on aHPC pattern similarity change \\(\\chi^2\\)(1)=9.87, p=0.002 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,2), &quot;residual&quot;) re_names &lt;- c(&quot;intercept (SD)&quot;, &quot;virtual time&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Virtual time explains representational change for same-sequence events in the anterior hippocampus&quot;) # convert the huxtable to a flextable for word export stable_lme_aHPC_virtime_same_seq &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Virtual time explains representational change for same-sequence events in the anterior hippocampus fixed effects termestimateSEt-value95% CI intercept-0.0003260.000211-1.54-0.0007400.000088 virtual time0.0007510.0002203.420.0003070.001196 random effects grouptermestimate participantintercept (SD)0.000001 participantvirtual time0.000257 residualSD0.006917 model comparison modelnparAICLLX2dfp reduced model4-7943.563975.78 full model5-7951.433980.729.8710.002 model: ps_change~vir_time_diff+((1|sub_id)+(0+vir_time_diff|sub_id)); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation To visualize this effect, we plot the estimate for the fixed effect coefficient of virtual time together with its confidence intervals. Further we plot the estimated marginal means for illustration. lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=as.factor(term)) # dot plot of Fixed Effect Coefficients with CIs sfigmm_c &lt;- ggplot(data = lmm_full_bm[2,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), shape = 16, size = 1) + scale_fill_manual(values = unname(aHPC_colors[&quot;within_main&quot;])) + scale_color_manual(values = unname(aHPC_colors[&quot;within_main&quot;]), labels = c(&quot;virtual time (same seq)&quot;)) + scale_y_continuous(breaks = seq(from=0, to=0.00125, by=0.00025), labels = c(&quot;0.000&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, 0.001, &quot;&quot;)) + labs(x = &quot;virtual time&quot;, y=&quot;fixed effect\\ncoefficient&quot;, color = &quot;Time Metric&quot;) + guides(color = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;**&quot;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) sfigmm_c # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% get_complete_df() # plot marginal means sfigmm_d &lt;- ggplot(data = lmm_full_emm, aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, linetype=0) + scale_color_manual(values = unname(aHPC_colors[&quot;within_main&quot;])) + scale_fill_manual(values = unname(aHPC_colors[&quot;within_main&quot;])) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;z virtual time&#39;) + guides(color = &quot;none&quot;, fill = &quot;none&quot;) + theme_cowplot() sfigmm_d LME model assumptions lmm_diagplots_jb(lmm_full) Representational Change Visualization To illustrate the effect in a simpler way, we average pattern similarity changes based on a median split of events, separately for events from the same and from different sequences. # get data from aHPC rsa_dat_hpc &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;) # for each subject, ROI and same/diff day comparisons dichotomize virtual time difference # based on a median split, then average high/low temporal distances ps_change_means &lt;-rsa_dat_hpc %&gt;% group_by(sub_id, same_day) %&gt;% mutate(temp_dist = sjmisc::dicho(vir_time_diff, as.num = TRUE)+1) %&gt;% group_by(sub_id, same_day, temp_dist) %&gt;% summarise(ps_change = mean(ps_change), .groups = &#39;drop&#39;) # make the high/low temporal distance variable a factor ps_change_means &lt;- ps_change_means %&gt;% mutate( temp_dist = as.numeric(temp_dist), temp_dist_f = factor(temp_dist), temp_dist_f = plyr::mapvalues(temp_dist, from = c(1,2), to = c(&quot;low&quot;, &quot;high&quot;)), temp_dist_f = factor(temp_dist_f, levels= c(&quot;low&quot;, &quot;high&quot;)), same_day = as.factor(same_day), same_day = plyr::mapvalues(same_day, from = c(&quot;FALSE&quot;, &quot;TRUE&quot;), to = c(&quot;different day&quot;, &quot;same day&quot;)), same_day = factor(same_day, levels = c(&quot;same day&quot;, &quot;different day&quot;))) # Test pattern similarity for same-sequence events separated by high vs. low temporal distances set.seed(208) # set seed for reproducibility stats_same_day_high_low &lt;- paired_t_perm_jb(ps_change_means[ps_change_means$same_day==&quot;same day&quot; &amp; ps_change_means$temp_dist_f == &quot;high&quot;,]$ps_change - ps_change_means[ps_change_means$same_day==&quot;same day&quot; &amp; ps_change_means$temp_dist_f == &quot;low&quot;,]$ps_change) # Cohen&#39;s d with Hedges&#39; correction for paired samples using non-central t-distribution for CI d&lt;-cohen.d(d=ps_change_means[ps_change_means$same_day==&quot;same day&quot; &amp; ps_change_means$temp_dist_f == &quot;high&quot;,]$ps_change, f=ps_change_means[ps_change_means$same_day==&quot;same day&quot; &amp; ps_change_means$temp_dist_f == &quot;low&quot;,]$ps_change, paired=TRUE, pooled=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats_same_day_high_low$d &lt;- d$estimate stats_same_day_high_low$dCI_low &lt;- d$conf.int[[1]] stats_same_day_high_low$dCI_high &lt;- d$conf.int[[2]] huxtable(stats_same_day_high_low) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 0.0012.480.01970.0201270.0001730.00183One Sample t-testtwo.sided0.6370.07520.871 To visualize the effect, lets make a plot of the raw pattern similarity change for high vs. low temporal distances. # add a column with subject-specific jitter # generate custom repeating jitter (multiply with -1/1 to shift towards each other) ps_change_means &lt;- ps_change_means %&gt;% mutate( x_jit = as.numeric(temp_dist) + rep(jitter(rep(0,n_subs), amount = 0.05), each=4) * rep(c(-1,1),n_subs)) f4c &lt;- ggplot(data=ps_change_means %&gt;% filter(same_day == &quot;same day&quot;), aes(x=temp_dist, y=ps_change, fill = same_day, color = same_day)) + gghalves::geom_half_violin(data = ps_change_means %&gt;% filter(same_day == &quot;same day&quot;, temp_dist_f == &quot;low&quot;), aes(x=temp_dist, y=ps_change), position=position_nudge(-0.1), side = &quot;l&quot;, color = NA) + gghalves::geom_half_violin(data = ps_change_means %&gt;% filter(same_day == &quot;same day&quot;, temp_dist_f == &quot;high&quot;), aes(x=temp_dist, y=ps_change), position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_boxplot(aes(group = temp_dist),width = .1, colour = &quot;black&quot;, outlier.shape = NA) + geom_line(aes(x = x_jit, group=sub_id,), color = ultimate_gray, position = position_nudge(c(0.15, -0.15))) + geom_point(aes(x=x_jit), position = position_nudge(c(0.15, -0.15)), shape=16, size = 1) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;, width = 0, size = 0.5) + scale_fill_manual(values = unname(aHPC_colors[&quot;within_main&quot;])) + scale_color_manual(values = unname(aHPC_colors[&quot;within_main&quot;]), name = &quot;same sequence&quot;, labels = &quot;virtual time&quot;) + scale_x_continuous(breaks = c(1,2), labels=c(&quot;low&quot;, &quot;high&quot;)) + annotate(geom = &quot;text&quot;, x = 1.5, y = Inf, label = &#39;underline(&quot; * &quot;)&#39;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use, parse=TRUE) + ylab(&#39;pattern similarity change&#39;) + xlab(&#39;virtual temporal distance&#39;) + guides(fill= &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2), title.position = &quot;left&quot;)) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;right&quot;, strip.background = element_blank(), strip.text = element_blank()) # save source data source_dat &lt;-ggplot_build(f4c)$data[[4]] readr::write_tsv(source_dat %&gt;% select(x,y, group), file = file.path(dirs$source_dat_dir, &quot;f4c.txt&quot;)) Summary Statistics: paired t-test for aHPC pattern similarity changes for high vs. low temporal distances for same sequence events t27=2.48, p=0.020, d=0.64, 95% CI [0.08, 0.87] Not driven by first &amp; last event pairs In contrast to our previous work(21), where we observed negative correlations of pattern similarity and temporal distances, participants learned multiple sequences in this study. They might have formed strong associations of same-sequence events on top of inferring each event’s virtual time, potentially altering how temporal distances affected hippocampal pattern similarity (see Discussion). The effect of virtual temporal distances on pattern similarity changes remained significant when competing for variance with a control predictor accounting for comparisons of the first and last event of each sequence (Supplemental Figure 5A-C; summary statistics: t27=2.25, p=0.034, d=0.41, 95%% CI [0.04, 0.82]; mixed model: χ2(1)=5.36, p=0.021, Supplemental Table 3). Thus, the relationship of hippocampal event representations and temporal distances is not exclusively driven by associations of the events marking the transitions between sequences. Now, let’s see if the within-day hippocampus effect goes beyond the effect of the most extreme comparisons, i.e. comparisons between the first and last day of an event. These could for example be used by participants to chunk the events into virtual days. For that we create a binary variable (deviation-coded) that tells us whether a comparison is between the first and last event of a virtual day. Summary Statistics In the summary statistics approach, let’s test a multiple regression model that includes the virtual time predictor and a binary predictor for those first/last event pairs. The code below implements the first- and second-level analysis. set.seed(57) # set seed for reproducibility # extract all comparisons from the same day rsa_dat_same_day_aHPC &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE) # find comparisons of first and last event, which are the comparisons of events 1 and 5 # that have an order difference of 4 rsa_dat_same_day_aHPC$first_last &lt;- abs(rsa_dat_same_day_aHPC$event1 - rsa_dat_same_day_aHPC$event2) == 4 # do RSA using linear model and calculate z-score for model fit from permutations rsa_fit_aHPC_first_last &lt;- rsa_dat_same_day_aHPC %&gt;% group_by(sub_id) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;ps_change ~ vir_time_diff + first_last&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;, &quot;z_first_last&quot;)))) %&gt;% unnest_longer(z) %&gt;% filter(z_id != &quot;z_intercept&quot;) # run group-level t-tests on the RSA fits from the first level in aHPC for within-day stats &lt;- rsa_fit_aHPC_first_last %&gt;% filter(z_id == &quot;z_virtual_time&quot;) %&gt;% select(z) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit_aHPC_first_last %&gt;% filter(z_id==&quot;z_virtual_time&quot;))$z, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 0.4242.250.03270.0341270.03750.811One Sample t-testtwo.sided0.4130.03520.824 Summary Statistics: t-test against 0 for virtual time within sequence in aHPC with first-last-event-pair control predictor in the model t27=2.25, p=0.034, d=0.41, 95% CI [0.04, 0.82] # select the data to plot plot_dat &lt;- rsa_fit_aHPC_first_last %&gt;% mutate(z_id = factor(z_id, levels = c(&quot;z_virtual_time&quot;, &quot;z_first_last&quot;))) # raincloud plot sfiga &lt;- ggplot(plot_dat, aes(x=z_id, y=z, fill = z_id, color = z_id)) + gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_point(aes(x = as.numeric(z_id)-0.2, y = z), position = position_jitter(width = .1, height = 0), shape=16, size = 1, alpha = 0.7) + geom_boxplot(position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ scale_color_manual(values=unname(c(aHPC_colors[&quot;within_main&quot;], ultimate_gray)), name = &quot;RSA predictor&quot;, labels = c(&quot;virtual time&quot;, &quot;first &amp; last event pair&quot;)) + scale_fill_manual(values=unname(c(aHPC_colors[&quot;within_main&quot;], ultimate_gray))) + scale_x_discrete(labels=c(&quot;virtual time&quot;, &quot;first &amp; last&quot;)) + ylab(&#39;z RSA model fit&#39;) + xlab(&#39;predictor&#39;) + guides(fill = &quot;none&quot;, color = guide_legend(override.aes=list(fill=NA, alpha = 1, size=2))) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.direction = &quot;horizontal&quot;, legend.position = &quot;bottom&quot;, legend.justification = &quot;center&quot;) sfiga Linear Mixed Effects Now, let’s do the same using a mixed model. We add the first-last variable as a fixed effect to the model in addition to virtual time to ensure that the effect of virtual time is not exclusively driven by comparisons of the first and last events. The model converges without warnings only with a random effects structure reduced to random slopes for virtual time. set.seed(58) # set seed for reproducibility # extract the comparisons from the same day and add variable indicating whether comparison # is between first and last event of a day rsa_dat_same_day &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE) %&gt;% mutate(first_last = (as.numeric(abs(event1 - event2) == 4)-0.5)*2) # define the full model with virtual time difference and the extreme pair predictor as # fixed effects and random intercepts for subject # maximal random effects structure with random slopes and intercepts --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff + first_last + (1 + vir_time_diff + first_last | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE, control=lmerControl(optCtrl=list(maxfun=20000))) ## boundary (singular) fit: see ?isSingular # thus we remove random intercepts for first-last control predictor -&gt; still singular set.seed(349) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff + first_last + (1 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE, control=lmerControl(optCtrl=list(maxfun=90000))) ## boundary (singular) fit: see ?isSingular # thus we also remove the correlation between random slopes for virtual time and random intercepts set.seed(349) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff + first_last + (1 + vir_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE, control=lmerControl(optCtrl=list(maxfun=90000))) summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff + first_last + ((1 | sub_id) + (0 + vir_time_diff | sub_id)) ## Data: rsa_dat_same_day ## Control: lmerControl(optCtrl = list(maxfun = 90000)) ## ## AIC BIC logLik deviance df.resid ## -7950.2 -7920.0 3981.1 -7962.2 1114 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.5046 -0.6260 -0.0376 0.6554 3.8528 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id (Intercept) 7.365e-13 8.582e-07 ## sub_id.1 vir_time_diff 6.680e-08 2.585e-04 ## Residual 4.781e-05 6.914e-03 ## Number of obs: 1120, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -1.523e-05 4.207e-04 -0.036 ## vir_time_diff 6.257e-04 2.644e-04 2.367 ## first_last 3.568e-04 4.177e-04 0.854 # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # compare against a reduced model set.seed(301) # set seed for reproducibility formula &lt;- &quot;ps_change ~ first_last + (1 + vir_time_diff || sub_id)&quot; lmm_no_vir_time &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular lmm_aov &lt;- anova(lmm_no_vir_time, lmm_full) Mixed Model: Fixed effect of virtual time on aHPC pattern similarity change with first-last-event-pair control predictor \\(\\chi^2\\)(1)=5.36, p=0.021 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;, &quot;first-last pair&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,2), &quot;residual&quot;) re_names &lt;- c(&quot;intercept (SD)&quot;, &quot;virtual time (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Virtual time explains representational change for same-sequence events in the anterior hippocampus when controlling for the effect of first-last event pairs&quot;) # convert the huxtable to a flextable for word export stable_lme_aHPC_virtime_same_seq_first_last &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Virtual time explains representational change for same-sequence events in the anterior hippocampus when controlling for the effect of first-last event pairs fixed effects termestimateSEt-value95% CI intercept-0.0000150.000421-0.04-0.0008410.000810 virtual time0.0006260.0002642.370.0000990.001152 first-last pair0.0003570.0004180.85-0.0004620.001176 random effects grouptermestimate participantintercept (SD)0.000001 participantvirtual time (SD)0.000258 residualSD0.006914 model comparison modelnparAICLLX2dfp reduced model5-7946.813978.40 full model6-7950.163981.085.3610.021 model: ps_change~vir_time_diff+first_last+((1|sub_id)+(0+vir_time_diff|sub_id)); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation LME Plots. # let&#39;s plot the effect lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=factor(term, levels = c(&quot;vir_time_diff&quot;, &quot;first_last&quot;, &quot;(Intercept)&quot;))) # dot plot of Fixed Effect Coefficients with CIs sfigb &lt;- ggplot(data = lmm_full_bm[c(2,3),], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), size = 1, shape = 16) + #scale_fill_manual(values = c(unname(aHPC_colors[&quot;within_main&quot;]), &quot;gray&quot;)) + scale_color_manual(values = c(unname(aHPC_colors[&quot;within_main&quot;]), ultimate_gray), labels = c(&quot;Virtual Time&quot;, &quot;First-last&quot;)) + labs(x = element_blank(), y = &quot;fixed effect\\ncoefficient&quot;, color = &quot;Time Metric&quot;) + guides(color = &quot;none&quot;, fill = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + #coord_fixed(ratio = 3\\\\\\\\) + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) sfigb # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% get_complete_df() # plot marginal means sfigc &lt;- ggplot(data = lmm_full_emm, aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, linetype=0, show.legend = FALSE) + scale_color_manual(values = c(&quot;gray&quot;, unname(aHPC_colors[&quot;within_main&quot;])), name = element_blank(), labels =c(&quot;first &amp; last event&quot;, &quot;virtual time&quot;)) + scale_fill_manual(values = c(&quot;gray&quot;, unname(aHPC_colors[&quot;within_main&quot;]))) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;fixed effect&#39;) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + theme_cowplot() sfigc LME model assumptions lmm_diagplots_jb(lmm_full) 8.5 aHPC: virtual time within sequence when including order &amp; real time We further tested whether the effect of virtual time on anterior hippocampal pattern similarity change persisted when including the absolute difference between sequence positions (“order”) and the interval in seconds between events (“real time”) as control predictors of no interest in the model. To follow up on the within-day effect in the hippocampus, let’s see if virtual correlates with pattern similarity change above and beyond the effects of order and real time. We will look at this in two ways. One model with multiple predictors The first way is to run models in which the three time metrics compete for variance. Temporal distances based on order and real time are control regressors of no interest, we test only the effect of virtual time for significance. Summary Statistics In the summary statistics approach, this comes down to a multiple regression on the participant-level. Again, we compute z-values for each regression coefficient from permutations of the dependent variable, which maintains the correlation structure of the regressors. Then we test the z-values for virtual time against 0 on the group level. set.seed(59) # set seed for reproducibility # extract all comparisons from the same day rsa_dat_same_day_aHPC &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE) # do RSA using linear model and calculate z-score for model fit from permutations rsa_fit_aHPC_mult_reg &lt;- rsa_dat_same_day_aHPC %&gt;% group_by(sub_id) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;, &quot;z_order&quot;, &quot;z_real_time&quot;)))) %&gt;% unnest_longer(z) %&gt;% filter(z_id != &quot;z_intercept&quot;) # run group-level t-tests on the RSA fits from the first level in aHPC for within-day stats &lt;- rsa_fit_aHPC_mult_reg %&gt;% filter(z_id ==&quot;z_virtual_time&quot;) %&gt;% select(z) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit_aHPC_mult_reg %&gt;% filter(z_id == &quot;z_virtual_time&quot;))$z, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 0.4372.180.03850.0401270.02480.849One Sample t-testtwo.sided0.3990.02180.808 Summary Statistics: t-test against 0 for virtual time within sequence in aHPC with order and real time in the model t27=2.18, p=0.040, d=0.40, 95% CI [0.02, 0.81] # make z_id a factor and reorder it rsa_fit_aHPC_mult_reg &lt;- rsa_fit_aHPC_mult_reg %&gt;% mutate( z_id = factor(z_id, levels = c(&quot;z_virtual_time&quot;, &quot;z_order&quot;, &quot;z_real_time&quot;))) # raincloud plot f4d &lt;- ggplot(rsa_fit_aHPC_mult_reg, aes(x=z_id, y=z, fill = z_id, colour = z_id)) + gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_point(aes(x = as.numeric(z_id)-0.2), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ ylab(&#39;z RSA model fit&#39;) + xlab(&#39;time metric&#39;) + scale_x_discrete(labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;)) + scale_color_manual(values=time_colors, name = &quot;time metric&quot;, labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;)) + scale_fill_manual(values=time_colors) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use) + guides(fill = &quot;none&quot;, color = guide_legend(override.aes=list(fill=NA, alpha = 1, size=2))) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;) f4d # save source data source_dat &lt;-ggplot_build(f4d)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y, group), file = file.path(dirs$source_dat_dir, &quot;f4d.txt&quot;)) Linear Mixed Effects For the mixed model, we use order differences and real time differences as fixed effects in addition to virtual time. As a random effect structure we first include random intercepts and random slopes for each subject for all 3 fixed effects. This model is singular due to an estimated correlation of 1 between random slopes and random intercepts. We remove the random intercepts, but the fit is still singular. Removing the random slopes for the two fixed effects of no interest (order and real time) also does not help. We thus drop the random intercepts (as above) as well to end up with a model that converges without warnings. Thus, the full model for this analysis has 3 fixed effects and random slopes for virtual time for each subject. set.seed(87) # set seed for reproducibility # extract the comparisons from the same day rsa_dat_same_day &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE) # define the full model with virtual time difference, order difference and real time difference as # fixed effects and random intercepts and random slopes for all effects # fails to converge and is singular after restart formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff + order_diff + real_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular #lmm_full &lt;- update(lmm_full, start=getME(lmm_full, &quot;theta&quot;)) # remove the correlation between random slopes and random intercepts formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff + order_diff + real_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # thus we reduce the random effect structure by excluding random slopes for the fixed effects of no interest # as above this results in a singular fit (the random intercept variance estimated to be 0) formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # next step is to remove the correlation between random slopes and random intercepts formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # we thus reduce further and keep only the random slopes for virtual time differences # now the model converges without warnings set.seed(332) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (0 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE) summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff + order_diff + real_time_diff + (0 + vir_time_diff | sub_id) ## Data: rsa_dat_same_day ## ## AIC BIC logLik deviance df.resid ## -7950.8 -7920.6 3981.4 -7962.8 1114 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.4923 -0.6444 -0.0269 0.6629 3.8017 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id vir_time_diff 6.744e-08 0.0002597 ## Residual 4.778e-05 0.0069126 ## Number of obs: 1120, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -2.812e-04 2.191e-04 -1.284 ## vir_time_diff 1.321e-03 5.415e-04 2.439 ## order_diff 1.235e-05 9.077e-04 0.014 ## real_time_diff -6.760e-04 1.019e-03 -0.663 # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # one way of testing for significance is by comparing the likelihood against a simpler model. # Here, we drop the effect of virtual time difference and run an ANOVA. See e.g. Bodo Winter tutorial set.seed(321) # set seed for reproducibility formula &lt;- &quot;ps_change ~ order_diff + real_time_diff + (0 + vir_time_diff | sub_id)&quot; lmm_no_vir_time &lt;- lme4::lmer(formula, data = rsa_dat_same_day, REML = FALSE) lmm_aov &lt;- anova(lmm_no_vir_time, lmm_full) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 5-7.95e+03-7.92e+033.98e+03-7.96e+03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6-7.95e+03-7.92e+033.98e+03-7.96e+035.9210.015 Mixed Model: Fixed effect of virtual time on aHPC within-sequence pattern similarity change with order and real time in model \\(\\chi^2\\)(1)=5.92, p=0.015 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,1), &quot;residual&quot;) re_names &lt;- c(&quot;virtual time (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Virtual time explains representational change for same-sequence events in the anterior hippocampus when including order and real time in the model&quot;) # convert the huxtable to a flextable for word export stable_lme_aHPC_virtime_same_seq_time_metrics &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Virtual time explains representational change for same-sequence events in the anterior hippocampus when including order and real time in the model fixed effects termestimateSEt-value95% CI intercept-0.0002810.000219-1.28-0.0007110.000149 virtual time0.0013210.0005412.440.0002580.002383 order0.0000120.0009080.01-0.0017680.001793 real time-0.0006760.001019-0.66-0.0026750.001323 random effects grouptermestimate participantvirtual time (SD)0.000260 residualSD0.006913 model comparison modelnparAICLLX2dfp reduced model5-7946.843978.42 full model6-7950.763981.385.9210.015 model: ps_change~vir_time_diff+order_diff+real_time_diff+(0+vir_time_diff|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation Mixed model plots. # set RNG set.seed(23) lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=as.factor(term) %&gt;% factor(levels = c(&quot;vir_time_diff&quot;, &quot;order_diff&quot;, &quot;real_time_diff&quot;))) # dot plot of Fixed Effect Coefficients with CIs sfigmm_e &lt;- ggplot(data = lmm_full_bm[2:4,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), shape = 16, size = 1) + scale_fill_manual(values = time_colors) + scale_color_manual(values = time_colors, labels = c(&quot;virtual time (same seq.)&quot;, &quot;order&quot;, &quot;real Time&quot;)) + labs(x = &quot;time metric&quot;, y = &quot;fixed effect\\nestimate&quot;, color = &quot;Time Metric&quot;) + guides(color = &quot;none&quot;, fill = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) sfigmm_e # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% get_complete_df # convert the group variable to a factor to control the order of facets below lmm_full_emm$group &lt;- factor(lmm_full_emm$group, levels = c(&quot;vir_time_diff&quot;, &quot;order_diff&quot;, &quot;real_time_diff&quot;)) # plot marginal means sfigmm_f &lt;- ggplot(data = lmm_full_emm, aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, linetype=0) + scale_color_manual(values = time_colors, name = element_blank(), labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;)) + scale_fill_manual(values = time_colors) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;z time metric&#39;) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), strip.background = element_blank(), strip.text.x = element_blank()) sfigmm_f LME model assumptions lmm_diagplots_jb(lmm_full) We are now ready to create figure 4 of the manuscript: layoutBD = &quot;BBCCCDDD&quot; layoutABD =&quot; ABBBBBB ABBBBBB&quot; f4 &lt;- f4a+ {f4b + f4c + f4d + plot_layout(design = layoutBD, guides = &quot;collect&quot;) &amp; theme(text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), legend.spacing.x = unit(1, &#39;mm&#39;), legend.key.size = unit(3,&quot;mm&quot;), legend.position = &#39;bottom&#39;, legend.justification = &quot;right&quot;, legend.margin = margin(0,0,0,1, unit=&quot;cm&quot;))} + plot_layout(design = layoutABD, guides = &quot;keep&quot;) &amp; plot_annotation(theme = theme(plot.margin = margin(t=0, r=0, b=0, l=-19, unit=&quot;pt&quot;)), tag_levels = &#39;A&#39;)&amp; theme(text = element_text(size=10, family=font2use)) f4[[1]] &lt;- f4[[1]] + theme(legend.position = c(0.5,-0.1963), legend.text=element_text(size=8), legend.title=element_text(size=8), plot.tag = element_text(margin = margin(l = 13))) # save and print fn &lt;- here(&quot;figures&quot;, &quot;f4&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=f4, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=f4, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;) Figure 4. Sequence representations in anterior hippocampus reflect constructed event times. A. The anterior hippocampus region of interest is displayed on the MNI template with voxels outside the field of view shown in lighter shades of gray. Color code denotes probability of a voxel to be included in the mask based on participant-specific ROIs (see Methods). B. The Z-values based on permutation tests of participant-specific linear models assessing the effect of virtual time on pattern similarity change for event pairs from the same sequence were significantly positive. C. To illustrate the effect shown in B, average pattern similarity change values are shown for same-sequence event pairs that are separated by low and high temporal distances based on a median split. D. Z-values show the relationship of the different time metrics to representational change based on participant-specific multiple regression analyses. Virtual time predicts pattern similarity change with event order and real time in the model as control predictors of no interest. B-D. Circles are individual participant data; boxplots show median and upper/lower quartile along with whiskers extending to most extreme data point within 1.5 interquartile ranges above/below the upper/lower quartile; black circle with error bars corresponds to mean±S.E.M.; distributions show probability density function of data points. ** p&lt;0.01; * p&lt;0.05 Virtual time explains residuals of order and real time We implemented participant-specific regression analyses with order and real time distances as predictors of hippocampal pattern similarity change. The plot shows a significant effect of virtual temporal distances when tested on the residuals of these linear models. Thus, variance that cannot be explained by the other time metrics can be accounted for by virtual temporal distances. This analysis was conducted only using the summary statistics approach because the residuals of a mixed model are more difficult to interpret than those of participant-specific regression analyses using ordinary least squares. The second way of running this analysis is via a two-step procedure. We first predict pattern similarity change from the ordinal and real-time relationships and store the residuals, i.e. variance in pattern similarity change that cannot be explained by the effects of order and real time. Then, we test in a second model whether virtual time can explain these residuals. Summary Statistics The first step here is a multiple linear regression model with order and real time distances as predictors. We do not use permutations here because we are only interested in the residuals of this model. In the second step, we run a linear model with permutations to obtain z-values for group-level stats for the effect of virtual time on the residuals. set.seed(102) # set seed for reproducibility # extract all comparisons from the same day rsa_dat_same_day_aHPC &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE) # run multiple regression model with order and real time as predictors and store residuals resids &lt;- rsa_dat_same_day_aHPC %&gt;% group_by(sub_id) %&gt;% # run the linear model do(resids = residuals(lm(ps_change ~ order_diff + real_time_diff, data=.))) %&gt;% select(resids) %&gt;% unnest(resids) # add to the original data frame rsa_dat_same_day_aHPC$resids &lt;- resids$resids # do RSA using linear model and calculate z-score for model fit rsa_fit_aHPC_mult_reg_resids &lt;- rsa_dat_same_day_aHPC %&gt;% group_by(sub_id) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;resids ~ vir_time_diff&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;)))) %&gt;% unnest_wider(z) # run a group-level t-test on the RSA fits from the first level in aHPC for within-day stats &lt;- rsa_fit_aHPC_mult_reg_resids %&gt;% select(z_virtual_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=rsa_fit_aHPC_mult_reg_resids$z_virtual_time, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 0.1772.230.03450.0343270.0140.341One Sample t-testtwo.sided0.4090.0310.819 Summary Statistics for virtual time on residuals of order and real time: t27=2.23, p=0.034, d=0.41, 95% CI [0.03, 0.82] We further observed that the residuals of linear models, in which hippocampal representational change was predicted from order and real time, were related to virtual temporal distances (Supplemental Figure 3D; t27=2.23, p=0.034, d=0.41, 95% CI [0.03, 0.82] ), demonstrating that virtual time accounts for variance that the other time metrics fail to explain. # select the data to plot plot_dat &lt;- rsa_fit_aHPC_mult_reg_resids plot_dat$group &lt;- factor(1) # raincloud sfigd &lt;- ggplot(plot_dat, aes(x=1, y=z_virtual_time, color = group, fill=group)) + gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_point(aes(x = 0.8, y = z_virtual_time), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ scale_color_manual(values=unname(aHPC_colors[&quot;within_main&quot;])) + scale_fill_manual(values=unname(aHPC_colors[&quot;within_main&quot;])) + ylab(&#39;z RSA model fit&#39;) + xlab(element_blank()) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;, axis.ticks.x = element_blank(), axis.text.x = element_blank()) sfigd Supplemental Figure Let’s compose a figure from the above plots to summarize these results of the control analyses for the same-sequence effects. layout = &quot; AAAABDD AAAACDD&quot; sfig &lt;- sfiga + sfigb + sfigc + sfigd + plot_layout(design = layout, guides = &#39;collect&#39;) &amp; theme(text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.position = &quot;bottom&quot;) &amp; plot_annotation(theme = theme(plot.margin = margin(t=-3, r=-6, b=-7, l=-6, unit=&quot;pt&quot;)), tag_levels = &#39;A&#39;) sfig[[4]] &lt;- sfig[[4]] + theme(legend.position = &quot;none&quot;) # save and print to screen fn &lt;- here(&quot;figures&quot;, &quot;sf05&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=sfig, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=sfig, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;) Supplemental Figure 5. The relationship of virtual time and hippocampal pattern similarity change is not driven by the first and last event of a sequence. A. Z-values from the summary statistics approach show a significant positive effect of virtual time on pattern similarity change in the anterior hippcampus when competing for variance with a control predictor of no interest accounting for variance explained by whether pairs of events were made up from the first and last event of a sequence or not. B, C. Fixed effect estimate with confidence intervals (B) and estimated marginal means (C) visualize the results of the corresponding mixed model. D. We implemented participant-specific regression analyses with order and real time distances as predictors of hippocampal pattern similarity change. The plot shows a significant effect of virtual temporal distances when tested on the residuals of these linear models. Thus, variance that cannot be explained by the other time metrics can be accounted for by virtual temporal distances. This analysis was conducted only using the summary statistics approach because the residuals of a mixed model are more difficult to interpret than those of participant-specific regression analyses using ordinary least squares. A, D. Circles show individual participant Z-values from the summary statistics approach; boxplot shows median and upper/lower quartile along with whiskers extending to most extreme data point within 1.5 interquartile ranges above/below the upper/lower quartile; black circle with error bars corresponds to mean±S.E.M.; distribution shows probability density function of data points. * p&lt;0.05 8.6 aHPC: virtual time across sequences Having established that the anterior hippocampus forms sequence memories shaped by event relations in virtual time, we want to ask whether the hippocampus also generalizes across sequences. So next, we focus on pattern similarity changes for events that stem from different virtual days. We separately tested the effect of virtual time for event pairs from the same or different sequences and thus use a Bonferroni-corrected α-level of 0.025 for these tests. Summary statistics set.seed(103) # set seed for reproducibility # run a group-level t-test on the RSA fits from the first level in aHPC for across-day stats &lt;- rsa_fit %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == FALSE) %&gt;% select(z_virtual_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == FALSE))$z_virtual_time, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.5-2.650.01330.012827-0.887-0.113One Sample t-testtwo.sided-0.486-0.906-0.105 Summary Statistics: t-test against 0 for virtual time across sequences in aHPC t27=-2.65, p=0.013, d=-0.49, 95% CI [-0.91, -0.10] # select the data to plot plot_dat &lt;- rsa_fit %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == FALSE) ggplot(plot_dat, aes(x=1, y=z_virtual_time, fill = roi, color = roi)) + gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_point(aes(x = 0.8, y = z_virtual_time), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ scale_color_manual(values=unname(aHPC_colors[&quot;across_main&quot;])) + scale_fill_manual(values=unname(aHPC_colors[&quot;across_main&quot;])) + ylab(&#39;z RSA model fit&#39;) + xlab(element_blank()) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;**&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;, axis.ticks.x = element_blank(), axis.text.x = element_blank()) Linear Mixed Effects Here, we fit a model with virtual time as the predictor for pattern similarity change for the across-day comparisons in the anterior hippocampus. We start with a full random effects structure including random slopes and random intercepts for each subject. We reduce the random effects structure, but unfortunately even a simple model with only random slopes for the effect of virtual time results in a singular fit warning; even when trying different optimizers. We keep the random slopes to account for the within-subject dependencies in the data. # extract the comparisons from different days rsa_dat_diff_day &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == FALSE) # define the full model with virtual time difference as # fixed effect and by-subject random slopes and random intercepts --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff + (1 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # remove correlation --&gt; still singular formula &lt;- &quot;ps_change ~ vir_time_diff + (1 + vir_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # only random slopes set.seed(389) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff + (0 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE, #control = lmerControl(optimizer =&#39;optimx&#39;, optCtrl=list(method=&#39;nlminb&#39;)) #control = lmerControl(optimizer =&quot;bobyqa&quot;, optCtrl=list(maxfun=40000)) control = lmerControl(optimizer =&#39;nloptwrap&#39;, optCtrl=list(method=&#39;NLOPT_LN_NELDERMEAD&#39;, maxfun=100000)) ) ## boundary (singular) fit: see ?isSingular summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff + (0 + vir_time_diff | sub_id) ## Data: rsa_dat_diff_day ## Control: lmerControl(optimizer = &quot;nloptwrap&quot;, optCtrl = list(method = &quot;NLOPT_LN_NELDERMEAD&quot;, maxfun = 1e+05)) ## ## AIC BIC logLik deviance df.resid ## -29625.4 -29600.0 14816.7 -29633.4 4196 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.3020 -0.6877 0.0324 0.6564 3.5548 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id vir_time_diff 0.00e+00 0.000000 ## Residual 5.05e-05 0.007107 ## Number of obs: 4200, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -6.075e-05 1.098e-04 -0.553 ## vir_time_diff -2.745e-04 1.096e-04 -2.505 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;, optimizer = &#39;nloptwrap&#39;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # one way of testing for significance is by comparing the likelihood against a simpler model. # Here, we drop the effect of virtual time difference and run an ANOVA. See e.g. Bodo Winter tutorial set.seed(328) # set seed for reproducibility formula &lt;- &quot;ps_change ~ 1 + (0 + vir_time_diff | sub_id)&quot; lmm_no_vir_time &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) lmm_aov &lt;-anova(lmm_no_vir_time, lmm_full) Mixed Model: Fixed effect of virtual time across sequences on aHPC pattern similarity change \\(\\chi^2\\)(1)=6.01, p=0.014 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,1), &quot;residual&quot;) re_names &lt;- c(&quot;virtual time (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Virtual time explains representational change for different-sequence events in the anterior hippocampus&quot;) # convert the huxtable to a flextable for word export stable_lme_aHPC_virtime_diff_seq &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Virtual time explains representational change for different-sequence events in the anterior hippocampus fixed effects termestimateSEt-value95% CI intercept-0.0000610.000110-0.55-0.0002760.000155 virtual time-0.0002750.000110-2.51-0.000491-0.000058 random effects grouptermestimate participantvirtual time (SD)0.000000 residualSD0.007107 model comparison modelnparAICLLX2dfp reduced model3-29621.3914813.69 full model4-29625.4014816.706.0110.014 model: ps_change~vir_time_diff+(0+vir_time_diff|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation ### PLOTS lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=as.factor(term)) # dot plot of Fixed Effect Coefficients with CIs sfigmm_g &lt;-ggplot(data = lmm_full_bm[2,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), shape = 16, size = 1) + scale_fill_manual(values = unname(aHPC_colors[&quot;across_main&quot;])) + scale_color_manual(values = unname(aHPC_colors[&quot;across_main&quot;]), labels = c(&quot;virtual time (diff. seq.)&quot;), name = element_blank()) + labs(x = element_blank(), y = &quot;fixed effect\\nestimate&quot;, color = &quot;&quot;) + guides(fill = &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2, linetype=0))) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(legend.position = &quot;none&quot;, axis.text.x=element_blank()) sfigmm_g # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% ggeffects::get_complete_df() # plot marginal means sfigmm_h &lt;- ggplot(data = lmm_full_emm[lmm_full_emm$group == &quot;vir_time_diff&quot;,], aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, linetype=0) + scale_color_manual(values = unname(aHPC_colors[&quot;across_main&quot;]), name = element_blank(), labels = &quot;virtual time&quot;) + scale_fill_manual(values = unname(aHPC_colors[&quot;across_main&quot;])) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;z time metric&#39;) + #ggtitle(&quot;Estimated marginal means&quot;) + guides(fill= &quot;none&quot;, color= &quot;none&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;) sfigmm_h LME model assumptions lmm_diagplots_jb(lmm_full) Representational Change Visualization To illustrate the effect in a simpler way, we average pattern similarity changes based on a median split of events, separately for events from the same and from different sequences. # Test pattern similarity for different-sequence events separated by high vs. low temporal distances set.seed(209) # set seed for reproducibility stats_diff_day_high_low &lt;- paired_t_perm_jb(ps_change_means[ps_change_means$same_day==&quot;different day&quot; &amp; ps_change_means$temp_dist_f == &quot;high&quot;,]$ps_change - ps_change_means[ps_change_means$same_day==&quot;different day&quot; &amp; ps_change_means$temp_dist_f == &quot;low&quot;,]$ps_change) # Cohen&#39;s d with Hedges&#39; correction for paired samples using non-central t-distribution for CI d&lt;-cohen.d(d=ps_change_means[ps_change_means$same_day==&quot;different day&quot; &amp; ps_change_means$temp_dist_f == &quot;high&quot;,]$ps_change, f=ps_change_means[ps_change_means$same_day==&quot;different day&quot; &amp; ps_change_means$temp_dist_f == &quot;low&quot;,]$ps_change, paired=TRUE, pooled=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats_diff_day_high_low$d &lt;- d$estimate stats_diff_day_high_low$dCI_low &lt;- d$conf.int[[1]] stats_diff_day_high_low$dCI_high &lt;- d$conf.int[[2]] huxtable(stats_diff_day_high_low) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.000711-3.260.003040.002527-0.00116-0.000263One Sample t-testtwo.sided-0.89-1.03-0.209 To visualize the effect, lets make a plot of the raw pattern similarity change for high vs. low temporal distances. f5b &lt;- ggplot(data=ps_change_means %&gt;% filter(same_day == &quot;different day&quot;), aes(x=temp_dist, y=ps_change, fill = same_day, color = same_day)) + gghalves::geom_half_violin(data = ps_change_means %&gt;% filter(same_day == &quot;different day&quot;, temp_dist_f == &quot;low&quot;), aes(x=temp_dist, y=ps_change), position=position_nudge(-0.1), side = &quot;l&quot;, color = NA) + gghalves::geom_half_violin(data = ps_change_means %&gt;% filter(same_day == &quot;different day&quot;, temp_dist_f == &quot;high&quot;), aes(x=temp_dist, y=ps_change), position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_boxplot(aes(group = temp_dist),width = .1, colour = &quot;black&quot;, outlier.shape = NA) + geom_line(aes(x = x_jit, group=sub_id,), color = ultimate_gray, position = position_nudge(c(0.15, -0.15))) + geom_point(aes(x=x_jit), position = position_nudge(c(0.15, -0.15)), shape=16, size = 1) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;, width = 0, size = 0.5) + scale_fill_manual(values = unname(aHPC_colors[&quot;across_main&quot;])) + scale_color_manual(values = unname(aHPC_colors[&quot;across_main&quot;]), name = &quot;different sequence&quot;, labels = &quot;virtual time&quot;) + scale_x_continuous(breaks = c(1,2), labels=c(&quot;low&quot;, &quot;high&quot;)) + annotate(geom = &quot;text&quot;, x = 1.5, y = Inf, label = &#39;underline(&quot; ** &quot;)&#39;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use, parse=TRUE) + ylab(&#39;pattern similarity change&#39;) + xlab(&#39;virtual temporal distance&#39;) + guides(fill= &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2), title.position = &quot;left&quot;)) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;, strip.background = element_blank(), strip.text = element_blank()) f5b # save source data source_dat &lt;-ggplot_build(f5b)$data[[4]] readr::write_tsv(source_dat %&gt;% select(x,y, group), file = file.path(dirs$source_dat_dir, &quot;f5b.txt&quot;)) Summary Statistics: paired t-test for aHPC pattern similarity changes for high vs. low temporal distances for different sequence events t27=-3.26, p=0.002, d=-0.89, 95% CI [-1.03, -0.21] 8.7 aHPC: virtual time across sequence when including order &amp; real time We also test whether virtual time predicts aHPC pattern similarity changes for event pairs from different sequences when including order and real time in the model Summary Statistics # SUMMARY STATS AHPC WITH ORDER AND REAL TIME set.seed(59) # set seed for reproducibility # extract all comparisons from the same day rsa_dat_diff_day_aHPC &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == FALSE) # do RSA using linear model and calculate z-score for model fit from permutations rsa_fit_aHPC_mult_reg &lt;- rsa_dat_diff_day_aHPC %&gt;% group_by(sub_id) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;, &quot;z_order&quot;, &quot;z_real_time&quot;)))) %&gt;% unnest_longer(z) %&gt;% filter(z_id != &quot;z_intercept&quot;) # run group-level t-tests on the RSA fits from the first level in aHPC for within-day stats &lt;- rsa_fit_aHPC_mult_reg %&gt;% filter(z_id ==&quot;z_virtual_time&quot;) %&gt;% select(z) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # find outliers is_outlier &lt;- function(x) { return(x &lt; quantile(x, 0.25) - 1.5 * IQR(x) | x &gt; quantile(x, 0.75) + 1.5 * IQR(x)) } out_ind &lt;- is_outlier(rsa_fit_aHPC_mult_reg$z[rsa_fit_aHPC_mult_reg$z_id == &quot;z_virtual_time&quot;]) out_sub &lt;- subjects[out_ind] # run second-level analysis on data points that are not outliers set.seed(123) # set seed for reproducibility stats &lt;- rsa_fit_aHPC_mult_reg %&gt;% filter(sub_id != out_sub, z_id == &quot;z_virtual_time&quot;) %&gt;% select(&quot;z&quot;) %&gt;% paired_t_perm_jb(., n_perm = n_perm) huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternative -0.418-2.620.01460.015526-0.746-0.0897One Sample t-testtwo.sided # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit_aHPC_mult_reg %&gt;% filter(z_id == &quot;z_virtual_time&quot;, sub_id != out_sub))$z, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.418-2.620.01460.015526-0.746-0.0897One Sample t-testtwo.sided-0.489-0.918-0.1 Summary Statistics: t-test against 0 for virtual time across sequences in aHPC with order and real time in the model t26=-2.62, p=0.015, d=-0.49, 95% CI [-0.92, -0.10] # make z_id a factor and reorder it rsa_fit_aHPC_mult_reg &lt;- rsa_fit_aHPC_mult_reg %&gt;% mutate( z_id = factor(z_id, levels = c(&quot;z_virtual_time&quot;, &quot;z_order&quot;, &quot;z_real_time&quot;))) # raincloud plot sfig6_a &lt;- ggplot(rsa_fit_aHPC_mult_reg, aes(x=z_id, y=z, fill = z_id, colour = z_id)) + gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_point(aes(x = as.numeric(z_id)-0.2), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ ylab(&#39;z RSA model fit&#39;) + xlab(&#39;time metric&#39;) + scale_x_discrete(labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;)) + scale_color_manual(values = c(unname(aHPC_colors[&quot;across_main&quot;]), time_colors[c(2,3)]), name = &quot;time metric&quot;, labels = c(&quot;virtual&quot;, &quot;order&quot;, &quot;real&quot;)) + scale_fill_manual(values = c(unname(aHPC_colors[&quot;across_main&quot;]), time_colors[c(2,3)])) + guides(fill= &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2))) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;) sfig6_a Linear Mixed Effects # MIXED EFFECT AHPC WITH ORDER AND REAL TIME set.seed(87) # set seed for reproducibility # extract the comparisons from different days, exclude outlier rsa_dat_diff_day &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == FALSE, sub_id != out_sub) # define the full model with virtual time difference, order difference and real time difference as # fixed effects and random intercepts and random slopes for all effects # fails to converge and is singular after restart formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff + order_diff + real_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular #lmm_full &lt;- update(lmm_full, start=getME(lmm_full, &quot;theta&quot;)) # remove the correlation between random slopes and random intercepts formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff + order_diff + real_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # thus we reduce the random effect structure by excluding random slopes for the fixed effects of no interest # as above this results in a singular fit (the random intercept variance estimated to be 0) formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # next step is to remove the correlation between random slopes and random intercepts formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # we thus reduce further and keep only the random slopes for virtual time differences # but the model converges with the warning about the singular fit set.seed(332) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (0 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff + order_diff + real_time_diff + (0 + vir_time_diff | sub_id) ## Data: rsa_dat_diff_day ## ## AIC BIC logLik deviance df.resid ## -28597.1 -28559.3 14304.6 -28609.1 4044 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.2924 -0.6858 0.0382 0.6566 3.6191 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id vir_time_diff 0.000e+00 0.000000 ## Residual 5.008e-05 0.007077 ## Number of obs: 4050, groups: sub_id, 27 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -0.0001006 0.0001115 -0.902 ## vir_time_diff -0.0006234 0.0002944 -2.118 ## order_diff -0.0003476 0.0004777 -0.728 ## real_time_diff 0.0007023 0.0005288 1.328 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # one way of testing for significance is by comparing the likelihood against a simpler model. # Here, we drop the effect of virtual time difference and run an ANOVA. See e.g. Bodo Winter tutorial set.seed(321) # set seed for reproducibility formula &lt;- &quot;ps_change ~ order_diff + real_time_diff + (0 + vir_time_diff | sub_id)&quot; lmm_no_vir_time &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular lmm_aov &lt;- anova(lmm_no_vir_time, lmm_full) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 5-2.86e+04-2.86e+041.43e+04-2.86e+04&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6-2.86e+04-2.86e+041.43e+04-2.86e+044.4810.0343 Mixed Model: Fixed effect of virtual time on aHPC across-sequence pattern similarity change with order and real time in model \\(\\chi^2\\)(1)=4.48, p=0.034 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,1), &quot;residual&quot;) re_names &lt;- c(&quot;virtual time (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Virtual time explains representational change for different-sequence events in the anterior hippocampus when including order and real time in the model&quot;) # convert the huxtable to a flextable for word export stable_lme_aHPC_virtime_diff_seq_time_metrics &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Virtual time explains representational change for different-sequence events in the anterior hippocampus when including order and real time in the model fixed effects termestimateSEt-value95% CI intercept-0.0001010.000112-0.90-0.0003190.000118 virtual time-0.0006230.000294-2.12-0.001201-0.000046 order-0.0003480.000478-0.73-0.0012840.000589 real time0.0007020.0005291.33-0.0003340.001739 random effects grouptermestimate participantvirtual time (SD)0.000000 residualSD0.007077 model comparison modelnparAICLLX2dfp reduced model5-28594.6414302.32 full model6-28597.1214304.564.4810.034 model: ps_change~vir_time_diff+order_diff+real_time_diff+(0+vir_time_diff|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation Mixed model plots. # set RNG set.seed(23) lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=as.factor(term) %&gt;% factor(levels = c(&quot;vir_time_diff&quot;, &quot;order_diff&quot;, &quot;real_time_diff&quot;))) # dot plot of Fixed Effect Coefficients with CIs sfig6_b &lt;- ggplot(data = lmm_full_bm[2:4,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), shape = 16, size = 1) + scale_fill_manual(values = c(unname(aHPC_colors[&quot;across_main&quot;]), time_colors[c(2,3)])) + scale_color_manual(values = c(unname(aHPC_colors[&quot;across_main&quot;]), time_colors[c(2,3)]), labels = c(&quot;Virtual&quot;, &quot;Order&quot;, &quot;Real&quot;)) + labs(x = &quot;time metric&quot;, y = &quot;fixed effect\\nestimate&quot;, color = &quot;Time Metric&quot;) + guides(color = &quot;none&quot;, fill = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) sfig6_b # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% get_complete_df # convert the group variable to a factor to control the order of facets below lmm_full_emm$group &lt;- factor(lmm_full_emm$group, levels = c(&quot;vir_time_diff&quot;, &quot;order_diff&quot;, &quot;real_time_diff&quot;)) # plot marginal means sfig6_c &lt;- ggplot(data = lmm_full_emm, aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, linetype=0) + scale_color_manual(values = c(unname(aHPC_colors[&quot;across_main&quot;]), time_colors[c(2,3)]), name = element_blank(), labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;)) + scale_fill_manual(values = c(unname(aHPC_colors[&quot;across_main&quot;]), time_colors[c(2,3)])) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;z time metric&#39;) + guides(fill = &quot;none&quot;, color= &quot;none&quot;) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), strip.background = element_blank(), strip.text.x = element_blank()) sfig6_c 8.8 aHPC: virtual time within vs. across sequences The previous results show positive correlations of pattern similarity change with temporal distances between event pairs from the same sequence and negative correlations with temporal distances when events are from different sequences. Next, we want to quantify whether these two modes of representation are significantly different from each other. Second-level analysis For the summary statistics approach, we run a permutation-based paired t-test to check for a difference in the effect of virtual time depending on whether we look at within vs. across sequences. set.seed(105) # set seed for reproducibility stats &lt;- rsa_fit %&gt;% filter(roi==&quot;aHPC_lr&quot;) %&gt;% # calculate difference between within- an across-day effect for each participant group_by(sub_id) %&gt;% summarise( same_day_diff = z_virtual_time[same_day==TRUE]-z_virtual_time[same_day==FALSE], .groups = &quot;drop&quot;) %&gt;% # test difference against 0 select(same_day_diff) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for paired samples using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == TRUE))$z_virtual_time, f=(rsa_fit %&gt;% filter(roi == &quot;aHPC_lr&quot;, same_day == FALSE))$z_virtual_time, paired=TRUE, pooled =TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 1.123.710.0009410.001270.51.74One Sample t-testtwo.sided1.050.2871.13 Summary Statistics: paired t-test comparing effects of virtual time within and across sequences in aHPC t27=3.71, p=0.001, d=1.05, 95% CI [0.29, 1.13] To visualize the difference between the effects of virtual time for events from the same or different day, we create a pair of rainclouds facing each other. We use a custom jitter so that the length of lines connecting individual subject data points remains constant, thus keeping their slopes comparable. # generate custom repeating jitter (multiply with -1/1 to shift towards each other) plot_dat &lt;- rsa_fit %&gt;% filter(roi==&quot;aHPC_lr&quot;) %&gt;% mutate( same_day_num = plyr::mapvalues(same_day, from = c(&quot;FALSE&quot;, &quot;TRUE&quot;),to = c(1,0)), same_day_char = plyr::mapvalues(same_day, from = c(&quot;FALSE&quot;, &quot;TRUE&quot;), to = c(&quot;different&quot;, &quot;same&quot;)), same_day_char = fct_relevel(same_day_char, c(&quot;same&quot;, &quot;different&quot;)), x_jit = as.numeric(same_day_char) + rep(jitter(rep(0,n_subs), amount=0.05), each=2) * rep(c(-1,1),n_subs)) f5a&lt;- ggplot(data=plot_dat, aes(x=same_day_char, y=z_virtual_time, fill = same_day_char, color = same_day_char)) + geom_boxplot(aes(group=same_day_char), position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + scale_fill_manual(values = unname(c(aHPC_colors[&quot;within_main&quot;], aHPC_colors[&quot;across_main&quot;]))) + scale_color_manual(values = unname(c(aHPC_colors[&quot;within_main&quot;], aHPC_colors[&quot;across_main&quot;])), name = &quot;sequence&quot;, labels=c(&quot;same&quot;, &quot;different&quot;)) + gghalves::geom_half_violin(data = plot_dat %&gt;% filter(same_day == TRUE), aes(x=same_day_char, y=z_virtual_time), position=position_nudge(-0.1), side = &quot;l&quot;, color = NA) + gghalves::geom_half_violin(data = plot_dat %&gt;% filter(same_day == FALSE), aes(x=same_day_char, y=z_virtual_time), position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;, width = 0, size = 0.5) + geom_line(aes(x = x_jit, group=sub_id,), color = ultimate_gray, position = position_nudge(c(0.15, -0.15))) + geom_point(aes(x=x_jit, fill = same_day_char), position = position_nudge(c(-0.15, 0.15)), shape=16, size = 1) + ylab(&#39;z RSA model fit&#39;) + xlab(&#39;sequence&#39;) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = c(1, 2), y = Inf, label = c(&#39;**&#39;, &#39;*&#39;), hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + annotate(geom = &quot;text&quot;, x = 1.5, y = Inf, label = &#39;underline(&quot; *** &quot;)&#39;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use, parse = TRUE) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;bottom&quot;) f5a # save source data source_dat &lt;-ggplot_build(f5a)$data[[6]] readr::write_tsv(source_dat %&gt;% select(x,y, group), file = file.path(dirs$source_dat_dir, &quot;f5a.txt&quot;)) Linear Mixed Effects Next, we test whether there is an interaction effect, i.e. we test whether the effect of virtual time on hippocampal pattern similarity change depends on whether we look at event pairs from the same vs. from different days. We again have to reduce our model from the maximal random effects structure. We end up with a model that keeps only the random slopes for the interaction of virtual time and day. This is in keeping with the paper on random effect structures when testing interactions that emphasizes the importance of random slopes for the interaction of interest Barr, 2013, Frontiers Psychology. # get the data rsa_dat_aHPC &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot;) # test the interaction for aHPC # maximal random effect structure --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + (1 + vir_time_diff * same_day_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) ## boundary (singular) fit: see ?isSingular # no correlations with random intercept --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + (1 + vir_time_diff * same_day_dv || sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) ## boundary (singular) fit: see ?isSingular # random intercepts and random slopes only for the critical interaction --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + (1 + vir_time_diff : same_day_dv || sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) ## boundary (singular) fit: see ?isSingular # random slopes for critical interactions set.seed(334) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + (0 + vir_time_diff : same_day_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff * same_day_dv + (0 + vir_time_diff:same_day_dv | sub_id) ## Data: rsa_dat_aHPC ## ## AIC BIC logLik deviance df.resid ## -37581.7 -37542.3 18796.9 -37593.7 5314 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.3282 -0.6823 0.0196 0.6564 3.7447 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id vir_time_diff:same_day_dv 3.083e-08 0.0001756 ## Residual 4.993e-05 0.0070659 ## Number of obs: 5320, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -0.0001934 0.0001209 -1.600 ## vir_time_diff 0.0002384 0.0001223 1.950 ## same_day_dv -0.0001327 0.0001209 -1.098 ## vir_time_diff:same_day_dv 0.0005129 0.0001267 4.049 # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # test against reduced model set.seed(309) # set seed for reproducibility lmm_reduced &lt;- update(lmm_full, ~ . - vir_time_diff:same_day_dv) lmm_aov &lt;- anova(lmm_reduced, lmm_full) # store for future use (MDS) lmm_aHPC_interaction &lt;- lmm_full Mixed Model: Fixed effect of interaction of virtual time with sequence identity on aHPC pattern similarity change \\(\\chi^2\\)(1)=14.37, p=0.000 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;, &quot;day&quot;, &quot;interaction virtual time and day&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,1), &quot;residual&quot;) re_names &lt;- c(&quot;interaction virtual time and day (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: The effect of virtual time differs between same-sequence and different-sequence events in the anterior hippocampus&quot;) # convert the huxtable to a flextable for word export stable_lme_aHPC_virtime_same_vs_diff_seq &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: The effect of virtual time differs between same-sequence and different-sequence events in the anterior hippocampus fixed effects termestimateSEt-value95% CI intercept-0.0001930.000121-1.60-0.0004300.000044 virtual time0.0002380.0001221.95-0.0000010.000478 day-0.0001330.000121-1.10-0.0003700.000104 interaction virtual time and day0.0005130.0001274.050.0002610.000765 random effects grouptermestimate participantinteraction virtual time and day (SD)0.000176 residualSD0.007066 model comparison modelnparAICLLX2dfp reduced model5-37569.3818789.69 full model6-37581.7518796.8714.3711.50e-04 model: ps_change~vir_time_diff*same_day_dv+(0+vir_time_diff:same_day_dv|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=as.factor(term)) # dot plot of Fixed Effect Coefficients with CIs sfigmm_i &lt;- ggplot(data = lmm_full_bm[2:4,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), shape = 16, size = 1) + scale_fill_manual(values = c(rep(&quot;black&quot;,6), &quot;#DD8D29&quot;)) + scale_color_manual(values = c(ultimate_gray, time_within_across_color, day_time_int_color), name = element_blank(), labels = c(&quot;sequence&quot;, &quot;virtual time (all events)&quot;, &quot;interaction&quot;)) + labs(x = element_blank(), y = &quot;fixed effect\\nestimate&quot;) + theme_cowplot() + guides(fill = &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2, linetype=0))) + annotate(geom = &quot;text&quot;, x = 3, y = Inf, label = &quot;***&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank(), legend.position = &quot;bottom&quot;) sfigmm_i # estimate marginal means (&amp; add factor for easier plotting) hpc_emms &lt;- ggeffects::ggpredict(lmm_full, terms = c(&quot;vir_time_diff&quot;, &quot;same_day_dv&quot;)) %&gt;% as_tibble() %&gt;% mutate(day = factor(if_else(group == 1, true = &quot;same day&quot;, false = &quot;different day&quot;), levels = c(&quot;same day&quot;, &quot;different day&quot;))) # plot marginal means sfigmm_j &lt;- ggplot(hpc_emms, aes(x = x, y = predicted, colour = day, fill = day)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, linetype=0) + geom_line(size = 0.5) + #geom_line(aes(linetype = day), size = 0.5) + #scale_linetype_manual(values=c(&quot;solid&quot;, &quot;2121&quot;)) + scale_fill_manual(values = unname(c(aHPC_colors[c(&quot;within_main&quot;, &quot;across_main&quot;)])), name = element_blank()) + scale_color_manual(values = unname(c(aHPC_colors[c(&quot;within_main&quot;, &quot;across_main&quot;)]))) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;z virtual time&#39;) + guides(fill = &quot;none&quot;, color = &quot;none&quot;, linetype = &quot;none&quot;) + theme_cowplot() + theme(legend.title = element_blank()) sfigmm_j LME model assumptions lmm_diagplots_jb(lmm_full) Interaction of sequence membership and order # interaction of order and sequence (including only cirtical random slopes to avoid singular fit) formula &lt;- &quot;ps_change ~ order_diff * same_day_dv + (0 + order_diff : same_day_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) lmm_reduced &lt;- update(lmm_full, ~ . - order_diff:same_day_dv) lmm_aov &lt;- anova(lmm_reduced, lmm_full) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 5-3.76e+04-3.75e+041.88e+04-3.76e+04&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6-3.76e+04-3.75e+041.88e+04-3.76e+049.9810.00158 Mixed Model: Fixed effect of interaction of order with sequence identity on aHPC pattern similarity change \\(\\chi^2\\)(1)=9.98, p=0.002 Interaction of sequence membership and real time # interaction of real time and sequence (including only cirtical random slopes to avoid singular fit) formula &lt;- &quot;ps_change ~ real_time_diff * same_day_dv + (0 + real_time_diff : same_day_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) lmm_reduced &lt;- update(lmm_full, ~ . - real_time_diff:same_day_dv) lmm_aov &lt;- anova(lmm_reduced, lmm_full) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 5-3.76e+04-3.75e+041.88e+04-3.76e+04&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6-3.76e+04-3.75e+041.88e+04-3.76e+049.2710.00232 Mixed Model: Fixed effect of interaction of real time with sequence identity on aHPC pattern similarity change \\(\\chi^2\\)(1)=9.27, p=0.002 Interaction of sequence membership and virtual time when including interactions with other time metrics # test the interaction for aHPC # maximal random effect structure --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + order_diff:same_day_dv + real_time_diff:same_day_dv+ (1 + vir_time_diff * same_day_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) ## boundary (singular) fit: see ?isSingular # no correlations with random intercept --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + order_diff:same_day_dv + real_time_diff:same_day_dv+ (1 + vir_time_diff * same_day_dv || sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) ## boundary (singular) fit: see ?isSingular # random intercepts and random slopes only for the critical interaction --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + order_diff:same_day_dv + real_time_diff:same_day_dv+ (1 + vir_time_diff : same_day_dv || sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) ## boundary (singular) fit: see ?isSingular # random slopes for critical interactions set.seed(334) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + order_diff:same_day_dv + real_time_diff:same_day_dv+ (0 + vir_time_diff : same_day_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_aHPC, REML=FALSE) summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff * same_day_dv + order_diff:same_day_dv + real_time_diff:same_day_dv + (0 + vir_time_diff:same_day_dv | sub_id) ## Data: rsa_dat_aHPC ## ## AIC BIC logLik deviance df.resid ## -37579.5 -37526.9 18797.8 -37595.5 5312 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.3143 -0.6830 0.0224 0.6567 3.7470 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id vir_time_diff:same_day_dv 3.092e-08 0.0001758 ## Residual 4.991e-05 0.0070647 ## Number of obs: 5320, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -0.0001895 0.0001219 -1.554 ## vir_time_diff 0.0002373 0.0001237 1.919 ## same_day_dv -0.0001303 0.0001214 -1.074 ## vir_time_diff:same_day_dv 0.0007689 0.0002621 2.934 ## same_day_dv:order_diff 0.0002866 0.0004180 0.686 ## same_day_dv:real_time_diff -0.0005584 0.0004640 -1.203 # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # test against reduced model set.seed(309) # set seed for reproducibility lmm_reduced &lt;- update(lmm_full, ~ . - vir_time_diff:same_day_dv) lmm_aov &lt;- anova(lmm_reduced, lmm_full) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 7-3.76e+04-3.75e+041.88e+04-3.76e+04&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8-3.76e+04-3.75e+041.88e+04-3.76e+048.5710.00342 Mixed Model: Fixed effect of interaction of virtual time with sequence identity on aHPC pattern similarity change when including interaction with other time metrics \\(\\chi^2\\)(1)=8.57, p=0.003 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;, &quot;day&quot;, &quot;interaction virtual time and day&quot;, &quot;interaction order and day&quot;, &quot;interaction real time and day&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,1), &quot;residual&quot;) re_names &lt;- c(&quot;interaction virtual time and day (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: The effect of virtual time differs between same-sequence and different-sequence events in the anterior hippocampus when including interactions with other time metrics&quot;) # convert the huxtable to a flextable for word export stable_lme_aHPC_virtime_same_vs_diff_seq_all_interactions &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: The effect of virtual time differs between same-sequence and different-sequence events in the anterior hippocampus when including interactions with other time metrics fixed effects termestimateSEt-value95% CI intercept-0.0001900.000122-1.55-0.0004280.000049 virtual time0.0002370.0001241.92-0.0000050.000480 day-0.0001300.000121-1.07-0.0003680.000108 interaction virtual time and day0.0007690.0002622.930.0002550.001283 interaction order and day0.0002870.0004180.69-0.0005330.001106 interaction real time and day-0.0005580.000464-1.20-0.0014680.000351 random effects grouptermestimate participantinteraction virtual time and day (SD)0.000176 residualSD0.007065 model comparison modelnparAICLLX2dfp reduced model7-37572.9618793.48 full model8-37579.5318797.778.5710.003 model: ps_change~vir_time_diff*same_day_dv+order_diff:same_day_dv+real_time_diff:same_day_dv+(0+vir_time_diff:same_day_dv|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=factor(term, levels = c(&quot;(Intercept)&quot;, &quot;same_day_dv&quot;, &quot;vir_time_diff&quot;, &quot;vir_time_diff:same_day_dv&quot;, &quot;same_day_dv:order_diff&quot;, &quot;same_day_dv:real_time_diff&quot;))) # dot plot of Fixed Effect Coefficients with CIs sfig_x &lt;- ggplot(data = lmm_full_bm[2:6,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), shape = 16, size = 1) + #scale_fill_manual(values = c(rep(&quot;black&quot;,6), &quot;#DD8D29&quot;)) + scale_color_manual(values = c(ultimate_gray, time_within_across_color, day_time_int_color, time_colors[c(2,3)]), name = &quot;mixed model predictor&quot;, labels = c(&quot;sequence&quot;, &quot;virtual time&quot;, &quot;interaction (virtual time)&quot;, &quot;interaction (order)&quot;, &quot;interaction (real time)&quot;)) + labs(x = element_blank(), y = &quot;fixed effect\\nestimate&quot;) + annotate(geom = &quot;text&quot;, x = 3, y = Inf, label = &quot;**&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + guides(fill = &quot;none&quot;, color = guide_legend(override.aes=list(fill=NA, alpha = 1, size=2, linetype=0), title.position = &quot;top&quot;)) + theme_cowplot() + theme(axis.text.x=element_blank(), legend.position = &quot;bottom&quot;) sfig_x MDS To explore how the event sequences could be arranged in a low-dimensional representational space, we want to run multi-dimensional scaling. To implement these analyses, we use the smacof-package, see also the helpful vignette here. MDS requires a distance matrix as an input. We will use a distance matrix based on the population-level predictions from the linear mixed model fitted to the anterior hippocampus pattern similarity change data as an input. This model captures the interaction between virtual temporal distances and sequence membership of event pairs on pattern similarity in the aHPC. We first initialize a data frame that has one row for each possible pairwise comparison of two events and their (z-scored) absolute virtual temporal distances. We use this data frame as “new” data to predict pattern similarity changes from the linear mixed model. The resulting predicted pairwise similarity changes are converted to a matrix, converted to distances and scaled to the range from 0 to 1. We aimed to explore how hippocampal event representations of the different sequences could be embedded in a low-dimensional representational space to give rise to the positive and negative correlations of pattern similarity change and temporal distances for same-sequence and different-sequence events, respectively. For each pair of events, we generated an expected similarity value (Supplemental Figure 6D) using the fixed effects of the mixed model fitted to hippocampal pattern similarity that captures the interaction between virtual temporal distances and sequence membership (c.f. Figure 5, Supplemental Figure 4IJ, and Supplemental Table 7). Using the predict-method implemented in the lme4-package109, we generated model-derived similarity values for all event pairs given their temporal distances and sequence membership. We chose this approach over the raw pattern similarity values to obtain less noisy estimates of the pairwise distances. # check if virtual time exists #(added for OSF code, which will not have this variable in the workspace yet because it did not run the design illustration script included in the full analysis) if (!exists(&quot;virtual_time&quot;)){virtual_time &lt;- beh_data %&gt;% filter(sub_id==&quot;31&quot;) %&gt;% pull(&quot;virtual_time&quot;)} # virtual time is the same for all subjects # create a tibble to use for MDS # this tibble should have a row for each possible pairwise comparison of two events mds_df &lt;- expand_grid(virtual_time1=virtual_time, virtual_time2=virtual_time) %&gt;% mutate( # add design variables day1 = rep(1:n_days, each=n_days*n_events_day^2), day2 = rep(1:n_days, each=n_events_day, times=n_events_day*n_days), event1 = rep(1:n_events_day, each=n_events_day*n_days, times=n_days), event2 = rep(1:n_events_day, times=n_days^2*n_events_day), idx1 = (day1-1)*5+event1, # for later conversion to matrix ixd2 = (day2-1)*5+event2, # for later conversion to matrix same_day = day1==day2, same_day_dv = plyr::mapvalues(same_day, from = c(FALSE, TRUE), to = c(-1, 1)), .before = virtual_time1) %&gt;% filter(!(day1==day2 &amp; event1 == event2)) %&gt;% # remove diagonal, important to do before z-scoring! mutate(vir_time_diff = scale(abs(virtual_time1-virtual_time2))) # z-scored absolute virtual time difference # for each event pair, get a predicted pattern similarity change value from the mixed model fitted to the aHPC data (ignoring the random effects) mds_df$ps_change_hat &lt;- predict(lmm_aHPC_interaction, newdata=mds_df, re.form=NA) # turn into matrix (can probably be done in a prettier way without the loop) ps_change_hat_mat &lt;- matrix(0, nrow=n_events_day*n_days, ncol=n_events_day*n_days) for (i in 1:nrow(mds_df)){ ps_change_hat_mat[mds_df$idx1[i], mds_df$ixd2[i]] &lt;- mds_df$ps_change_hat[i] } # make plot of the resulting predicted pattern similarity change matrix ps_change_hat_df &lt;- reshape2::melt(ps_change_hat_mat) f0a &lt;- ggplot(ps_change_hat_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile() + scale_y_reverse() + scale_fill_scico(palette = &quot;buda&quot;) + ggtitle(&quot;predicted similarity change&quot;) + xlab(&quot;events by day&quot;) + ylab(&quot;events by day&quot;) + labs(fill = &quot;ps change&quot;) + theme_cowplot() + coord_fixed() #theme(aspect.ratio = 1) # convert to distance matrix ps_dist_hat_mat &lt;- ps_change_hat_mat %&gt;% smacof::sim2diss(method = &quot;corr&quot;) # rescale from 0 to 1 ps_dist_hat_mat &lt;- rescale(ps_dist_hat_mat, c(0,1)) diag(ps_dist_hat_mat)&lt;-NA # make plot of the resulting distance matrix ps_dist_hat_df &lt;- reshape2::melt(ps_dist_hat_mat) f0b &lt;- ggplot(ps_dist_hat_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile() + scale_y_reverse() + scale_fill_scico(palette = &quot;buda&quot;, limits=c(0,1)) + theme_cowplot() + coord_fixed()+ #theme(aspect.ratio = 1) + ggtitle(&quot;corresponding distance matrix&quot;) + xlab(&quot;events by day&quot;) + ylab(&quot;events by day&quot;) + labs(fill = &quot;distance&quot;) f0a +f0b &amp; plot_annotation(tag_levels = &#39;A&#39;, title = &quot;MDS based on mixed model fitted to aHPC pattern similarity change&quot;) &amp; theme(text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), legend.position = &#39;right&#39;, plot.tag = element_text(size = 10)) # make plot of lower triangle only of the resulting distance matrix lt &lt;- upper.tri(ps_dist_hat_mat, diag = TRUE) lt_df &lt;- reshape2::melt(lt) ps_dist_hat_df &lt;- reshape2::melt(ps_dist_hat_mat) ps_dist_hat_df&lt;-ps_dist_hat_df[lt_df$value,] sfig_mds_input &lt;- ggplot(ps_dist_hat_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile() + scale_y_reverse() + scale_fill_scico(palette = &quot;buda&quot;, limits=c(0,1), breaks=c(0,1)) + theme_cowplot() + coord_fixed() + xlab(&quot;events by sequence&quot;) + ylab(&quot;events by sequence&quot;) + guides(fill=guide_colorbar(title=&quot;distance&quot;, title.hjust = 1, barwidth = 0.3, barheight = 0.7)) + theme(text = element_text(size=6, family = font2use), axis.text = element_text(size=6), legend.text=element_text(size=6), legend.title=element_text(size=6), legend.position = c(1,1), legend.justification = c(1,1)) sfig_mds_input # save and print fn &lt;- here(&quot;figures&quot;, &quot;mds_input_mat&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=sfig_mds_input, units = &quot;cm&quot;, width = 3, height = 3, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=sfig_mds_input, units = &quot;cm&quot;, width = 3, height = 3, dpi = &quot;retina&quot;, device = &quot;png&quot;) We are now ready to run MDS. We subject the distance matrix to MDS using an ordinal model (i.e. non-metric MDS) using two dimensions. Because MDS is sensitive to the starting configuration, we run MDS many times using random starting configurations. From the resulting configurations, we pick the one resulting in the lowest stress. This procedure is described in the vignette of the smacof-package, which we use for these analyses. Using the smacof-package111, the model-predicted similarities were converted to distances and the resulting distance matrix (Supplemental Figure 6D) was subjected to non-metric multidimensional scaling using two dimensions. We chose two dimensions to be able to intuitively visualize the results. Because MDS is sensitive to starting values, we ran multidimensional scaling 1000 times with random initial configurations and visualized the resulting configuration with the lowest stress value. Basing this analysis on the model-derived similarities assumes the same relationship of virtual temporal distances for all event pairs from different sequences, but we would like to note that not all solutions we observed, in particular those with higher stress values, resulted in parallel configurations for the four sequences. Metric MDS Let’s begin by testing out MDS using a ratio transformation. # because MDS is sensitive to the start configuration, let&#39;s run it with many different random configurations (take the best in the end) n_rand_starts &lt;- 1000 # maximum number of iterations max_it &lt;- 5000 # run MDS for 2 dimensions n_rand_starts times with random start configuration and store best fit set.seed(106) # set seed for reproducibility stressvec &lt;- rep(NA, n_rand_starts) fitbest &lt;- mds(ps_dist_hat_mat, ndim = 2, itmax = max_it, init = &quot;random&quot;, type = &quot;ratio&quot;) stressvec[1] &lt;- fitbest$stress for(i in 2:n_rand_starts) { fitran &lt;- mds(ps_dist_hat_mat, ndim = 2, itmax = max_it, init = &quot;random&quot;, type = &quot;ratio&quot;) stressvec[i] &lt;- fitran$stress if (fitran$stress &lt; fitbest$stress){ fitbest &lt;- fitran} } # get data frame of resulting configuration conf &lt;- as.data.frame(fitbest$conf) conf$day &lt;- factor(rep(1:n_days, each = n_events_day)) conf$event &lt;- factor(rep(1:n_events_day,n_days)) conf$virtual_time &lt;- virtual_time # visualize as 2D scatter with lines of the configuration ratioMDS_confplot &lt;- ggplot(conf, aes(x=D1,y=D2, group=day, color=virtual_time, fill=virtual_time, shape=event)) + geom_path(aes(linetype=day),size=0.5, color=&quot;grey&quot;) + scale_linetype_manual(values=c(&quot;solid&quot;, &quot;dashed&quot;, &quot;dotted&quot;, &quot;3111&quot;)) + geom_point(size=2) + scale_shape_manual(values = c(21:25)) + scico::scale_fill_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;, breaks=c(8,22)) + scico::scale_color_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + guides(linetype=guide_legend(title = &quot;sequence&quot;, title.position=&quot;top&quot;, title.hjust = 0.5, order=3), shape=guide_legend(override.aes = list(shape = c(21:25), color = NA, fill = &quot;grey&quot;), title.position=&quot;top&quot;, title.hjust = 0.5, order=2), fill=guide_colorbar(title=&quot;virtual time&quot;, title.position=&quot;top&quot;, title.hjust = 1, barwidth = 2, barheight = 0.5, order=1), color= &quot;none&quot; ) + xlab(&quot;dimension 1&quot;) + ylab(&quot;dimension 2&quot;) + theme_cowplot() + coord_fixed() + theme(#aspect.ratio = 1, legend.spacing.x = unit(1, &#39;mm&#39;), legend.key.size = unit(2.5,&quot;mm&quot;), #legend.margin = margin(0,0,0,0.5, unit=&quot;cm&quot;), legend.position = &quot;right&quot;, legend.justification = &quot;left&quot;) ratioMDS_confplot Let’s test how good this fit really is. As described in the smacof-vignette, one way to do so is to run a permutation test to compare the observed stress value against a surrogate distribution generated from applying MDS to a shuffled distance matrix. To visualize the result, we plot a histogram of the permutation distribution as well as the critical (5th percentile) and the observed stress value. # permutation test to assess fit relative to permuted data set.seed(107) # set seed for reproducibility perm_fitbest &lt;- permtest(fitbest, nrep = 1000, verbose = FALSE) perm_fitbest ## ## Call: permtest.smacof(object = fitbest, nrep = 1000, verbose = FALSE) ## ## SMACOF Permutation Test ## Number of objects: 20 ## Number of replications (permutations): 1000 ## ## Observed stress value: 0.366 ## p-value: 0.999 # get p and z-value p_perm &lt;- sum(c(perm_fitbest$stress.obs, perm_fitbest$stressvec) &lt;= perm_fitbest$stress.obs)/(perm_fitbest$nrep+1) z_perm &lt;- (perm_fitbest$stress.obs-mean(perm_fitbest$stressvec))/sd(perm_fitbest$stressvec) # plot of permutation test results perm_fitbest_df &lt;- tibble(stress_perms = perm_fitbest$stressvec) prctile &lt;- as.character(expression(95^{th}*&quot; percentile&quot;)) ratioMDS_permplot &lt;- ggplot(perm_fitbest_df, aes(x=stress_perms)) + geom_histogram(aes(y=stat(width*density)), bins = 50, fill = ultimate_gray) + geom_segment(aes(x = perm_fitbest$stress.obs, y = 0, xend = perm_fitbest$stress.obs, yend = 0.02), color=&quot;#972D15&quot;) + geom_segment(aes(x = quantile(stress_perms, 0.05), y = 0, xend = quantile(stress_perms, 0.05), yend = 0.04), color=&quot;Black&quot;, linetype=&quot;dashed&quot;) + geom_label(aes(x=quantile(stress_perms, probs=0.05), y=0.04, label = prctile), hjust=0, vjust=0, size=6/.pt, fill = &quot;white&quot;, alpha=0.5, color=&quot;BLACK&quot;, parse = TRUE, label.padding = unit(0.1, &quot;lines&quot;), label.size = 1/.pt,) + geom_label(aes(x=perm_fitbest$stress.obs, y=0.02, label=&quot;observed stress&quot;), label.padding = unit(0.1, &quot;lines&quot;), label.size = 1/.pt, hjust=1, vjust=0.5, size=6/.pt, fill = &quot;white&quot;, alpha=0.5, color=&quot;#972D15&quot;) + scale_y_continuous(labels = scales::percent) + ylab(&quot;% of shuffles&quot;) + xlab(&quot;stress (null distr.)&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) ratioMDS_permplot Permutation test of observed stress against shuffles for metric (ratio) MDS: z=3.3, p=0.999 Non-metric MDS Because the permutation test is not significant for metric MDS, we continue with non-metric MDS. # because MDS is sensitive to the start configuration, let&#39;s run it with many different random configurations (take the best in the end) n_rand_starts &lt;- 1000 # maximum number of iterations max_it &lt;- 5000 # run MDS for 2 dimensions n_rand_starts times with random start configuration and store best fit set.seed(106) # set seed for reproducibility stressvec &lt;- rep(NA, n_rand_starts) fitbest &lt;- mds(ps_dist_hat_mat, ndim = 2, itmax = max_it, init = &quot;random&quot;, type = &quot;ordinal&quot;) stressvec[1] &lt;- fitbest$stress for(i in 2:n_rand_starts) { fitran &lt;- mds(ps_dist_hat_mat, ndim = 2, itmax = max_it, init = &quot;random&quot;, type = &quot;ordinal&quot;) stressvec[i] &lt;- fitran$stress if (fitran$stress &lt; fitbest$stress){ fitbest &lt;- fitran} } # get data frame of resulting configuration conf &lt;- as.data.frame(fitbest$conf) conf$day &lt;- factor(rep(1:n_days, each = n_events_day)) conf$event &lt;- factor(rep(1:n_events_day,n_days)) conf$virtual_time &lt;- virtual_time # visualize as 2D scatter with lines of the configuration f5c &lt;- ggplot(conf, aes(x=D1,y=D2, group=day, color=virtual_time, fill=virtual_time, shape=event)) + geom_path(aes(linetype=day),size=0.5, color=&quot;grey&quot;) + scale_linetype_manual(values=c(&quot;solid&quot;, &quot;dashed&quot;, &quot;dotted&quot;, &quot;3111&quot;)) + geom_point(size=2) + scale_shape_manual(values = c(21:25)) + scico::scale_fill_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;, breaks=c(8,22)) + scico::scale_color_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + guides(linetype=guide_legend(title = &quot;sequence&quot;, title.position=&quot;top&quot;, title.hjust = 0.5, order=3), shape=guide_legend(override.aes = list(shape = c(21:25), color = NA, fill = &quot;grey&quot;), title.position=&quot;top&quot;, title.hjust = 0.5, order=2), fill=guide_colorbar(title=&quot;virtual time&quot;, title.position=&quot;top&quot;, title.hjust = 1, barwidth = 2, barheight = 0.5, order=1), color= &quot;none&quot; ) + xlab(&quot;dimension 1&quot;) + ylab(&quot;dimension 2&quot;) + theme_cowplot() + coord_fixed() + theme(#aspect.ratio = 1, legend.spacing.x = unit(1, &#39;mm&#39;), legend.key.size = unit(2.5,&quot;mm&quot;), #legend.margin = margin(0,0,0,0.5, unit=&quot;cm&quot;), legend.position = &quot;right&quot;, legend.justification = &quot;left&quot;) f5c # save source data source_dat &lt;-ggplot_build(f5c)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x, y, colour, group, shape), file = file.path(dirs$source_dat_dir, &quot;f5c.txt&quot;)) #comparing metric and non-metric MDS mds_fig &lt;- f5c + ratioMDS_confplot + plot_layout(guides = &quot;collect&quot;) &amp; theme(text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), plot.tag = element_text(size = 10)) &amp; plot_annotation(tag_levels = &#39;A&#39;) mds_fig[[1]] &lt;- mds_fig[[1]] + ggtitle(&quot;non-metric MDS&quot;) mds_fig[[2]] &lt;- mds_fig[[2]] + ggtitle(&quot;metric MDS&quot;) mds_fig &lt;- mds_fig &amp; theme(text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), plot.tag = element_text(size = 10), legend.position = &quot;bottom&quot;) &amp; plot_annotation(tag_levels = &#39;A&#39;) # save and print (height was 18 when incl. searchlights) fn &lt;- here(&quot;figures&quot;, &quot;mds_fig&quot;) ggsave(paste0(fn, &quot;.png&quot;), plot=mds_fig, units = &quot;cm&quot;, width = 14.4, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;) mds_fig Let’s test how good the non-metric fit really is. As described in the smacof-vignette, one way to do so is to run a permutation test to compare the observed stress value against a surrogate distribution generated from applying MDS to a shuffled distance matrix. To visualize the result, we plot a histogram of the permutation distribution as well as the critical (5th percentile) and the observed stress value. We tested the stress value of the resulting configuration against a surrogate distribution of stress values obtained from permuting the input distances on each of 1000 iterations. Using the mean and standard deviation of the resulting null distribution, we obtained a z-value as a test statistic and report the proportion of stress values in the null distribution that were equal to or smaller than the observed stress value (Supplemental Figure 6E). # permutation test to assess fit relative to permuted data set.seed(107) # set seed for reproducibility perm_fitbest &lt;- permtest(fitbest, nrep = 1000, verbose = FALSE) perm_fitbest ## ## Call: permtest.smacof(object = fitbest, nrep = 1000, verbose = FALSE) ## ## SMACOF Permutation Test ## Number of objects: 20 ## Number of replications (permutations): 1000 ## ## Observed stress value: 0.261 ## p-value: &lt;0.001 # get p and z-value p_perm &lt;- sum(c(perm_fitbest$stress.obs, perm_fitbest$stressvec) &lt;= perm_fitbest$stress.obs)/(perm_fitbest$nrep+1) z_perm &lt;- (perm_fitbest$stress.obs-mean(perm_fitbest$stressvec))/sd(perm_fitbest$stressvec) # plot of permutation test results perm_fitbest_df &lt;- tibble(stress_perms = perm_fitbest$stressvec) prctile &lt;- as.character(expression(95^{th}*&quot; percentile&quot;)) sfig6_e &lt;- ggplot(perm_fitbest_df, aes(x=stress_perms)) + geom_histogram(aes(y=stat(width*density)), bins = 50, fill = ultimate_gray) + geom_segment(aes(x = perm_fitbest$stress.obs, y = 0, xend = perm_fitbest$stress.obs, yend = 0.02), color=&quot;#972D15&quot;) + geom_segment(aes(x = quantile(stress_perms, 0.05), y = 0, xend = quantile(stress_perms, 0.05), yend = 0.04), color=&quot;Black&quot;, linetype=&quot;dashed&quot;) + geom_label(aes(x=quantile(stress_perms, probs=0.05), y=0.04, label = prctile), hjust=0, vjust=0, size=6/.pt, fill = &quot;white&quot;, alpha=0.5, color=&quot;BLACK&quot;, parse = TRUE, label.padding = unit(0.1, &quot;lines&quot;), label.size = 1/.pt,) + geom_label(aes(x=perm_fitbest$stress.obs, y=0.02, label=&quot;observed stress&quot;), label.padding = unit(0.1, &quot;lines&quot;), label.size = 1/.pt, hjust=0, vjust=0.5, size=6/.pt, fill = &quot;white&quot;, alpha=0.5, color=&quot;#972D15&quot;) + scale_y_continuous(labels = scales::percent) + ylab(&quot;% of shuffles&quot;) + xlab(&quot;stress (null distr.)&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) sfig6_e Permutation test of observed stress against shuffles for non-metric (ordinal) MDS: z=-3.5, p=0.001 To test whether the resulting configuration exhibits higher distances between pairs of events that have a high input distance we run a t-test for independent samples based on a median split (high vs. low input distance). # create dataframe that includes event and MDS configuration information idx_mat &lt;- matrix(TRUE, nrow=20, ncol=20) fit_best_df&lt;- tibble(day1 = day[which(lower.tri(idx_mat), arr.ind = TRUE)[,&quot;row&quot;]], day2 = day[which(lower.tri(idx_mat), arr.ind = TRUE)[,&quot;col&quot;]], event1 = event[which(lower.tri(idx_mat), arr.ind = TRUE)[,&quot;row&quot;]], event2 = event[which(lower.tri(idx_mat), arr.ind = TRUE)[,&quot;col&quot;]], delta = as.numeric(fitbest$delta), confdist = as.numeric(fitbest$confdist), same_day = day1 == day2) # median split variable based on input distances (delta) fit_best_df &lt;- fit_best_df %&gt;% mutate(high_low = sjmisc::dicho(fit_best_df$delta), high_low_f = factor(if_else(high_low ==1, &quot;high&quot;, &quot;low&quot;))) # do t-test based on median split stats &lt;- fit_best_df %&gt;% do(tidy(t.test(confdist ~ high_low_f, data = ., var.equal = TRUE))) # Cohen&#39;s d with Hedges&#39; correction for independent samples d&lt;-cohen.d(d = fit_best_df%&gt;%filter(high_low_f == &quot;high&quot;) %&gt;% pull(confdist), f = fit_best_df%&gt;%filter(high_low_f == &quot;low&quot;) %&gt;% pull(confdist), paired=FALSE, hedges.correction=TRUE, noncentral=FALSE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] huxtable(stats) %&gt;% theme_article() estimateestimate1estimate2statisticp.valueparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 0.4191.10.6839.352.71e-171880.3310.507Two Sample t-testtwo.sided1.351.031.67 sfig6_f &lt;- ggplot(fit_best_df, aes(x=high_low, y=confdist, fill=high_low, color=high_low)) + geom_boxplot(width = .1, colour = &quot;black&quot;, outlier.shape = NA, outlier.size = 2) + gghalves::geom_half_violin(data=fit_best_df %&gt;% filter(high_low==0), aes(x=1, y=confdist), position=position_nudge(-0.1), side = &quot;l&quot;, colour=NA) + gghalves::geom_half_violin(data=fit_best_df %&gt;% filter(high_low==1), aes(x=2, y=confdist), position=position_nudge(0.1), side = &quot;r&quot;, colour=NA) + geom_point(aes(x=ifelse(high_low==0, 1.2, 1.8)), alpha = 1, position = position_jitter(width = .1, height = 0), shape = 16, size = 1) + stat_summary(fun = mean, geom = &quot;point&quot;, size=1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;BLACK&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;BLACK&quot;, width = 0, size = 0.5) + ylab(&#39;MDS distance&#39;)+xlab(&#39;input distance&#39;) + annotate(geom = &quot;text&quot;, x = 1.5, y = Inf, label = &#39;underline(&quot; *** &quot;)&#39;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use, parse=TRUE) + scale_x_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + scale_color_manual(values=c(&quot;lightgrey&quot;, &quot;darkgrey&quot;), labels=c(&quot;low&quot;, &quot;high&quot;)) + scale_fill_manual(values=c(&quot;lightgrey&quot;, &quot;darkgrey&quot;)) + guides(color=guide_legend(&quot;input distance&quot;, override.aes = list(shape=16, size=1, linetype=0, fill=NA, alpha=1)), fill = &quot;none&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;, text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8)) sfig6_f Summary Statistics: t-test of resulting MDS distances contrasting event pairs separated by high vs. low input distances (median split): t188=9.35, p=0.000, d=1.35, 95% CI [1.03, 1.67] Additionally, we contrasted the distances between pairs of events in the resulting configuration between distances separated by high or low (median split) input distances using a t-test for independent samples (Supplemental Figure 6F). Using a Spearman correlation, we quantified the relationship of the input distances and the distances in the resulting configuration (Supplemental Figure 6G). To further assess the fit of the configuration to the underlying distance matrix, we calculate the correlation between input distances between event pairs and the distances in the resulting MDS configuration as well as conducting a permutation test to assess the resulting stress value. # correlate input distances and configuration distances stats &lt;- tidy(cor.test(fit_best_df$delta, fit_best_df$confdist, method = &quot;spearman&quot;)) ## Warning in cor.test.default(fit_best_df$delta, fit_best_df$confdist, method = &quot;spearman&quot;): Cannot compute exact p-value with ties # Shepard plot showing input distances and configuration distances sfig6_g &lt;- ggplot(fit_best_df,aes(x = rank(delta), y = rank(confdist))) + geom_point(size = 0.5) + geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ x&quot;, color = &quot;darkgrey&quot;) + #annotate(&quot;text&quot;, x=Inf, y=-Inf, hjust=&quot;inward&quot;, vjust=&quot;inward&quot;, size=6/.pt, # label=sprintf(&quot;r=%.2f\\np=%.3f&quot;, round(stats$estimate, digits=2), round(stats$p.value, digits=3)), # family=font2use) + xlab(&quot;input distance&quot;) + ylab(&quot;MDS distance&quot;) + theme_cowplot() sfig6_g Spearman correlation of input distances and distances in resulting MDS configuration: r=0.46, p=0.000 We are ready to create figure 5: # start figure 5 by making the MDS plot layout = &quot; A A B&quot; f5c_guide &lt;- f5c / guide_area() + plot_layout(design =layout, guides = &quot;collect&quot;) &amp; theme(legend.direction = &quot;horizontal&quot;, legend.box = &quot;vertical&quot;, text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), plot.tag = element_text(size = 10)) # add the three panels together layout = &quot;ABC&quot; f5 &lt;- f5a + f5b + f5c_guide + plot_layout(guides = &quot;keep&quot;, design = layout) &amp; theme(text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), plot.tag = element_text(size = 10)) &amp; plot_annotation(tag_levels = &#39;A&#39;) # save and print (height was 18 when incl. searchlights) fn &lt;- here(&quot;figures&quot;, &quot;f5&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=f5, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=f5, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;) Figure 5. The anterior hippocampus generalizes temporal relations across sequences. A. Z-values show results of participant-specific linear models quantifying the effect of virtual time for event pairs from the same sequence (blue, as in Figure 4B) and from different sequences (red). Temporal distance is negatively related to hippocampal representational change for event pairs from different sequences. See Supplemental Figure 4EF for mixed model analysis of across-sequence comparisons. The effect of virtual time differs for comparisons within the same sequence or between two different sequences. B. To illustrate the effect shown in A, average pattern similarity change values are shown for across–sequence event pairs that are separated by low and high temporal distances based on a median split. C. Multidimensional scaling results show low-dimensional embedding of the event sequences. Shapes indicate event order, color shows virtual times of events. The different lines connect the events belonging to the four sequences for illustration. *** p≤0.001; ** p&lt;0.01; * p&lt;0.05 UMAP To check if the MDS results depend on the specific algorithm, let’s also try the UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction) algorithm on the mixed model distance matrix. # set up umap config based on defaults, but using distance matrix as input and define the two parameters custom_config = umap.defaults custom_config$input = &quot;dist&quot; custom_config$n_neighbors &lt;- 6 # k nearest neighbors: high-values push towards global structure at cost of detail custom_config$min_dist &lt;- 0.1 # minimum distance: high values lead to looser packing, but better topology # run umap using the LME distance matrix as input. use the custom config set.seed(306) # set seed for reproducibility umap_dist = umap(ps_dist_hat_mat, config = custom_config) # turn this solution into a tibble and include info about days and events umap_df &lt;- tibble(day = factor(rep(c(1:n_days), each=n_events_day)), event = factor(rep(1:n_events_day,n_days)), virtual_time &lt;- virtual_time, umapX = umap_dist$layout[,1], umapY = umap_dist$layout[,2]) # visualize as 2D scatter with lines of the configuration sfig6_x &lt;- ggplot(umap_df, aes(x=umapX,y=umapY, group=day, color=virtual_time, fill=virtual_time, shape=event)) + geom_path(aes(linetype=day),size=0.5, color=&quot;grey&quot;) + scale_linetype_manual(values=c(&quot;solid&quot;, &quot;dashed&quot;, &quot;dotted&quot;, &quot;3111&quot;)) + geom_point(size=2) + scale_shape_manual(values = c(21:25)) + scico::scale_fill_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;, breaks=c(8,22)) + scico::scale_color_scico(begin = 0.1, end = 0.7, palette = &quot;devon&quot;) + guides(linetype = guide_legend(title = &quot;sequence&quot;, title.position=&quot;top&quot;, title.hjust = 0.5, order=1), shape = guide_legend(override.aes = list(shape = c(21:25), color = NA, fill = &quot;grey&quot;), title.position=&quot;top&quot;, title.hjust = 0.5, order=2), fill = guide_colorbar(title=&quot;virtual time&quot;, title.position=&quot;top&quot;, title.hjust = 1, barwidth = 0.5, barheight = 1), color = &quot;none&quot; ) + xlab(&quot;dimension 1&quot;) + ylab(&quot;dimension 2&quot;) + theme_cowplot() + coord_fixed() + theme(legend.spacing.x = unit(1, &#39;mm&#39;), legend.key.size = unit(2.5,&quot;mm&quot;), legend.position = &quot;right&quot;, legend.justification = &quot;left&quot;) sfig6_x Let’s create supplemental figure 6 layout = &quot; AADDFF AADDFF BCEEGG&quot; sfig6 &lt;- sfig6_a + sfig6_b + sfig6_c + plot_spacer() + sfig6_e + sfig6_f + sfig6_g + plot_layout(design = layout, guides = &quot;collect&quot;) + plot_annotation(theme = theme(plot.margin = margin(t=0, r=0, b=0, l=0, unit=&quot;cm&quot;)), tag_levels = list(c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;))) &amp; theme(text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.position = &#39;bottom&#39;, plot.tag = element_text(size = 10), legend.margin=margin(r = 0, unit=&#39;cm&#39;)) fn &lt;- here(&quot;figures&quot;, &quot;sf06&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=sfig6, units = &quot;cm&quot;, width = 17.4, height = 12, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=sfig6, units = &quot;cm&quot;, width = 17.4, height = 12, dpi = &quot;retina&quot;, device = &quot;png&quot;) Supplemental Figure 6. Virtual time predicts hippocampal pattern similarity change for events from different sequences. A. Z-values show the relationship of the different time metrics to representational change in the anterior hippocampus based on participant-specific multiple regression analyses for pairs of events from different sequences. Circles show participant-specific Z-values from summary statistics approach; boxplot shows median and upper/lower quartile along with whiskers extending to most extreme data point within 1.5 interquartile ranges above/below the upper/lower quartile; black circle with error bars corresponds to mean±S.E.M.; distribution shows probability density function of data points. B, C. Parameter estimates with 95% CIs (B) and estimated marginal means (C) show the fixed effects of the three time metrics from the corresponding mixed model. * p&lt;0.05 after exclusion of one outlier excluded based on the boxplot criterion. D. A linear mixed model capturing the interaction effect of virtual temporal distances and sequence membership (Figure 5, Supplemental Figure 4IJ) was fitted to hippocampal representational change. An event-by-event similarity matrix was derived from the fixed effects of this model. Similarities were converted distances and then used as input for multidimensional scaling (see Methods). E. The stress value observed in the MDS analysis (red line) was significantly smaller than the 5th percentile (black dashed line) of a surrogate distribution of stress values obtained from shuffling the dissimilarities before running MDS in each of 1000 iterations. F. Pairs of events separated by a large distance in the input distance matrix were separated by a larger Euclidean distance in the resulting MDS configuration (t188=9.35, p&lt;0.001, d=1.35, 95% CI [1.03, 1.67]). *** p &lt;0.001. G. There was a significant Spearman correlation of input distances and MDS configuration distances (r=0.46, p&lt;0.001), but visual inspection reveals a non-linear relationship where very high distances are systematically underestimated in the MDS configuration. This is likely because the data were projected onto only two dimensions for visualization. More dimensions would be needed to improve the fit of the MDS configuration and the input distance matrix. Distances are shown as ranks because non-metric MDS was used (high ranks for high distances). 8.9 alEC So far we focused on the anterior hippocampus. But what about our second region of interest, the anterior-lateral entorhinal cortex (alEC)? alEC: virtual time within/across sequences separately First, we run the t-tests against zero for the alEC, separately for events from the same or two different sequences. set.seed(108) # set seed for reproducibility # run a group-level t-test on the RSA fits from the first level in alEC for within-day stats &lt;- rsa_fit %&gt;% filter(roi == &quot;alEC_lr&quot;, same_day == TRUE) %&gt;% select(z_virtual_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternative -0.233-1.260.2170.21527-0.6110.145One Sample t-testtwo.sided # re-run without outliers (see outliers in subsequent plot) alEC_within &lt;- rsa_fit %&gt;% filter(roi == &quot;alEC_lr&quot;, same_day == TRUE) # find outliers is_outlier &lt;- function(x) { return(x &lt; quantile(x, 0.25) - 1.5 * IQR(x) | x &gt; quantile(x, 0.75) + 1.5 * IQR(x)) } out_ind &lt;- is_outlier(alEC_within$z_virtual_time) # run second-level analysis on data points that are not outliers set.seed(123) # set seed for reproducibility stats &lt;- alEC_within[!out_ind,] %&gt;% select(z_virtual_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=alEC_within[!out_ind,]$z_virtual_time, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.482-3.540.001660.00224-0.763-0.201One Sample t-testtwo.sided-0.685-1.17-0.268 Summary Statistics: t-test against 0 for virtual time within sequence in alEC (outliers excluded) t24=-3.54, p=0.002, d=-0.69, 95% CI [-1.17, -0.27] # run a group-level t-test on the RSA fits from the first level in alEC for across-day set.seed(145) # set seed for reproducibility stats &lt;- rsa_fit %&gt;% filter(roi == &quot;alEC_lr&quot;, same_day == FALSE) %&gt;% select(z_virtual_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit %&gt;% filter(roi == &quot;alEC_lr&quot;, same_day == FALSE))$z_virtual_time, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.253-1.60.1210.12227-0.5770.0709One Sample t-testtwo.sided-0.294-0.6920.0803 Summary Statistics: t-test against 0 for virtual time across sequences in alEC t27=-1.6, p=0.122, d=-0.29, 95% CI [-0.69, 0.08] alEC: virtual time within vs. across sequences Summary Statistics Let’s contrast the effect of virtual time on entorhinal pattern similarity change depending on whether we compare events from the same sequence or from different sequences. set.seed(109) # set seed for reproducibility stats &lt;- rsa_fit %&gt;% filter(roi==&quot;alEC_lr&quot;) %&gt;% # calculate difference between within- an across-day effect for each participant group_by(sub_id) %&gt;% summarise( same_day_diff = z_virtual_time[same_day==TRUE]-z_virtual_time[same_day==FALSE], .groups = &quot;drop&quot;) %&gt;% # test difference against 0 select(same_day_diff) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for paired samples using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit %&gt;% filter(roi == &quot;alEC_lr&quot;, same_day == TRUE))$z_virtual_time, f=(rsa_fit %&gt;% filter(roi == &quot;alEC_lr&quot;, same_day == FALSE))$z_virtual_time, paired=TRUE, pooled=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 0.01990.0730.9420.94227-0.5390.579One Sample t-testtwo.sided0.0213-0.3630.391 Summary Statistics: paired t-test comparing effect of virtual time within and across sequences in alEC t27=0.07, p=0.942, d=0.02, 95% CI [-0.36, 0.39] To finish this, we make a plot of the same and different sequence effect to go into the supplement. # generate custom repeating jitter (multiply with -1/1 to shift towards each other) plot_dat &lt;- rsa_fit %&gt;% filter(roi==&quot;alEC_lr&quot;) %&gt;% mutate( same_day_num = plyr::mapvalues(same_day, from = c(&quot;FALSE&quot;, &quot;TRUE&quot;),to = c(1,0)), same_day_char = plyr::mapvalues(same_day, from = c(&quot;FALSE&quot;, &quot;TRUE&quot;), to = c(&quot;different&quot;, &quot;same&quot;)), same_day_char = fct_relevel(same_day_char, c(&quot;same&quot;, &quot;different&quot;)), x_jit = as.numeric(same_day_char) + rep(jitter(rep(0,n_subs), amount=0.05), each=2) * rep(c(-1,1),n_subs)) sfig7_a &lt;- ggplot(data=plot_dat, aes(x=same_day_char, y=z_virtual_time, fill = same_day_char, color = same_day_char)) + geom_boxplot(aes(group=same_day_char), position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + scale_fill_manual(values = unname(c(aHPC_colors[&quot;within_main&quot;], aHPC_colors[&quot;across_main&quot;]))) + scale_color_manual(values = unname(c(aHPC_colors[&quot;within_main&quot;], aHPC_colors[&quot;across_main&quot;])), name = &quot;sequence&quot;, labels=c(&quot;same&quot;, &quot;different&quot;)) + gghalves::geom_half_violin(data = plot_dat %&gt;% filter(same_day == TRUE), aes(x=same_day_char, y=z_virtual_time), position=position_nudge(-0.1), side = &quot;l&quot;, color = NA) + gghalves::geom_half_violin(data = plot_dat %&gt;% filter(same_day == FALSE), aes(x=same_day_char, y=z_virtual_time), position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;, width = 0, size = 0.5) + geom_line(aes(x = x_jit, group=sub_id,), color = ultimate_gray, position = position_nudge(c(0.15, -0.15))) + geom_point(aes(x=x_jit, fill = same_day_char), position = position_nudge(c(-0.15, 0.15)), shape=16, size = 1) + ylab(&#39;z RSA model fit&#39;) + xlab(&#39;sequence&#39;) + guides(fill = &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2))) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;**&quot;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use) + annotate(geom = &quot;text&quot;, x = 1.5, y = Inf, label = &#39;underline(&quot; n.s. &quot;)&#39;, hjust = 0.5, vjust=1, size = 8/.pt, family=font2use, parse=TRUE) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;) sfig7_a Mixed Effects Test the interaction between sequence membership and virtual time on alEC representational change in mixed model analysis. # get the data rsa_dat_alEC &lt;- rsa_dat %&gt;% filter(roi == &quot;alEC_lr&quot;) # test the interaction for alEC # maximal random effect structure --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + (1 + vir_time_diff * same_day_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_alEC, REML=FALSE) ## boundary (singular) fit: see ?isSingular # no correlations with random intercept --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + (1 + vir_time_diff * same_day_dv || sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_alEC, REML=FALSE) ## boundary (singular) fit: see ?isSingular # random intercepts and random slopes only for the critical interaction --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + (1 + vir_time_diff : same_day_dv || sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_alEC, REML=FALSE) ## boundary (singular) fit: see ?isSingular # random slopes for critical interactions set.seed(378) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv + (0 + vir_time_diff : same_day_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_alEC, REML=FALSE) ## boundary (singular) fit: see ?isSingular summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff * same_day_dv + (0 + vir_time_diff:same_day_dv | sub_id) ## Data: rsa_dat_alEC ## ## AIC BIC logLik deviance df.resid ## -29766.7 -29727.3 14889.4 -29778.7 5314 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.5825 -0.6461 0.0014 0.6380 4.1761 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id vir_time_diff:same_day_dv 0.000000 0.00000 ## Residual 0.000217 0.01473 ## Number of obs: 5320, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.0002612 0.0002521 1.036 ## vir_time_diff -0.0005716 0.0002549 -2.242 ## same_day_dv 0.0001285 0.0002521 0.510 ## vir_time_diff:same_day_dv -0.0002320 0.0002549 -0.910 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # test against reduced model set.seed(370) # set seed for reproducibility lmm_reduced &lt;- update(lmm_full, ~ . - vir_time_diff:same_day_dv) ## boundary (singular) fit: see ?isSingular lmm_aov &lt;- anova(lmm_reduced, lmm_full) Mixed Model: Fixed effect of interaction of virtual time with sequence identity on alEC pattern similarity change \\(\\chi^2\\)(1)=0.83, p=0.363 alEC: virtual time for all event pairs Because there is no significant difference when we look at event pairs from the same vs. different days in the above analysis, we collapse across all comparisons to test for an effect of virtual time on entorhinal pattern similarity change. Summary Statistics set.seed(110) # set seed for reproducibility # do RSA using linear model and calculate z-score for model fit from 1000 permutations rsa_fit_ec &lt;- rsa_dat %&gt;% filter(roi == &quot;alEC_lr&quot;) %&gt;% group_by(sub_id) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;ps_change ~ vir_time_diff&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;)))) %&gt;% unnest_wider(z) # run a group-level t-test on the RSA fits from the first level in alEC for both within-day and across days stats &lt;- rsa_fit_ec %&gt;% select(z_virtual_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=rsa_fit_ec$z_virtual_time, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.335-2.310.0290.029427-0.634-0.0371One Sample t-testtwo.sided-0.423-0.835-0.0451 Summary Statistics: t-test against 0 for virtual time for all pairs in alEC t27=-2.31, p=0.029, d=-0.42, 95% CI [-0.84, -0.05] Linear Mixed Effects When fitting a linear mixed effects model to test the effect of virtual time on pattern similarity change in the alEC, we thus use all pairwise comparisons. Due to singular fits we reduce the random effects structure to only include random slopes for virtual time. The model is still singular, but we keep these random slopes in the model to avoid anti-conservativity and to account for the within-subject dependencies. # extract the comparisons from the same day rsa_dat_ec &lt;- rsa_dat %&gt;% filter(roi == &quot;alEC_lr&quot;) # define the full model with virtual time difference as # fixed effect and random intercepts for subject --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff + (1 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_ec, REML = FALSE) ## boundary (singular) fit: see ?isSingular # remove the correlation between random slopes and intercepts --&gt; still singular formula &lt;- &quot;ps_change ~ vir_time_diff + (1 + vir_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_ec, REML = FALSE) ## boundary (singular) fit: see ?isSingular # keep only random slopes --&gt; still singular set.seed(301) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff + (0 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_ec, REML = FALSE) ## boundary (singular) fit: see ?isSingular #lmm_full = lme4::lmer(formula, data=rsa_dat_mtl, REML=FALSE, # control = lmerControl(optimizer =&quot;bobyqa&quot;, optCtrl=list(maxfun=100000))) #lmm_full = lmer(formula, data=rsa_dat_mtl, REML=FALSE, # control = lmerControl(optimizer =&#39;nloptwrap&#39;, # optCtrl=list(method=&#39;NLOPT_LN_NELDERMEAD&#39;, maxfun=100000))) #lmm_full = lme4::lmer(formula, data=rsa_dat_mtl, REML=FALSE, # control = lmerControl(optimizer =&#39;optimx&#39;, optCtrl=list(method=&#39;nlminb&#39;))) summary(lmm_full, corr=FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff + (0 + vir_time_diff | sub_id) ## Data: rsa_dat_ec ## ## AIC BIC logLik deviance df.resid ## -29769.8 -29743.5 14888.9 -29777.8 5316 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.5888 -0.6468 0.0016 0.6373 4.1684 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id vir_time_diff 0.0000000 0.00000 ## Residual 0.0002171 0.01473 ## Number of obs: 5320, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.0001670 0.0002020 0.827 ## vir_time_diff -0.0004237 0.0002023 -2.095 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # test against reduced model set.seed(317) # set seed for reproducibility formula &lt;- &quot;ps_change ~ 1 + (0 + vir_time_diff | sub_id)&quot; lmm_no_vir_time &lt;- lme4::lmer(formula, data = rsa_dat_ec, REML = FALSE) ## boundary (singular) fit: see ?isSingular lmm_aov &lt;- anova(lmm_no_vir_time, lmm_full) Mixed Model: Fixed effect of virtual time on alEC pattern similarity change (all pairs) \\(\\chi^2\\)(1)=4.39, p=0.036 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,1), &quot;residual&quot;) re_names &lt;- c(&quot;virtual time (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Virtual time explains representational change in the anterior-lateral entorhinal cortex (all events)&quot;) # convert the huxtable to a flextable for word export stable_lme_alEC_virtime_all_events &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Virtual time explains representational change in the anterior-lateral entorhinal cortex (all events) fixed effects termestimateSEt-value95% CI intercept0.0001670.0002020.83-0.0002290.000563 virtual time-0.0004240.000202-2.09-0.000820-0.000027 random effects grouptermestimate participantvirtual time (SD)0.000000 residualSD0.014734 model comparison modelnparAICLLX2dfp reduced model3-29767.3914886.69 full model4-29769.7714888.894.3910.036 model: ps_change~vir_time_diff+(0+vir_time_diff|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation Visualize the effects. lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=as.factor(term)) # dot plot of Fixed Effect Coefficients with CIs sfigmm_k &lt;- ggplot(data = lmm_full_bm[2,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), shape = 16, size = 1) + scale_fill_manual(values = unname(alEC_colors[&quot;main&quot;])) + scale_color_manual(values = unname(alEC_colors[&quot;main&quot;]), labels = c(&quot;Virtual Time&quot;)) + labs(x = &quot;virtual time&quot;, y=&quot;fixed effect\\ncoefficient&quot;, color = &quot;Time Metric&quot;) + guides(color = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) sfigmm_k # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% get_complete_df() # plot marginal means sfigmm_l &lt;- ggplot(data = lmm_full_emm, aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, linetype=0) + scale_color_manual(values = unname(alEC_colors[&quot;main&quot;])) + scale_fill_manual(values = unname(alEC_colors[&quot;main&quot;])) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;z virtual time&#39;) + guides(color = &quot;none&quot;, fill = &quot;none&quot;) + theme_cowplot() sfigmm_l Pattern Similarity Change Visualization To further visualize the effect, lets make a plot of the raw pattern similarity change for high vs. low temporal distances, averaged across all pairs. # get data from alEC rsa_dat_ec &lt;- rsa_dat %&gt;% filter(roi == &quot;alEC_lr&quot;) # for each subject dichotomize virtual time difference # based on a median split, then average high/low temporal distances ps_change_means &lt;-rsa_dat_ec %&gt;% group_by(sub_id) %&gt;% mutate(temp_dist = sjmisc::dicho(vir_time_diff, as.num = TRUE)+1) %&gt;% group_by(sub_id, temp_dist) %&gt;% summarise(ps_change = mean(ps_change), .groups = &#39;drop&#39;) # make the high/low temporal distance variable a factor ps_change_means &lt;- ps_change_means %&gt;% mutate( temp_dist = as.numeric(temp_dist), temp_dist_f = factor(temp_dist), temp_dist_f = plyr::mapvalues(temp_dist, from = c(1,2), to = c(&quot;low&quot;, &quot;high&quot;)), temp_dist_f = factor(temp_dist_f, levels= c(&quot;low&quot;, &quot;high&quot;)), roi = &quot;alEC&quot;) # add a column with subject-specific jitter # generate custom repeating jitter (multiply with -1/1 to shift towards each other) ps_change_means &lt;- ps_change_means %&gt;% mutate( x_jit = as.numeric(temp_dist) + rep(jitter(rep(0,n_subs), amount = 0.05), each=2) * rep(c(-1,1),n_subs)) f6c&lt;- ggplot(data=ps_change_means, aes(x=temp_dist, y=ps_change, fill = roi, color = roi)) + gghalves::geom_half_violin(data = ps_change_means %&gt;% filter(temp_dist_f == &quot;low&quot;), aes(x=temp_dist, y=ps_change), position=position_nudge(-0.1), side = &quot;l&quot;, color = NA) + gghalves::geom_half_violin(data = ps_change_means %&gt;% filter(temp_dist_f == &quot;high&quot;), aes(x=temp_dist, y=ps_change), position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_boxplot(aes(group = temp_dist),width = .1, colour = &quot;black&quot;, outlier.shape = NA) + geom_line(aes(x = x_jit, group=sub_id,), color = ultimate_gray, position = position_nudge(c(0.15, -0.15))) + geom_point(aes(x=x_jit), position = position_nudge(c(0.15, -0.15)), shape=16, size = 1) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;, width = 0, size = 0.5) + scale_fill_manual(values = unname(alEC_colors[&quot;main&quot;])) + scale_color_manual(values = unname(alEC_colors[&quot;main&quot;]))+ scale_x_continuous(breaks = c(1,2), labels=c(&quot;low&quot;, &quot;high&quot;)) + ylab(&#39;pattern similarity change&#39;) + xlab(&#39;temporal distance&#39;) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;, strip.background = element_blank(), strip.text = element_blank()) f6c # save source data source_dat &lt;-ggplot_build(f6c)$data[[4]] readr::write_tsv(source_dat %&gt;% select(x, y, group), file = file.path(dirs$source_dat_dir, &quot;f6c.txt&quot;)) Now we are ready to make figure 6 of the manuscript. layoutBC &lt;- &quot;BCC&quot; f6bc &lt;- f6b + f6c + plot_layout(design = layoutBC) f6a_guide &lt;- guide_area() + f6a + plot_layout(design = &quot;ABBBB&quot;, guides = &quot;collect&quot;) layout &lt;- &quot; AAA BCC BCC BCC&quot; f6 &lt;- f6a + f6b + f6c + plot_layout(design = layout) &amp; theme(text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.position = &#39;none&#39;) &amp; plot_annotation(theme = theme(plot.margin = margin(t=0, r=-5, b=0, l=-5, unit=&quot;pt&quot;)), tag_levels = &#39;A&#39;) f6[[1]] &lt;- f6[[1]] + theme_no_ticks() # save and print fn &lt;- here(&quot;figures&quot;, &quot;f6&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=f6, units = &quot;cm&quot;, width = 8.8, height = 12, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=f6, units = &quot;cm&quot;, width = 8.8, height = 12, dpi = &quot;retina&quot;, device = &quot;png&quot;) Figure 6. The anterior-lateral entorhinal cortex uses a shared representational format for relations of events from the same and different sequences. A. The anterior-lateral entorhinal cortex region of interest is displayed on the MNI template with voxels outside the field of view shown in lighter shades of gray. Color code denotes probability of a voxel to be included based on participant-specific masks (see Methods). B. Z-values for participant-specific RSA model fits show a negative relationship between pattern similarity change and virtual temporal distances when collapsing across all event pairs. C. To illustrate the effect in B, raw pattern similarity change in the anterior-lateral entorhinal cortex was averaged for events separated by low and high temporal distances based on a median split. * p&lt;0.05 alEC: virtual time for all event pairs when including order &amp; real time We also asked whether virtual time explains alEC pattern similarity change when including order and real time in the model as predictors of no interest. Based on the findings above, we collapse across event pairs from the same and from different sequences. Summary Statistics # SUMMARY STATS alEC WITH ORDER AND REAL TIME set.seed(59) # set seed for reproducibility # extract all comparisons from the same day rsa_dat_alEC &lt;- rsa_dat %&gt;% filter(roi == &quot;alEC_lr&quot;) # do RSA using linear model and calculate z-score for model fit from permutations rsa_fit_alEC_mult_reg &lt;- rsa_dat_alEC %&gt;% group_by(sub_id) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;, &quot;z_order&quot;, &quot;z_real_time&quot;)))) %&gt;% unnest_longer(z) %&gt;% filter(z_id != &quot;z_intercept&quot;) # run group-level t-tests on the RSA fits from the first level in alEC (virtual time) stats &lt;- rsa_fit_alEC_mult_reg %&gt;% filter(z_id ==&quot;z_virtual_time&quot;) %&gt;% select(z) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit_alEC_mult_reg %&gt;% filter(z_id == &quot;z_virtual_time&quot;))$z, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.144-0.7020.4890.49527-0.5670.278One Sample t-testtwo.sided-0.129-0.5130.245 Summary Statistics: t-test against 0 for virtual time (all comparisons) in aLEC with order and real time in the model t27=-0.7, p=0.495, d=-0.13, 95% CI [-0.51, 0.25] # make z_id a factor and reorder it rsa_fit_alEC_mult_reg &lt;- rsa_fit_alEC_mult_reg %&gt;% mutate( z_id = factor(z_id, levels = c(&quot;z_virtual_time&quot;, &quot;z_order&quot;, &quot;z_real_time&quot;))) # raincloud plot sfig7_b &lt;- ggplot(rsa_fit_alEC_mult_reg, aes(x=z_id, y=z, fill = z_id, colour = z_id)) + gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_point(aes(x = as.numeric(z_id)-0.2), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ ylab(&#39;z RSA model fit&#39;) + xlab(&#39;time metric&#39;) + scale_x_discrete(labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;)) + scale_color_manual(values = c(unname(alEC_colors[&quot;main&quot;]), time_colors[c(2,3)]), name = &quot;time metric&quot;, labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;)) + scale_fill_manual(values = c(unname(alEC_colors[&quot;main&quot;]), time_colors[c(2,3)])) + guides(fill = &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2))) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;) sfig7_b Try real time in alEC for response letter. set.seed(110) # set seed for reproducibility # do RSA using linear model and calculate z-score for model fit from 1000 permutations rsa_fit_ec &lt;- rsa_dat %&gt;% filter(roi == &quot;alEC_lr&quot;) %&gt;% group_by(sub_id) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;ps_change ~ real_time_diff&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_real_time&quot;)))) %&gt;% unnest_wider(z) # run a group-level t-test on the RSA fits from the first level in alEC for both within-day and across days stats &lt;- rsa_fit_ec %&gt;% select(z_real_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=rsa_fit_ec$z_real_time, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.296-2.170.03890.038227-0.576-0.0162One Sample t-testtwo.sided-0.398-0.807-0.021 Summary Statistics: t-test against 0 for real time for all pairs in alEC t27=-2.17, p=0.038, d=-0.40, 95% CI [-0.81, -0.02] # run group-level t-tests on the RSA fits from the first level in alEC [real time] stats &lt;- rsa_fit_alEC_mult_reg %&gt;% filter(z_id ==&quot;z_real_time&quot;) %&gt;% select(z) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit_alEC_mult_reg %&gt;% filter(z_id == &quot;z_virtual_time&quot;))$z, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 0.1480.7640.4510.44627-0.250.547One Sample t-testtwo.sided-0.129-0.5130.245 Summary Statistics: t-test against 0 for real time (all comparisons) in aLEC with order and virtual time in the model t27=0.76, p=0.446, d=-0.13, 95% CI [-0.51, 0.25] # run group-level t-tests on the RSA fits from the first level in alEC [order] stats &lt;- rsa_fit_alEC_mult_reg %&gt;% filter(z_id ==&quot;z_order&quot;) %&gt;% select(z) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit_alEC_mult_reg %&gt;% filter(z_id == &quot;z_order&quot;))$z, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.155-0.8850.3840.38427-0.5140.204One Sample t-testtwo.sided-0.162-0.5490.211 Summary Statistics: t-test against 0 for order (all comparisons) in aLEC with virtual and real time in the model t27=-0.89, p=0.384, d=-0.16, 95% CI [-0.55, 0.21] Linear Mixed Effects # LME alEC WITH ORDER AND REAL TIME set.seed(59) # set seed for reproducibility # extract all comparisons from the same day rsa_dat_alEC &lt;- rsa_dat %&gt;% filter(roi == &quot;alEC_lr&quot;) # define the full model with virtual time difference, order difference and real time difference as # fixed effects and random intercepts and random slopes for all effects # fails to converge and is singular after restart formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff + order_diff + real_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_alEC, REML = FALSE) ## boundary (singular) fit: see ?isSingular # remove the correlation between random slopes and random intercepts formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff + order_diff + real_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_alEC, REML = FALSE) ## boundary (singular) fit: see ?isSingular # thus we reduce the random effect structure by excluding random slopes for the fixed effects of no interest # as above this results in a singular fit (the random intercept variance estimated to be 0) formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_alEC, REML = FALSE) ## boundary (singular) fit: see ?isSingular # next step is to remove the correlation between random slopes and random intercepts formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (1+vir_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_alEC, REML = FALSE) ## boundary (singular) fit: see ?isSingular # we thus reduce further and keep only the random slopes for virtual time differences, but fit is still singular set.seed(332) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff + order_diff + real_time_diff + (0 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_alEC, REML = FALSE) ## boundary (singular) fit: see ?isSingular summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff + order_diff + real_time_diff + (0 + vir_time_diff | sub_id) ## Data: rsa_dat_alEC ## ## AIC BIC logLik deviance df.resid ## -29766.6 -29727.1 14889.3 -29778.6 5314 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.5966 -0.6483 0.0047 0.6405 4.1872 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id vir_time_diff 0.0000000 0.00000 ## Residual 0.0002171 0.01473 ## Number of obs: 5320, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.0001670 0.0002020 0.827 ## vir_time_diff -0.0005762 0.0005307 -1.086 ## order_diff -0.0007122 0.0008621 -0.826 ## real_time_diff 0.0008616 0.0009656 0.892 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # one way of testing for significance is by comparing the likelihood against a simpler model. # Here, we drop the effect of virtual time difference and run an ANOVA. See e.g. Bodo Winter tutorial set.seed(321) # set seed for reproducibility formula &lt;- &quot;ps_change ~ order_diff + real_time_diff + (0 + vir_time_diff | sub_id)&quot; lmm_no_vir_time &lt;- lme4::lmer(formula, data = rsa_dat_alEC, REML = FALSE) ## boundary (singular) fit: see ?isSingular lmm_aov &lt;- anova(lmm_no_vir_time, lmm_full) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 5-2.98e+04-2.97e+041.49e+04-2.98e+04&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6-2.98e+04-2.97e+041.49e+04-2.98e+041.1810.278 Mixed Model: Fixed effect of virtual time on alEC (all event pairs) pattern similarity change with order and real time in model \\(\\chi^2\\)(1)=1.18, p=0.278 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,1), &quot;residual&quot;) re_names &lt;- c(&quot;virtual time (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Virtual time does not explain representational change for different-sequence events in the anterior-lateral entorhinal cortex when including order and real time in the model&quot;) # convert the huxtable to a flextable for word export stable_lme_alEC_virtime_all_comps_time_metrics &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Virtual time does not explain representational change for different-sequence events in the anterior-lateral entorhinal cortex when including order and real time in the model fixed effects termestimateSEt-value95% CI intercept0.0001670.0002020.83-0.0002290.000563 virtual time-0.0005760.000531-1.09-0.0016170.000464 order-0.0007120.000862-0.83-0.0024020.000978 real time0.0008620.0009660.89-0.0010310.002754 random effects grouptermestimate participantvirtual time (SD)0.000000 residualSD0.014733 model comparison modelnparAICLLX2dfp reduced model5-29767.4114888.71 full model6-29766.5914889.301.1810.278 model: ps_change~vir_time_diff+order_diff+real_time_diff+(0+vir_time_diff|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation Mixed model plots. # set RNG set.seed(23) lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=as.factor(term) %&gt;% factor(levels = c(&quot;vir_time_diff&quot;, &quot;order_diff&quot;, &quot;real_time_diff&quot;))) # dot plot of Fixed Effect Coefficients with CIs sfig7_c &lt;- ggplot(data = lmm_full_bm[2:4,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), shape = 16, size = 1) + scale_fill_manual(values = c(unname(alEC_colors[&quot;main&quot;]), time_colors[c(2,3)])) + scale_color_manual(values = c(unname(alEC_colors[&quot;main&quot;]), time_colors[c(2,3)]), labels = c(&quot;Virtual Time&quot;, &quot;Order&quot;, &quot;Real Time&quot;)) + labs(x = &quot;time metric&quot;, y = &quot;fixed effect\\nestimate&quot;, color = &quot;Time Metric&quot;) + guides(color = &quot;none&quot;, fill = &quot;none&quot;) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) sfig7_c # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% get_complete_df # convert the group variable to a factor to control the order of facets below lmm_full_emm$group &lt;- factor(lmm_full_emm$group, levels = c(&quot;vir_time_diff&quot;, &quot;order_diff&quot;, &quot;real_time_diff&quot;)) # plot marginal means sfig7_d &lt;- ggplot(data = lmm_full_emm, aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, linetype=0) + scale_color_manual(values = c(unname(alEC_colors[&quot;main&quot;]), time_colors[c(2,3)]), name = element_blank(), labels = c(&quot;virtual time&quot;, &quot;order&quot;, &quot;real time&quot;)) + scale_fill_manual(values = c(unname(alEC_colors[&quot;main&quot;]), time_colors[c(2,3)])) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;z time metric&#39;) + guides(fill = &quot;none&quot;, color= &quot;none&quot;) + theme_cowplot() + theme(plot.title = element_text(hjust = 0.5), strip.background = element_blank(), strip.text.x = element_blank()) sfig7_d 8.9.0.1 alEC Supplemental Figure layout = &quot; AAABBBC AAABBBD&quot; sfig7 &lt;- sfig7_a + sfig7_b + sfig7_c + sfig7_d + plot_layout(design = layout, guides = &#39;collect&#39;) &amp; theme(text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.position = &quot;bottom&quot;) &amp; plot_annotation(tag_levels = &#39;A&#39;) # save and print to screen fn &lt;- here(&quot;figures&quot;, &quot;sf07&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=sfig7, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=sfig7, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;) Supplemental Figure 7. Pattern similarity change in the anterior-lateral entorhinal cortex. A. Relationship of pattern similarity change and temporal distances between events from the same and different sequences in the anterior-lateral entorhinal cortex. There was no statistically significant difference between correlations of virtual temporal distances and representational change in the anterior-lateral entorhinal cortex depending on whether event pairs were from the same or different sequences. Entorhinal representational change was negatively related to temporal distances between events from the same sequence (summary statistics: t24=-3.54, p=0.002, d=-0.69, 95% CI [-1.17, -0.27]; α=0.025, corrected for separate tests of events of the same and different sequences; three outliers excluded based on the boxplot criterion). The relationship between entorhinal pattern similarity change for events from different sequences was not statistically different from zero (summary statistics: t27=-1.60, p=0.122, d=-0.29, 95% CI [-0.69, 0.08]; α=0.025, corrected for separate tests of events of the same and different sequences). ** p&lt;0.01 after outlier exclusion. B. Z-values show the relationship of the different time metrics to representational change in the anterior-lateral entorhinal cortex based on participant-specific multiple regression analyses. Analysis includes all pairs of events. C, D. Parameter estimates with 95% CI (C) and estimated marginal means (D) show the fixed effects of the three time metrics from the corresponding mixed model. A,B. Circles show participant-specific Z-values from summary statistics approach; boxplot shows median and upper/lower quartile along with whiskers extending to most extreme data point within 1.5 interquartile ranges above/below the upper/lower quartile; black circle with error bars corresponds to mean±S.E.M.; distribution shows probability density function of data points. 8.10 aHPC vs. alEC The findings above suggest differences between how sequence memories in aHPC and alEC are shaped by virtual time depending on whether events from the same or from different sequences are compared. Let’s qualify these differences by running models with both ROIs. Summary Statistics Permutation-based repeated measures ANOVAs were carried out using the permuco package111 and we report generalized η2 as effect sizes computed using the afex-package112. In the summary statistics approach, this comes down to a 2x2 repeated measures ANOVA with the factors ROI (aHPC vs. alEC) and sequence (same vs. different). We use a permutation-based repeated measures ANOVA to show there is a significant interaction between the two factors. # sanity check &amp; effect size: parametric ANOVA using # run a 2x2 repeated measures ANOVA from afex package aov_fit &lt;- afex::aov_ez(id = &quot;sub_id&quot;, dv = &quot;z_virtual_time&quot;, data = rsa_fit, within = c(&quot;roi&quot;, &quot;same_day&quot;), print.formula = TRUE, es=&quot;ges&quot;) ## Formula send to aov_car: z_virtual_time ~ 1 + Error(sub_id/(roi * same_day)) # print stats aov_fit ## Anova Table (Type 3 tests) ## ## Response: z_virtual_time ## Effect df MSE F ges p.value ## 1 roi 1, 27 0.82 3.10 + .024 .089 ## 2 same_day 1, 27 1.22 7.41 * .082 .011 ## 3 roi:same_day 1, 27 1.09 7.76 ** .076 .010 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 set.seed(111) # set seed for reproducibility # permutation repeated measures from permuco package (use factorial same_day variable) aov_fit_perm &lt;- permuco::aovperm(formula = z_virtual_time ~ roi * same_day_char + Error(sub_id/(roi*same_day_char)), data = rsa_fit, method = &quot;Rd_kheradPajouh_renaud&quot;, np =n_perm) summary(aov_fit_perm) SSndfnSSddfdMSEnMSEdFparametric P(&gt;F)permutation P(&gt;F) 2.55122.2272.550.8223.1&nbsp;0.0895&nbsp;0.0859 9.06133&nbsp;&nbsp;279.061.22&nbsp;7.410.0112&nbsp;0.0115 8.43129.3278.431.09&nbsp;7.760.009640.0102 Summary Statistics: permutation-based repeated measures ANOVA (Rd_kheradPajouh_renaud method) main effect of ROI: F1,27=3.1, p=0.086, \\(\\eta^2\\)=0.02 main effect of sequence: F1,27=7.41, p=0.012, \\(\\eta^2\\)=0.08 interaction ROI and sequence: F1,27=7.76, p=0.010, \\(\\eta^2\\)=0.08 ### Linear Mixed Effects {-} To test whether the hippocampus and entorhinal cortex differentially represent virtual time between events within a virtual day and also virtual days, we also run a linear mixed model on pattern similarity change in both regions that includes event pairs that are from the same day, but also from different days. We include the fixed effects ROI (aHPC and alEC), day (same or different) and virtual time difference as well as their interaction in the model. # get data from aHPC and alEC for within and across day comparisons rsa_dat_mtl &lt;- rsa_dat %&gt;% filter(roi == &quot;aHPC_lr&quot; | roi == &quot;alEC_lr&quot;) # recode the ROI factor to -1 and 1 rsa_dat_mtl &lt;- rsa_dat_mtl %&gt;% mutate( roi_dv = plyr::mapvalues(roi, from = c(&quot;alEC_lr&quot;, &quot;aHPC_lr&quot;), to = c(-1, 1))) # run LME for effect of time with factors same/different day vs. ROI # random intercepts and all random slopes and interactions --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv * roi_dv + (1 + vir_time_diff * same_day_dv * roi_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_mtl, REML=FALSE, control = lmerControl(optimizer =&quot;bobyqa&quot;, optCtrl=list(maxfun=40000))) ## boundary (singular) fit: see ?isSingular # remove correlation between random slopes and random intercepts --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv * roi_dv + (1 + vir_time_diff * same_day_dv * roi_dv || sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_mtl, REML=FALSE, control = lmerControl(optimizer =&quot;bobyqa&quot;, optCtrl=list(maxfun=40000))) ## boundary (singular) fit: see ?isSingular # keep only highest-order interaction as random slopes formula &lt;- &quot;ps_change ~ vir_time_diff * same_day_dv * roi_dv + (1 + vir_time_diff : same_day_dv : roi_dv | sub_id)&quot; lmm_full = lme4::lmer(formula, data=rsa_dat_mtl, REML=FALSE, control = lmerControl(optimizer =&quot;bobyqa&quot;, optCtrl=list(maxfun=40000))) ## boundary (singular) fit: see ?isSingular #lmm_full = lme4::lmer(formula, data=rsa_dat_mtl, REML=FALSE, # control = lmerControl(optimizer =&#39;optimx&#39;, optCtrl=list(method=&#39;nlminb&#39;, maxit=100000))) set.seed(398) # set seed for reproducibility lmm_full = lme4::lmer(formula, data=rsa_dat_mtl, REML=FALSE, control = lmerControl(optimizer =&#39;nloptwrap&#39;, optCtrl=list(method=&#39;NLOPT_LN_NELDERMEAD&#39;, maxfun=100000))) ## boundary (singular) fit: see ?isSingular summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff * same_day_dv * roi_dv + (1 + vir_time_diff:same_day_dv:roi_dv | sub_id) ## Data: rsa_dat_mtl ## Control: lmerControl(optimizer = &quot;nloptwrap&quot;, optCtrl = list(method = &quot;NLOPT_LN_NELDERMEAD&quot;, maxfun = 1e+05)) ## ## AIC BIC logLik deviance df.resid ## -64704.2 -64595.1 32367.1 -64734.2 10625 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.9391 -0.5647 0.0096 0.5629 5.3781 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## sub_id (Intercept) 2.460e-07 0.0004960 ## vir_time_diff:same_day_dv:roi_dv1 2.897e-08 0.0001702 -1.00 ## vir_time_diff:same_day_dv:roi_dv-1 1.772e-07 0.0004209 -0.15 0.15 ## Residual 1.332e-04 0.0115397 ## Number of obs: 10640, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -0.0001934 0.0002186 -0.885 ## vir_time_diff 0.0002384 0.0001997 1.194 ## same_day_dv -0.0001327 0.0001974 -0.672 ## roi_dv-1 0.0004546 0.0002792 1.628 ## vir_time_diff:same_day_dv 0.0005129 0.0002022 2.536 ## vir_time_diff:roi_dv-1 -0.0008100 0.0002824 -2.868 ## same_day_dv:roi_dv-1 0.0002612 0.0002792 0.936 ## vir_time_diff:same_day_dv:roi_dv-1 -0.0007449 0.0002938 -2.535 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, optimizer = &#39;bobyqa&#39;) # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # compare to reduced model without interaction term set.seed(351) # set seed for reproducibility lmm_reduced &lt;-update(lmm_full, ~ . - vir_time_diff:same_day_dv:roi_dv) ## boundary (singular) fit: see ?isSingular lmm_aov &lt;- anova(lmm_reduced, lmm_full) lmm_aov nparAICBIClogLikdevianceChisqDfPr(&gt;Chisq) 14-6.47e+04-6.46e+043.24e+04-6.47e+04&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 15-6.47e+04-6.46e+043.24e+04-6.47e+046.3110.012 Mixed Model: Fixed effect of interaction virtual time with sequence identity and ROI on pattern similarity change \\(\\chi^2\\)(1)=6.31, p=0.012 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;, &quot;day&quot;, &quot;ROI&quot;, &quot;virtual time * day&quot;, &quot;virtual time * ROI&quot;, &quot;day * ROI&quot;, &quot;virtual time * day * ROI&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,6), &quot;residual&quot;) re_names &lt;- c(&quot;intercept (SD)&quot;, &quot;corr. intercept, virtual time:day:ROI1&quot;, &quot;corr. intercept, virtual time:day:ROI-1&quot;, &quot;virtual time:day:ROI1 (SD)&quot;, &quot;corr. virtual time:day:ROI1, virtual time:day:ROI-1&quot;, &quot;virtual time:day:ROI-1 (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: The effect of virtual time differentially depends on sequence membership in the anterior hippocampus and the anterior-lateral entorhinal cortex&quot;) # convert the huxtable to a flextable for word export stable_lme_virtime_aHPC_vs_alEC &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: The effect of virtual time differentially depends on sequence membership in the anterior hippocampus and the anterior-lateral entorhinal cortex fixed effects termestimateSEt-value95% CI intercept-0.0001930.000219-0.89-0.0006220.000235 virtual time0.0002380.0002001.19-0.0001530.000630 day-0.0001330.000197-0.67-0.0005200.000254 ROI0.0004550.0002791.63-0.0000930.001002 virtual time * day0.0005130.0002022.540.0001170.000909 virtual time * ROI-0.0008100.000282-2.87-0.001363-0.000257 day * ROI0.0002610.0002790.94-0.0002860.000808 virtual time * day * ROI-0.0007450.000294-2.54-0.001321-0.000169 random effects grouptermestimate participantintercept (SD)0.000496 participantcorr. intercept, virtual time:day:ROI1-1.000000 participantcorr. intercept, virtual time:day:ROI-1-0.151340 participantvirtual time:day:ROI1 (SD)0.000170 participantcorr. virtual time:day:ROI1, virtual time:day:ROI-10.151340 participantvirtual time:day:ROI-1 (SD)0.000421 residualSD0.011540 model comparison modelnparAICLLX2dfp reduced model14-64699.8732363.94 full model15-64704.1932367.096.3110.012 model: ps_change~vir_time_diff*same_day_dv*roi_dv+(1+vir_time_diff:same_day_dv:roi_dv|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation 8.11 Searchlight Results We further probed how temporal distances between events shaped representational change using searchlight analyses. Using the procedures described above, we calculated pattern similarity change values for search spheres with a radius of 3 voxels around the center voxel. Search spheres were centered on all brain voxels within our field of view. Within a given search sphere, only gray matter voxels were analyzed. Search spheres not containing more than 25 gray matter voxels were discarded. For each search sphere, we implemented linear models to quantify the relationship between representational change and the learned temporal structure. Specifically, we assessed the relationship of pattern similarity change and absolute virtual temporal distances, separately for event pairs from the same sequences and from pairs from different sequences. In a third model, we included all event pairs and tested for an interaction effect of sequence membership (same or different) predictor and virtual temporal distances. The t-values of the respective regressors of interest were stored at the center voxel of a given search sphere. The resulting t-maps were registered to MNI space for group level statistics and spatially smoothed (FWHM 3mm). Group level statistics were carried out using random sign flipping implemented with FSL Randomise and threshold-free cluster enhancement. We corrected for multiple comparisons using a small volume correction mask including our a priori regions of interest, the anterior hippocampus and the anterior-lateral entorhinal cortex. Further, we used a liberal threshold of puncorrected&lt;0.001 to explore the data for additional effects within our field of view. Exploratory searchlight results are shown in Supplemental Figure 9 and clusters with a minimum extent of 30 voxels are listed in Supplemental Tables 12, 14 and 15. To complement our ROI-analyses we also ran searchlights analyses. Here, we used search spheres with a 3 voxel radius around the center voxel. Each brain voxel served as a sphere center, but only gray matter voxels were analyzed as features. We analyzed the relationship of virtual temporal distances to pattern similarity changes for events from the same day only, from different days only, and for the interaction of virtual time with the same/different day factor. First, we load the MNI template brain (1mm resolution) and gray out the voxels outside the field of view. radius=3 # 1 mm MNI template as background image mni_fn &lt;- file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;, &quot;MNI152_T1_1mm_brain.nii.gz&quot;) mni_nii &lt;- readNIfTI2(mni_fn) # load FOV mask and binarize it fov_fn &lt;- file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;, &quot;fov_mask_mni.nii.gz&quot;) fov_nii &lt;- readNIfTI2(fov_fn) fov_nii[fov_nii &gt;0] &lt;- 1 fov_nii[fov_nii !=1 ] &lt;- 0 # make a mask for the brain area outside our FOV out_fov &lt;- (fov_nii == 0) &amp; (mni_nii&gt;0) mni_nii[out_fov] &lt;- scales::rescale(mni_nii[out_fov], from=range(mni_nii[out_fov]), to=c(6000, 8207)) mni_nii[mni_nii == 0] &lt;- NA Virtual Time Within Sequence Report significant clusters after small volume correction. # read the cluster file for corrected results fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;same_day_vir_time&quot;, &quot;cluster_same_day_vir_time_svc_corrp_atlas.txt&quot;) corrp_df &lt;- readr::read_tsv(fn, col_types = c(&quot;ccdddddddddd&quot;)) # add column with p-values (not 1-p as given by FSL) and manual labels corrp_df &lt;- corrp_df %&gt;% mutate( p = 1-max_1minusp, atlas_label = c(&quot;left hippocampus&quot;, &quot;right hippocampus&quot;, &quot;left hippocampus&quot;)) %&gt;% select(-c(max_1minusp, &quot;Harvard_Oxford_Cortical&quot;, &quot;Harvard_Oxford_Subcortical&quot;, cluster_index)) %&gt;% relocate(atlas_label) # quick print #corrp_df %&gt;% huxtable::as_huxtable() %&gt;% theme_article() # print to screen cat(&quot;significant clusters in same day searchlight:&quot;, sprintf(&quot;\\n%s, peak voxel at x=%d, y=%d, z=%d; t=%.2f, corrected p=%.3f&quot;, corrp_df$atlas_label, corrp_df$x, corrp_df$y, corrp_df$z, corrp_df$t_extr, corrp_df$p)) ## significant clusters in same day searchlight: ## left hippocampus, peak voxel at x=-24, y=-13, z=-20; t=4.53, corrected p=0.006 ## right hippocampus, peak voxel at x=31, y=-16, z=-20; t=3.56, corrected p=0.035 ## left hippocampus, peak voxel at x=-27, y=-20, z=-15; t=3.47, corrected p=0.029 Report additional clusters from exploratory analysis with threshold p&lt;0.001 uncorrected and a minimum of 30 voxels per cluster. # read the cluster file for uncorrected results fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;same_day_vir_time&quot;, &quot;cluster_same_day_vir_time_fov_uncorrp_atlas.txt&quot;) uncorrp_df &lt;- readr::read_tsv(fn, col_types = c(&quot;ccdddddddddd&quot;)) # add column with p-values (not 1-p as given by FSL) and manual labels uncorrp_df &lt;- uncorrp_df %&gt;% mutate( p = 1-max_1minusp, atlas_label = c( &quot;frontal pole&quot;, &quot;frontal pole&quot;, &quot;left hippocampus&quot;, &quot;left entorhinal cortex&quot;, &quot;inferior frontal gyrus&quot;, &quot;lingual gyrus&quot;, &quot;frontal medial cortex&quot;, &quot;left hippocampus&quot;)) %&gt;% select(-c(max_1minusp, &quot;Harvard_Oxford_Cortical&quot;, &quot;Harvard_Oxford_Subcortical&quot;, cluster_index)) %&gt;% filter(!str_detect(atlas_label, &quot;hippocampus&quot;)) %&gt;% # remove hippocampal clusters (already reported with SVC) relocate(atlas_label) # add p-values because small values are reported as 0 by FSL cluster uncorrp_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_fov_fwhm3_tfce_p_tstat1.nii.gz&quot;) uncorrp_nii &lt;- readNIfTI2(uncorrp_fn) for (i in 1:nrow(uncorrp_df)){ coords &lt;- mni2vox(c(uncorrp_df[i,]$x,uncorrp_df[i,]$y,uncorrp_df[i,]$z)) uncorrp_df[i,]$p &lt;- 1-uncorrp_nii[coords[1], coords[2], coords[3]] } # quick print #uncorrp_df %&gt;% huxtable::as_huxtable() %&gt;% theme_article() Create supplemental table. corrp_ht &lt;- corrp_df %&gt;% as_huxtable() %&gt;% huxtable::set_contents(., row=1, value =c(&quot;Atlas Label&quot;, &quot;Voxel Extent&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;COG x&quot;, &quot;COG y&quot;, &quot;COG z&quot;, &quot;t&quot;, &quot;p&quot;)) %&gt;% huxtable::insert_row(., rep(&quot;&quot;, ncol(.)), after=0) %&gt;% huxtable::merge_across(.,1,1:ncol(.)) %&gt;% huxtable::set_contents(., row=1, value =&quot;Searchlight results in a priori regions of interest, p-values corrected using small volume correction&quot;) %&gt;% huxtable::set_header_rows(.,row=1, value=TRUE) %&gt;% theme_article() corrp_ht Searchlight results in a priori regions of interest, p-values corrected using small volume correction Atlas LabelVoxel ExtentxyzCOG xCOG yCOG ztp left hippocampus193-24-13-20-23.3-13.1-19.84.530.006 right hippocampus9631-16-2030.1-16.7-19.83.560.035 left hippocampus76-27-20-15-27.9-19.5-16.63.470.029 uncorrp_ht &lt;- uncorrp_df %&gt;% as_huxtable() %&gt;% huxtable::set_contents(., row=1, value =c(&quot;Atlas Label&quot;, &quot;Voxel Extent&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;COG x&quot;, &quot;COG y&quot;, &quot;COG z&quot;, &quot;t&quot;, &quot;p&quot;)) %&gt;% huxtable::insert_row(., rep(&quot;&quot;, ncol(.)), after=0) %&gt;% huxtable::merge_across(.,1,1:ncol(.)) %&gt;% huxtable::set_contents(., row=1, value =&quot;Exploratory searchlight results, p-values uncorrected&quot;) %&gt;% huxtable::set_header_rows(.,row=1, value=TRUE) %&gt;% theme_article() uncorrp_ht Exploratory searchlight results, p-values uncorrected Atlas LabelVoxel ExtentxyzCOG xCOG yCOG ztp frontal pole39950441648.3&nbsp;41.619.2&nbsp;3.960.0002 frontal pole1735341-751.1&nbsp;42.9-4.454.560.0002 left entorhinal cortex119-18-16-32-21.2&nbsp;-14.6-31.2&nbsp;3.450.0004 inferior frontal gyrus914027244.2&nbsp;28&nbsp;&nbsp;3.594.290.0002 lingual gyrus86-17-58-15-15.7&nbsp;-56.9-9.643.820.0002 frontal medial cortex49735-236.2936.7-24.1&nbsp;4.280.0004 # merge and convert to flexable that can be written to Word stable_srchlght_same_seq &lt;- rbind(corrp_ht, uncorrp_ht) %&gt;% huxtable::add_footnote(&quot;x, y, z refer to MNI coordinates of minimum p-value in cluster, t denotes the most extreme t-value, COG: center of gravity&quot;, border = 0.5) %&gt;% huxtable::as_flextable() %&gt;% flextable::fontsize(size=10, part=&quot;all&quot;) %&gt;% flextable::font(fontname = font2use) %&gt;% flextable::style(i=c(1,nrow(corrp_ht)+1), pr_c = officer::fp_cell(border.bottom = fp_border(), border.top = fp_border())) %&gt;% flextable::bg(i=c(1,nrow(corrp_ht)+1), bg=&quot;lightgrey&quot;, part=&quot;all&quot;) %&gt;% flextable::padding(., padding = 3, part=&quot;all&quot;) %&gt;% set_table_properties(., width=1, layout = &quot;autofit&quot;) %&gt;% flextable::set_caption(., caption = &quot;Searchlight Analysis: Virtual time explains representational change for same-sequence events&quot;, style=&quot;Normal&quot;) Here, we plot the results of the searchlight looking at the effect of virtual time on pattern similarity change for events from the same sequence. We show the t-values thresholded at p &lt; 0.01 uncorrected. To also show which voxels survive corrections for multiple comparisons (within aHPC and alEC), we also load the outline of the clusters surviving at corrected p&lt;0.05. In the resulting plot, voxels inside the black area are significant when correcting for multiple comparisons. # choose coordinates (these are chosen manually now) and get a data frame for labeling the plots coords &lt;- mni2vox(c(-24,-15,-19)) label_df &lt;- data.frame(col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;, &quot;z&quot;),vox2mni(coords)), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;, &quot;z&quot;),vox2mni(coords))) # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,2,3), mar_ind=coords, row_ind = c(1,1,1), col_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), center_coords=TRUE) # get uncorrected p-values in FOV pfov_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_fov_fwhm3_tfce_p_tstat1.nii.gz&quot;) pfov_nii &lt;- readNIfTI2(pfov_fn) # t-values to plot (FOV) tvals_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_fov_fwhm3_tstat1.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) # threshold t-values for plotting thresh &lt;- 0.01 t_nii[pfov_nii &lt; 1-thresh] &lt;- NA # get data frame for the t-values t_df &lt;- getBrainFrame(brains=t_nii, mar=c(1,2,3), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), mar_ind=coords, mask=NULL, center_coords=TRUE) # load the outline image outl_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_svc_fwhm3_tfce_corrp_outline.nii.gz&quot;) outl_nii &lt;- readNIfTI2(outl_fn) # remove zero voxels and get dataframe at coordinates outl_nii[outl_nii == 0] &lt;- NA o_df &lt;- getBrainFrame(brains=outl_nii, mar=c(1,2,3), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), mar_ind=coords, mask=NULL, center_coords=TRUE) f7a &lt;- ggTemplate + geom_tile(data=t_df, aes(x=row,y=col,fill=value))+ geom_tile(data=o_df, aes(x=row,y=col), fill=&quot;black&quot;)+ facet_wrap(~col_ind, nrow=3, ncol=1, strip.position = &quot;top&quot;, scales = &quot;free&quot;) + scico::scale_fill_scico(palette = &quot;devon&quot;, begin = 0.1, end=0.7, name = element_text(&quot;t&quot;), limits = round(range(t_df$value), digits=2), breaks = round(range(t_df$value), digits=2)) + ggtitle(label = &quot;\\nsame\\nsequence&quot;) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), plot.title = element_text(hjust = 0.5), text = element_text(size=10), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title = element_text(size=8), legend.key.width = unit(0.01, &quot;npc&quot;), legend.key.height = unit(0.015, &quot;npc&quot;), legend.position = &quot;bottom&quot;, legend.justification = c(0.5, 0), aspect.ratio = 1)+ guides(fill = guide_colorbar( direction = &quot;horizontal&quot;, title.position = &quot;top&quot;, title.hjust = 0.5, label.position = &quot;bottom&quot;))+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=0, y=c(-84.7,-70.37,-90.69), aes(label = label), size = 8/.pt, family=font2use, vjust=1) + # y coords found via command below (to keep text aligned across panels): # ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range[1]-diff(ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range)/100*3 theme_no_ticks() print(f7a) # save source data file.copy(from = tvals_fn, to = file.path(dirs$source_dat_dir, &quot;f7a.nii.gz&quot;)) ## [1] TRUE Virtual Time Across Sequences in Within-Searchlight Cluster The plot above shows a significant effect of virtual time on pattern similarity change in some voxels of the hippocampus. To ask whether these voxels also show the across-sequence effect, we run an ROI-analyis based on the significant voxels from the within-searchlight (peak cluster from left hemisphere, defined at p&lt;0.99 uncorrected within small volume correction mask). These data were calculated in the script virtem_mri_prepare_RSA_in_searchlight_cluster and are loaded here. To test whether within- and across-sequence representations overlap, we defined an ROI based on the within-sequence searchlight analysis. Specifically, voxels belonging to the cluster around the peak voxel, thresholded at p&lt;0.01 uncorrected within our small volume correction mask, were included. The analysis of representational change was then carried out as described for the other ROIs above. Load data # load the data col_types_list &lt;- cols_only( sub_id = col_character(), day1 = col_integer(), day2 = col_integer(), virtual_time1 = col_double(), virtual_time2 = col_double(), ps_change = col_double(), roi = col_factor()) fn &lt;- file.path(dirs$data4analysis, &quot;rsa_data_in_same-seq_searchlight_peak.txt&quot;) rsa_dat_within_cluster &lt;- as_tibble(read_csv(fn, col_types = col_types_list)) Prepare Time Metrics for RSA Create day factor and create virtual temporal distance variables, analogous to main ROI analyses above. # create predictors for RSA rsa_dat_within_cluster &lt;- rsa_dat_within_cluster %&gt;% mutate( # pair of events from say or different day (dv = deviation code) same_day = day1 == day2, same_day_dv = plyr::mapvalues(same_day, from = c(FALSE, TRUE), to = c(-1, 1)), # absolute difference in time metrics vir_time_diff = abs(virtual_time1 - virtual_time2)) %&gt;% # z-score the time metric predictors (within each subject) group_by(sub_id) %&gt;% mutate_at(c(&quot;vir_time_diff&quot;), scale, scale = TRUE) %&gt;% ungroup() Summary Statistics First-level RSA Let’s run the first-level analysis using virtual time as a predictor of pattern similarity change separately for each participant in this ROI. set.seed(107) # set seed for reproducibility # do RSA using linear model and calculate z-score for model fit based on permutations rsa_fit_within_cluster &lt;- rsa_dat_within_cluster %&gt;% group_by(sub_id, same_day) %&gt;% # run the linear model do(z = lm_perm_jb(in_dat = ., lm_formula = &quot;ps_change ~ vir_time_diff&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;)))) %&gt;% unnest_wider(z) # add group column used for plotting rsa_fit_within_cluster$group &lt;- factor(1) # add a factor with character labels for within/across days and one to later color control in facets rsa_fit_within_cluster &lt;- rsa_fit_within_cluster %&gt;% mutate(same_day_char = plyr::mapvalues(same_day, from = c(0, 1), to = c(&quot;across days&quot;, &quot;within days&quot;), warn_missing = FALSE), same_day_char = factor(same_day_char, levels = c(&quot;within days&quot;, &quot;across days&quot;))) Group Level # run a group-level t-test on the different sequence RSA fits in the within-searchlight cluster stats &lt;- rsa_fit_within_cluster %&gt;% filter(same_day == FALSE) %&gt;% select(z_virtual_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit_within_cluster %&gt;% filter(same_day == FALSE))$z_virtual_time, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high -0.424-2.190.0370.035727-0.821-0.0275One Sample t-testtwo.sided-0.403-0.812-0.0251 Summary Statistics: t-test against 0 for virtual time across sequences in within-sequence searchlight peak t27=-2.19, p=0.036, d=-0.40, 95% CI [-0.81, -0.03] Lastly, we create a raincloud plot of the z-values # select the data to plot plot_dat &lt;- rsa_fit_within_cluster %&gt;% filter(same_day == FALSE) f7b &lt;- ggplot(plot_dat, aes(x=1, y=z_virtual_time, fill = group, color = group)) + gghalves::geom_half_violin(position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + geom_point(aes(x = 0.8, y = z_virtual_time), alpha = 0.7, position = position_jitter(width = .1, height = 0), shape=16, size = 1) + geom_boxplot(position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(.1), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(.1), colour = &quot;black&quot;, width = 0, size = 0.5)+ scale_color_manual(values=unname(aHPC_colors[&quot;across_main&quot;])) + scale_fill_manual(values=unname(aHPC_colors[&quot;across_main&quot;])) + ylab(&#39;z RSA model fit&#39;) + xlab(element_blank()) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + ggtitle(&quot;\\ndifferent sequence in\\nsame sequence peak&quot;) + theme_cowplot() + theme(text = element_text(size=10), axis.text = element_text(size=8), legend.position = &quot;none&quot;, axis.ticks.x = element_blank(), axis.text.x = element_blank()) f7b # save source data source_dat &lt;-ggplot_build(f7b)$data[[2]] readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f7b.txt&quot;)) Linear Mixed Effects As for the main analyses, we also want to run the same model using linear mixed effects. Due to singular fits we have to reduce the random effects structure until only random slopes are in the model. # extract the comparisons from the same day rsa_dat_diff_day &lt;- rsa_dat_within_cluster %&gt;% filter(same_day == FALSE) # define the full model with virtual time difference as # fixed effect and by-subject random slopes and random intercepts --&gt; singular fit formula &lt;- &quot;ps_change ~ vir_time_diff + (1 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # remove correlation --&gt; still singular formula &lt;- &quot;ps_change ~ vir_time_diff + (1 + vir_time_diff || sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular # only random slopes set.seed(322) # set seed for reproducibility formula &lt;- &quot;ps_change ~ vir_time_diff + (0 + vir_time_diff | sub_id)&quot; lmm_full &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) ## boundary (singular) fit: see ?isSingular summary(lmm_full, corr = FALSE) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: ps_change ~ vir_time_diff + (0 + vir_time_diff | sub_id) ## Data: rsa_dat_diff_day ## ## AIC BIC logLik deviance df.resid ## -23260.0 -23234.6 11634.0 -23268.0 4196 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.0842 -0.6392 0.0084 0.6616 3.7002 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub_id vir_time_diff 0.0000000 0.00000 ## Residual 0.0002299 0.01516 ## Number of obs: 4200, groups: sub_id, 28 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -9.718e-05 2.343e-04 -0.415 ## vir_time_diff -4.782e-04 2.341e-04 -2.043 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular # tidy summary of the fixed effects that calculates 95% CIs lmm_full_bm &lt;- broom.mixed::tidy(lmm_full, effects = &quot;fixed&quot;, conf.int=TRUE, conf.method=&quot;profile&quot;, optimizer = &#39;nloptwrap&#39;) ## Computing profile confidence intervals ... # tidy summary of the random effects lmm_full_bm_re &lt;- broom.mixed::tidy(lmm_full, effects = &quot;ran_pars&quot;) # one way of testing for significance is by comparing the likelihood against a simpler model. # Here, we drop the effect of virtual time difference and run an ANOVA. See e.g. Bodo Winter tutorial set.seed(396) # set seed for reproducibility formula &lt;- &quot;ps_change ~ 1 + (0 + vir_time_diff | sub_id)&quot; lmm_no_vir_time &lt;- lme4::lmer(formula, data = rsa_dat_diff_day, REML = FALSE) lmm_aov &lt;-anova(lmm_no_vir_time, lmm_full) Mixed Model: Fixed effect of virtual time across sequences in within-sequence searchlight peak \\(\\chi^2\\)(1)=4.13, p=0.042 Make mixed model summary table that includes overview of fixed and random effects as well as the model comparison to the nested (reduced) model. fe_names &lt;- c(&quot;intercept&quot;, &quot;virtual time&quot;) re_groups &lt;- c(rep(&quot;participant&quot;,1), &quot;residual&quot;) re_names &lt;- c(&quot;virtual time (SD)&quot;, &quot;SD&quot;) lmm_hux &lt;- make_lme_huxtable(fix_df=lmm_full_bm, ran_df = lmm_full_bm_re, aov_mdl = lmm_aov, fe_terms =fe_names, re_terms = re_names, re_groups = re_groups, lme_form = gsub(&quot; &quot;, &quot;&quot;, paste0(deparse(formula(lmm_full)), collapse = &quot;&quot;, sep=&quot;&quot;)), caption = &quot;Mixed Model: Virtual time explains representational change for different-sequence events in the peak cluster of the same-sequence searchlight analysis&quot;) # convert the huxtable to a flextable for word export stable_lme_same_seq_cluster_virtime_diff_seq &lt;- convert_huxtable_to_flextable(ht = lmm_hux) # print to screen theme_article(lmm_hux) Mixed Model: Virtual time explains representational change for different-sequence events in the peak cluster of the same-sequence searchlight analysis fixed effects termestimateSEt-value95% CI intercept-0.0000970.000234-0.41-0.0005570.000362 virtual time-0.0004780.000234-2.04-0.000939-0.000018 random effects grouptermestimate participantvirtual time (SD)0.000000 residualSD0.015162 model comparison modelnparAICLLX2dfp reduced model3-23257.8711631.93 full model4-23260.0011634.004.1310.042 model: ps_change~vir_time_diff+(0+vir_time_diff|sub_id); SE: standard error, CI: confidence interval, SD: standard deviation, npar: number of parameters, LL: log likelihood, df: degrees of freedom, corr.: correlation ### PLOTS lmm_full_bm &lt;- lmm_full_bm %&gt;% mutate(term=as.factor(term)) # dot plot of Fixed Effect Coefficients with CIs sfigmm_m &lt;-ggplot(data = lmm_full_bm[2,], aes(x = term, color = term)) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dotted&quot;) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = NA), size = 0.5) + geom_point(aes(y = estimate), shape = 16, size = 1) + scale_fill_manual(values = unname(aHPC_colors[&quot;across_main&quot;])) + scale_color_manual(values = unname(aHPC_colors[&quot;across_main&quot;]), labels = c(&quot;virtual time (diff. seq.)&quot;), name = element_blank()) + labs(x = element_blank(), y = &quot;fixed effect\\nestimate&quot;) + guides(fill= &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2, linetype=0))) + annotate(geom = &quot;text&quot;, x = 1, y = Inf, label = &quot;*&quot;, hjust = 0.5, vjust = 1, size = 8/.pt, family=font2use) + theme_cowplot() + theme(legend.position = &quot;none&quot;, axis.text.x=element_blank()) sfigmm_m # estimate marginal means for each model term by omitting the terms argument lmm_full_emm &lt;- ggeffects::ggpredict(lmm_full, ci.lvl = 0.95) %&gt;% ggeffects::get_complete_df() # plot marginal means sfigmm_n &lt;- ggplot(data = lmm_full_emm[lmm_full_emm$group == &quot;vir_time_diff&quot;,], aes(color = group)) + geom_line(aes(x, predicted)) + geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), alpha = .2, linetype=0) + scale_color_manual(values = unname(aHPC_colors[&quot;across_main&quot;]), name = element_blank(), labels = &quot;virtual time&quot;) + scale_fill_manual(values = unname(aHPC_colors[&quot;across_main&quot;])) + ylab(&#39;estimated\\nmarginal means&#39;) + xlab(&#39;z virtual time&#39;) + #ggtitle(&quot;Estimated marginal means&quot;) + guides(fill = &quot;none&quot;, color = &quot;none&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;) sfigmm_n Virtual Time Across Sequences Next, we plot the results of the searchlight looking at the across-sequence comparisons. This effect fails to reach the significance threshold when correcting for multiple comparisons. For visualization, we show the effect at p&lt;0.05 uncorrected (within FOV). We report strongest cluster after small volume correction. # read the cluster file for corrected results fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;diff_day_vir_time&quot;, &quot;cluster_diff_day_vir_time_neg_svc_corrp_atlas.txt&quot;) corrp_df &lt;- readr::read_tsv(fn, col_types = c(&quot;ccdddddddddd&quot;)) # print to screen cat(&quot;no significant clusters in across-day virtual time (negative) searchlight:&quot;, sprintf(&quot;\\ncluster file has %d rows&quot;, nrow(corrp_df))) ## no significant clusters in across-day virtual time (negative) searchlight: ## cluster file has 0 rows # find and report peak voxel coordinates incl. p and t after SVC corrp_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_neg_randomise_svc_fwhm3_tfce_corrp_tstat1.nii.gz&quot;) corrp_nii &lt;- readNIfTI2(corrp_fn) coords &lt;- which(corrp_nii == max(c(corrp_nii)), arr.ind = TRUE) mnicoords &lt;- vox2mni(coords) tvals_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_neg_randomise_fov_fwhm3_tstat1.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) cat(&quot;peak voxel in across-day virtual time (negative) searchlight:&quot;, sprintf(&quot;\\nx=%d, y=%d, z=%d, t=%.2f, corrected p =%.3f&quot;, mnicoords[1], mnicoords[2], mnicoords[3], -t_nii[coords[1], coords[2], coords[3]], 1-max(c(corrp_nii)))) ## peak voxel in across-day virtual time (negative) searchlight: ## x=-26, y=-19, z=-15, t=-3.96, corrected p =0.071 Report additional clusters from exploratory analysis with threshold p&lt;0.001 uncorrected and a minimum of 30 voxels per cluster. # read the cluster file for uncorrected results fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;diff_day_vir_time&quot;, &quot;cluster_diff_day_vir_time_neg_fov_uncorrp_atlas.txt&quot;) uncorrp_df &lt;- readr::read_tsv(fn, col_types = c(&quot;ccdddddddddd&quot;)) # quick print #uncorrp_df %&gt;% huxtable::as_huxtable() %&gt;% theme_article() # add column with p-values (not 1-p as given by FSL) and manual labels uncorrp_df &lt;- uncorrp_df %&gt;% mutate( t_extr = t_extr*-1, # multiply t-values by minus 1 because we tested the negative contrast p = 1-max_1minusp, atlas_label = c( &quot;cerebellum&quot;, &quot;cerebellum&quot;, &quot;lingual gyrus&quot; )) %&gt;% select(-c(max_1minusp, &quot;Harvard_Oxford_Cortical&quot;, &quot;Harvard_Oxford_Subcortical&quot;, cluster_index)) %&gt;% relocate(atlas_label) # add p-values because small values are reported as 0 by FSL cluster uncorrp_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_neg_randomise_fov_fwhm3_tfce_p_tstat1.nii.gz&quot;) uncorrp_nii &lt;- readNIfTI2(uncorrp_fn) for (i in 1:nrow(uncorrp_df)){ coords &lt;- mni2vox(c(uncorrp_df[i,]$x,uncorrp_df[i,]$y,uncorrp_df[i,]$z)) uncorrp_df[i,]$p &lt;- 1-uncorrp_nii[coords[1], coords[2], coords[3]] } Create pretty table that can be written to Word. uncorrp_ht &lt;- uncorrp_df %&gt;% as_huxtable() %&gt;% huxtable::set_contents(., row=1, value =c(&quot;Atlas Label&quot;, &quot;Voxel Extent&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;COG x&quot;, &quot;COG y&quot;, &quot;COG z&quot;, &quot;t&quot;, &quot;p&quot;)) %&gt;% huxtable::insert_row(., rep(&quot;&quot;, ncol(.)), after=0) %&gt;% huxtable::merge_across(.,1,1:ncol(.)) %&gt;% huxtable::set_contents(., row=1, value =&quot;Exploratory searchlight results, p-values uncorrected&quot;) %&gt;% huxtable::set_header_rows(.,row=1, value=TRUE) %&gt;% theme_article() uncorrp_ht Exploratory searchlight results, p-values uncorrected Atlas LabelVoxel ExtentxyzCOG xCOG yCOG ztp cerebellum31419-68-3419.1&nbsp;-66.3-29.6&nbsp;-5.370.0002 cerebellum104-1-68-14-1.86-69.1-14.3&nbsp;-3.440.0002 lingual gyrus100-1-704-2.68-70.54.56-3.730.0002 stable_srchlght_diff_seq &lt;- uncorrp_ht %&gt;% huxtable::add_footnote(&quot;x, y, z refer to MNI coordinates of minimum p-value in cluster, t denotes the most extreme t-value, COG: center of gravity&quot;, border = 0.5) %&gt;% huxtable::as_flextable() %&gt;% flextable::fontsize(size=10, part=&quot;all&quot;) %&gt;% flextable::font(fontname = font2use) %&gt;% flextable::style(i=c(1), pr_c = officer::fp_cell(border.bottom = fp_border(), border.top = fp_border())) %&gt;% flextable::bg(i=1, bg=&quot;lightgrey&quot;, part=&quot;all&quot;) %&gt;% flextable::padding(., padding = 3, part=&quot;all&quot;) %&gt;% set_table_properties(., width=1, layout = &quot;autofit&quot;) %&gt;% flextable::set_caption(., caption = &quot;Searchlight Analysis: Virtual time explains representational change for different-sequence events&quot;, style=&quot;Normal&quot;) # choose coordinates (these are chosen manually now) and get a data frame for labeling the plots coords &lt;- mni2vox(c(-25,-19,-15)) label_df &lt;- data.frame(col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;), vox2mni(coords)), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;), vox2mni(coords))) # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,2,3), mar_ind=coords, row_ind = c(1,1,1), col_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), center_coords=TRUE) # get uncorrected p-values in FOV pfov_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_neg_randomise_fov_fwhm3_tfce_p_tstat1.nii.gz&quot;) pfov_nii &lt;- readNIfTI2(pfov_fn) # t-values to plot (FOV) tvals_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_neg_randomise_fov_fwhm3_tstat1.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) # CAVE: because we are looking at the negative contrast, multiply the t-values by -1 to actually show # negative t-values (they have previously been multiplied by -1 to run randomise, now revert this) t_nii &lt;- t_nii * -1 # threshold t-values for plotting thresh &lt;- 0.05 t_nii[pfov_nii &lt; 1-thresh] &lt;- NA # get data frame for the t-values t_df &lt;- getBrainFrame(brains=t_nii, mar=c(1,2,3), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), mar_ind=coords, mask=NULL, center_coords=TRUE) f7c &lt;- ggTemplate + geom_tile(data=t_df, aes(x=row,y=col,fill=value))+ facet_wrap(~col_ind, nrow=3, ncol=1, strip.position = &quot;top&quot;, scales = &quot;free&quot;) + scico::scale_fill_scico(palette = &quot;lajolla&quot;, begin = 0.3, end=0.9, direction = -1, name = element_text(&quot;t&quot;), limits = round(range(t_df$value), digits=2), breaks = round(range(t_df$value), digits=2)) + ggtitle(label = &quot;\\ndifferent\\nsequence&quot;) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), plot.title = element_text(hjust = 0.5), text = element_text(size=10), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title = element_text(size=8), legend.key.width = unit(0.01, &quot;npc&quot;), legend.key.height = unit(0.015, &quot;npc&quot;), legend.position = &quot;bottom&quot;, legend.justification = c(0.5, 0), aspect.ratio = 1)+ guides(fill = guide_colorbar( direction = &quot;horizontal&quot;, title.position = &quot;top&quot;, title.hjust = 0.5, label.position = &quot;bottom&quot;))+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=0, y=c(-84.7,-70.37,-90.69), aes(label = label), size = 8/.pt, family=font2use, vjust=1) + # y coords found via command below (to keep text aligned across panels): # ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range[1]-diff(ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range)/100*3 theme_no_ticks() print(f7c) file.copy(from = tvals_fn, to = file.path(dirs$source_dat_dir, &quot;f7c.nii.gz&quot;)) ## [1] TRUE Visualization of Overlap To visualize the overlap between the within- and the between-sequence effects we next create an overlap plot on a zoomed-in plot of the hippocampal region. This is for visual purposes only, so we use a liberal threshold of p&lt;0.05 within our small-volume-correction mask. # choose coordinates (these are chosen manually now) and get a data frame for labeling the plots coords &lt;- mni2vox(c(-25,-18,-16)) label_df &lt;- data.frame(col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;, &quot;z&quot;),vox2mni(coords)), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;, &quot;z&quot;),vox2mni(coords))) # for this plot we want to &quot;zoom in&quot;, which we accomplish by masking out all but a small region of the brain box2remove&lt;-array(1, dim = dim(mni_nii)) box2remove[c(1:51, 131:182),,] &lt;- 0 box2remove[,c(1:68, 150:218),] &lt;- 0 box2remove[,,c(1:21, 101:182)] &lt;- 0 mni_zoom_nii &lt;- mni_nii * box2remove mni_zoom_nii[mni_zoom_nii==0] &lt;- NA # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_zoom_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,2,3), mar_ind=coords, row_ind = c(1,1,1), col_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), center_coords=TRUE) thresh &lt;- 0.05 # get uncorrected p-values in FOV fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_svc_fwhm3_tfce_p_tstat1.nii.gz&quot;) sameday_nii &lt;- readNIfTI2(fn) sameday_nii[sameday_nii &gt; 1-thresh] = 1 sameday_nii[sameday_nii &lt; 1-thresh] = 0 fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_neg_randomise_svc_fwhm3_tfce_p_tstat1.nii.gz&quot;) diffday_nii &lt;- readNIfTI2(fn) diffday_nii[diffday_nii &gt; 1-thresh] = 2 diffday_nii[diffday_nii &lt; 1-thresh] = 0 overlap_nii &lt;- sameday_nii + diffday_nii overlap_nii[overlap_nii &lt; 1] = NA # get data frame for the t-values o_df &lt;- getBrainFrame(brains=overlap_nii, mar=c(1,2,3), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), mar_ind=coords, mask=NULL, center_coords=TRUE) %&gt;% mutate(value = as.factor(value), value = recode_factor(value, `1`=&quot;same&quot;, `2`=&quot;different&quot;, `3`=&quot;overlap&quot;)) f7d &lt;- ggTemplate + geom_tile(data=o_df, aes(x=row,y=col,fill=value))+ facet_wrap(~col_ind, nrow=3, ncol=1, strip.position = &quot;bottom&quot;, scales = &quot;free&quot;) + scale_fill_manual(values = unname(c(aHPC_colors[&quot;within_main&quot;], aHPC_colors[&quot;across_main&quot;], day_time_int_color)), guide = guide_legend(override.aes=list( alpha = 1, shape=16, size=2, linetype=0), direction = &quot;vertical&quot;, title=element_blank(), label.position = &quot;right&quot;)) + ggtitle(label = &quot;\\noverlap\\n&quot;) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), plot.title = element_text(hjust = 0.5), text = element_text(size=10), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title = element_text(size=8), legend.key.size = unit(0.02, &quot;npc&quot;), legend.position = &quot;bottom&quot;, legend.justification = c(0.5, 0), aspect.ratio = 1)+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=0, y=c(-71.87,-67.75,-42.93), aes(label = label), size = 8/.pt, family=font2use, vjust=1) + # y coords found via command below (to keep text aligned across panels): # ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range[1]-diff(ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range)/100*3 theme_no_ticks() print(f7d) # save source data overlap_nii[is.na(overlap_nii)] &lt;- 0 # replace NAs with 0 to avoid trouble writeNIfTI2(nim = overlap_nii, filename = file.path(dirs$source_dat_dir, &quot;f7d&quot;), dtype = TRUE) Virtual Time &amp; Sequence Interaction In our searchlight analyses, we also directly looked for voxels where the effect of virtual time differs depending on whether we are looking at events from the same sequence or from different sequences, i.e. an interaction of virtual temporal distances and a sequence factor (same/different). Report significant clusters after small volume correction. # read the cluster file for corrected results fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;day_time_interaction&quot;, &quot;cluster_day_time_interaction_svc_corrp_atlas.txt&quot;) corrp_df &lt;- readr::read_tsv(fn, col_types = c(&quot;ccdddddddddd&quot;)) # quick print #corrp_df %&gt;% huxtable::as_huxtable() %&gt;% theme_article() # add column with p-values (not 1-p as given by FSL) and manual labels corrp_df &lt;- corrp_df %&gt;% mutate( p = 1-max_1minusp, atlas_label = c(&quot;left hippocampus&quot;, &quot;right hippocampus&quot;)) %&gt;% select(-c(max_1minusp, &quot;Harvard_Oxford_Cortical&quot;, &quot;Harvard_Oxford_Subcortical&quot;, cluster_index)) %&gt;% relocate(atlas_label) # print to screen cat(&quot;significant clusters in day-time interaction searchlight:&quot;, sprintf(&quot;\\n%s, peak voxel at x=%d, y=%d, z=%d; t=%.2f, corrected p=%.3f&quot;, corrp_df$atlas_label, corrp_df$x, corrp_df$y, corrp_df$z, corrp_df$t_extr, corrp_df$p)) ## significant clusters in day-time interaction searchlight: ## left hippocampus, peak voxel at x=-26, y=-20, z=-15; t=4.15, corrected p=0.014 ## right hippocampus, peak voxel at x=31, y=-16, z=-21; t=4.25, corrected p=0.007 Report additional clusters from exploratory analysis with threshold p&lt;0.001 uncorrected and a minimum of 30 voxels per cluster. # read the cluster file for uncorrected results fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;day_time_interaction&quot;, &quot;cluster_day_time_interaction_fov_uncorrp_atlas.txt&quot;) uncorrp_df &lt;- readr::read_tsv(fn, col_types = c(&quot;ccdddddddddd&quot;)) # quick print #uncorrp_df %&gt;% huxtable::as_huxtable() %&gt;% theme_article() # add column with p-values (not 1-p as given by FSL) and manual labels uncorrp_df &lt;- uncorrp_df %&gt;% mutate( p = 1-max_1minusp, atlas_label = c( &quot;left hippocampus&quot;, &quot;right hippocampus&quot;, &quot;occipital pole&quot;, &quot;lingual gyrus&quot;, &quot;frontal pole&quot;, &quot;frontal pole&quot;, &quot;temporal fusiform cortex&quot;, &quot;intracalcarine sulcus&quot;)) %&gt;% select(-c(max_1minusp, &quot;Harvard_Oxford_Cortical&quot;, &quot;Harvard_Oxford_Subcortical&quot;, cluster_index)) %&gt;% filter(!str_detect(atlas_label, &quot;hippocampus&quot;)) %&gt;% # remove hippocampal clusters (already reported with SVC) relocate(atlas_label) # add p-values because small values are reported as 0 by FSL cluster uncorrp_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_fov_fwhm3_tfce_p_tstat1.nii.gz&quot;) uncorrp_nii &lt;- readNIfTI2(uncorrp_fn) for (i in 1:nrow(uncorrp_df)){ coords &lt;- mni2vox(c(uncorrp_df[i,]$x,uncorrp_df[i,]$y,uncorrp_df[i,]$z)) uncorrp_df[i,]$p &lt;- 1-uncorrp_nii[coords[1], coords[2], coords[3]] } Make pretty overview table that can be saved to Word. corrp_ht &lt;- corrp_df %&gt;% as_huxtable() %&gt;% huxtable::set_contents(., row=1, value =c(&quot;Atlas Label&quot;, &quot;Voxel Extent&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;COG x&quot;, &quot;COG y&quot;, &quot;COG z&quot;, &quot;t&quot;, &quot;p&quot;)) %&gt;% huxtable::insert_row(., rep(&quot;&quot;, ncol(.)), after=0) %&gt;% huxtable::merge_across(.,1,1:ncol(.)) %&gt;% huxtable::set_contents(., row=1, value =&quot;Searchlight results in a priori regions of interest, p-values corrected using small volume correction&quot;) %&gt;% huxtable::set_header_rows(.,row=1, value=TRUE) %&gt;% theme_article() corrp_ht Searchlight results in a priori regions of interest, p-values corrected using small volume correction Atlas LabelVoxel ExtentxyzCOG xCOG yCOG ztp left hippocampus359-26-20-15-23.4-15.5-18.64.150.014 right hippocampus33531-16-2130.7-15.1-20.14.250.007 uncorrp_ht &lt;- uncorrp_df %&gt;% as_huxtable() %&gt;% huxtable::set_contents(., row=1, value =c(&quot;Atlas Label&quot;, &quot;Voxel Extent&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;COG x&quot;, &quot;COG y&quot;, &quot;COG z&quot;, &quot;t&quot;, &quot;p&quot;)) %&gt;% huxtable::insert_row(., rep(&quot;&quot;, ncol(.)), after=0) %&gt;% huxtable::merge_across(.,1,1:ncol(.)) %&gt;% huxtable::set_contents(., row=1, value =&quot;Exploratory searchlight results, p-values uncorrected&quot;) %&gt;% huxtable::set_header_rows(.,row=1, value=TRUE) %&gt;% theme_article() uncorrp_ht Exploratory searchlight results, p-values uncorrected Atlas LabelVoxel ExtentxyzCOG xCOG yCOG ztp occipital pole10317-91-817.7&nbsp;-90.6-6.624.080.0002 lingual gyrus102-5-735-3.59-70.45.013.720.0002 frontal pole9643431845.4&nbsp;43.419.7&nbsp;4.310.0006 frontal pole4535431737&nbsp;&nbsp;&nbsp;43.218.5&nbsp;3.810.0006 temporal fusiform cortex40-25-10-45-25.3&nbsp;-10.3-42.9&nbsp;3.140.0004 intracalcarine sulcus33-4-7711-2.85-75.811.5&nbsp;3.560.0002 stable_srchlght_interaction &lt;- rbind(corrp_ht, uncorrp_ht) %&gt;% huxtable::add_footnote(&quot;x, y, z refer to MNI coordinates of minimum p-value in cluster, t denotes the most extreme t-value, COG: center of gravity&quot;, border = 0.5) %&gt;% huxtable::as_flextable() %&gt;% flextable::fontsize(size=10, part=&quot;all&quot;) %&gt;% flextable::font(fontname = font2use) %&gt;% flextable::style(i=c(1,nrow(corrp_ht)+1), pr_c = officer::fp_cell(border.bottom = fp_border(), border.top = fp_border())) %&gt;% flextable::bg(i=c(1,nrow(corrp_ht)+1), bg=&quot;lightgrey&quot;, part=&quot;all&quot;) %&gt;% flextable::padding(., padding = 3, part=&quot;all&quot;) %&gt;% flextable::set_table_properties(., width=1, layout = &quot;autofit&quot;) %&gt;% flextable::set_caption(., caption = &quot;Searchlight Analysis: Interaction of virtual time and sequence membership&quot;, style=&quot;Normal&quot;) As for the within-day effect, we show the t-values at p&lt;0.01 uncorrected (based on field of view) and outline voxels surviving corrections (small volume correction) in black. # choose coordinates (these are chosen manually now) and get a data frame for labeling the plots coords &lt;- mni2vox(c(31,-15,-19)) label_df &lt;- data.frame(col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;, &quot;z&quot;),vox2mni(coords)), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;, &quot;z&quot;),vox2mni(coords))) # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,2,3), mar_ind=coords, row_ind = c(1,1,1), col_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), center_coords=TRUE) # get uncorrected p-values in FOV pfov_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_fov_fwhm3_tfce_p_tstat1.nii.gz&quot;) pfov_nii &lt;- readNIfTI2(pfov_fn) # t-values to plot (FOV) tvals_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_fov_fwhm3_tstat1.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) # threshold t-values for plotting thresh &lt;- 0.01 t_nii[pfov_nii &lt; 1-thresh] &lt;- NA # get data frame for the t-values t_df &lt;- getBrainFrame(brains=t_nii, mar=c(1,2,3), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), mar_ind=coords, mask=NULL, center_coords=TRUE) # load outline outl_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_svc_fwhm3_tfce_corrp_outline.nii.gz&quot;) outl_nii &lt;- readNIfTI2(outl_fn) # remove zero voxels and get dataframe at coordinates outl_nii[outl_nii == 0] &lt;- NA o_df &lt;- getBrainFrame(brains=outl_nii, mar=c(1,2,3), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), mar_ind=coords, mask=NULL, center_coords=TRUE) f7e &lt;- ggTemplate + geom_tile(data=t_df, aes(x=row,y=col,fill=value))+ geom_tile(data=o_df, aes(x=row,y=col), fill=&quot;black&quot;)+ facet_wrap(~col_ind, nrow=3, ncol=1, strip.position = &quot;bottom&quot;, scales = &quot;free&quot;) + scico::scale_fill_scico(palette = &quot;buda&quot;, begin = 0, end=1, name = element_text(&quot;t&quot;), limits = round(range(t_df$value), digits=2), breaks = round(range(t_df$value), digits=2)) + ggtitle(label = &quot;\\nsame vs.\\ndifferent&quot;) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), plot.title = element_text(hjust = 0.5), text = element_text(size=10), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title = element_text(size=8), legend.key.width = unit(0.01, &quot;npc&quot;), legend.key.height = unit(0.015, &quot;npc&quot;), legend.position = &quot;bottom&quot;, legend.justification = c(0.5, 0), aspect.ratio = 1)+ guides(fill = guide_colorbar( direction = &quot;horizontal&quot;, title.position = &quot;top&quot;, title.hjust = 0.5, label.position = &quot;bottom&quot;))+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=0, y=c(-84.64,-67.25,-86.51), aes(label = label), size = 8/.pt, family=font2use, vjust=1) + # y coords found via command below (to keep text aligned across panels): # ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range[1]-diff(ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range)/100*3 theme_no_ticks() print(f7e) file.copy(from = tvals_fn, to = file.path(dirs$source_dat_dir, &quot;f7e.nii.gz&quot;)) ## [1] TRUE Let’s compose figure 7. layout &lt;- &quot;ABCDE&quot; f7 &lt;- f7a+f7b+f7c+f7d+f7e + plot_layout(design = layout, guides = &quot;keep&quot;) &amp; plot_annotation(theme = theme(plot.margin = margin(t=0, r=-8, b=0, l=-19, unit=&quot;pt&quot;)), tag_levels = &#39;A&#39;) &amp; theme(legend.text=element_text(size=8), legend.title=element_text(size=8), plot.tag = element_text(size = 10, face=&quot;bold&quot;), text = element_text(size=10, family=font2use), plot.title = element_text(family=font2use, size=10, face=&quot;plain&quot;)) f7[[1]] &lt;- f7[[1]] + theme(plot.tag = element_text(margin = margin(l =13))) # save and print fn &lt;- here(&quot;figures&quot;, &quot;f7&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=f7, units = &quot;cm&quot;, width = 17.4, height = 12, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=f7, units = &quot;cm&quot;, width = 17.4, height = 12, dpi = &quot;retina&quot;, device = &quot;png&quot;) Figure 7. Overlapping representations of within- and across-sequence relations. A. Searchlight analysis results show a positive relationship between representational change and virtual temporal distances for event pairs from the same sequence in the bilateral anterior hippocampus. Statistical image is thresholded at puncorrected&lt;0.01; voxels within black outline are significant after correction for multiple comparisons using small volume correction. B. In the peak cluster from the independent within-sequence searchlight analysis (A), representational change was negatively related to virtual temporal distances between events from different sequences. Circles show individual participant Z-values from summary statistics approach; boxplot shows median and upper/lower quartile along with whiskers extending to most extreme data point within 1.5 interquartile ranges above/below the upper/lower quartile; black circle with error bars corresponds to mean±S.E.M.; distribution shows probability density function of data points. C. Searchlight analysis results show negative relationship between representational change and temporal distances for different-sequence event pairs. Statistical image is thresholded at puncorrected &lt;0.05. D. Within the anterior hippocampus, the effects for events from the same sequence and from two different sequences overlap. Visualization is based on statistical images thresholded at puncorrected &lt;0.05 within small volume correction mask. E. Searchlight analysis results show a bilateral interaction effect in the anterior hippocampus that is defined by a differential relationship of virtual temporal distances and representational change for events from the same and different sequences. Statistical image is thresholded at puncorrected&lt;0.01; voxels within black outline are significant after correction for multiple comparisons using small volume correction. A, C-E. Results are shown on the MNI template with voxels outside the field of view displayed in lighter shades of gray. See Supplemental Figure 9 for additional exploratory results. * p&lt;0.05 Exploratory Searchlight Results Further, we used a liberal threshold of puncorrected&lt;0.001 to explore the data for additional effects within our field of view. Exploratory searchlight results are shown in Supplemental Figure 9 and clusters with a minimum extent of 30 voxels are listed in Supplemental Tables 12, 14 and 15. For completeness, let’s create a supplemental figure that shows the additional clusters outside of our a priori regions of interest. Same Sequence # choose coordinates (these are chosen manually now) and get a data frame for labeling the plots mni_coords &lt;- c(48, 6, -13) coords &lt;- c(mni2vox(c(48,NA,NA))[1], mni2vox(c(6,NA,NA))[1], mni2vox(c(NA,-13,NA))[2]) label_df &lt;- data.frame(col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;, &quot;y&quot;),mni_coords), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;, &quot;y&quot;),mni_coords)) # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,1,2), mar_ind=coords, row_ind = c(1,1,1), col_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;y&quot;),mni_coords), center_coords=TRUE) # get uncorrected p-values in FOV pfov_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_fov_fwhm3_tfce_p_tstat1.nii.gz&quot;) pfov_nii &lt;- readNIfTI2(pfov_fn) # t-values to plot (FOV) tvals_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_fov_fwhm3_tstat1.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) # threshold t-values for plotting thresh &lt;- 0.01 t_nii[pfov_nii &lt; 1-thresh] &lt;- NA # get data frame for the t-values t_df &lt;- getBrainFrame(brains=t_nii, mar=c(1,1,2), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;y&quot;),mni_coords), mar_ind=coords, mask=NULL, center_coords=TRUE) sfiga_srchlght &lt;- ggTemplate + geom_tile(data=t_df, aes(x=row,y=col,fill=value))+ facet_wrap(~col_ind, nrow=3, ncol=1, strip.position = &quot;top&quot;, scales = &quot;free&quot;) + scico::scale_fill_scico(palette = &quot;devon&quot;, begin = 0.1, end=0.7, name = element_text(&quot;t&quot;), limits = round(range(t_df$value), digits=2), breaks = round(range(t_df$value), digits=2)) + ggtitle(label = &quot;\\nsame\\nsequence&quot;) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), plot.title = element_text(hjust = 0.5), text = element_text(size=10), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title = element_text(size=8), legend.key.width = unit(0.02, &quot;npc&quot;), legend.key.height = unit(0.015, &quot;npc&quot;), legend.position = &quot;bottom&quot;, legend.justification = c(0.5, 0), aspect.ratio = 1)+ guides(fill = guide_colorbar( direction = &quot;horizontal&quot;, title.position = &quot;top&quot;, title.hjust = 0.5, label.position = &quot;bottom&quot;))+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=0, y=c(-77.13,-94.03,-68.28), aes(label = label), size = 8/.pt, family=font2use, vjust=1) + # y coords found via command below (to keep text aligned across panels): # ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range[1]-diff(ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range)/100*3 theme_no_ticks() print(sfiga_srchlght) Different Sequence # choose coordinates (these are chosen manually now) and get a data frame for labeling the plots mni_coords &lt;- c(-2, 19, -70) coords &lt;- c(mni2vox(c(-2,NA,NA))[1], mni2vox(c(19,NA,NA))[1], mni2vox(c(NA,-70,NA))[2]) label_df &lt;- data.frame(col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;y&quot;),mni_coords), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;y&quot;),mni_coords)) # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,1,2), mar_ind=coords, row_ind = c(1,1,1), col_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;y&quot;),mni_coords), center_coords=TRUE) # get uncorrected p-values in FOV pfov_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_neg_randomise_fov_fwhm3_tfce_p_tstat1.nii.gz&quot;) pfov_nii &lt;- readNIfTI2(pfov_fn) # t-values to plot (FOV) tvals_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_neg_randomise_fov_fwhm3_tstat1.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) # threshold t-values for plotting thresh &lt;- 0.01 t_nii[pfov_nii &lt; 1-thresh] &lt;- NA # get data frame for the t-values t_df &lt;- getBrainFrame(brains=t_nii, mar=c(1,1,2), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;y&quot;),mni_coords), mar_ind=coords, mask=NULL, center_coords=TRUE) sfigb_srchlght &lt;- ggTemplate + geom_tile(data=t_df, aes(x=row,y=col,fill=value))+ facet_wrap(~col_ind, nrow=3, ncol=1, strip.position = &quot;top&quot;, scales = &quot;free&quot;) + scico::scale_fill_scico(palette = &quot;lajolla&quot;, begin = 0.3, end=0.9, direction = -1, name = element_text(&quot;t&quot;), limits = round(range(t_df$value), digits=2), breaks = round(range(t_df$value), digits=2)) + ggtitle(label = &quot;\\ndifferent\\nsequence&quot;) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), plot.title = element_text(hjust = 0.5), text = element_text(size=10), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title = element_text(size=8), legend.key.width = unit(0.02, &quot;npc&quot;), legend.key.height = unit(0.015, &quot;npc&quot;), legend.position = &quot;bottom&quot;, legend.justification = c(0.5, 0), aspect.ratio = 1)+ guides(fill = guide_colorbar( direction = &quot;horizontal&quot;, title.position = &quot;top&quot;, title.hjust = 0.5, label.position = &quot;bottom&quot;))+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=0, y=c(-93.97,-84.79,-82.34), aes(label = label), size = 8/.pt, family=font2use, vjust=1) + # y coords found via command below (to keep text aligned across panels): # ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range[1]-diff(ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range)/100*3 theme_no_ticks() print(sfigb_srchlght) Virtual Time and Sequence Interaction # choose coordinates (these are chosen manually now) and get a data frame for labeling the plots mni_coords &lt;- c(-4,17,20) coords &lt;- c(mni2vox(c(-4,NA,NA))[1], mni2vox(c(17,NA,NA))[1], mni2vox(c(NA,NA,20))[3]) label_df &lt;- data.frame(col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;z&quot;),mni_coords), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;z&quot;),mni_coords)) # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,1,3), mar_ind=coords, row_ind = c(1,1,1), col_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;z&quot;),mni_coords), center_coords=TRUE) # get uncorrected p-values in FOV pfov_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_fov_fwhm3_tfce_p_tstat1.nii.gz&quot;) pfov_nii &lt;- readNIfTI2(pfov_fn) # t-values to plot (FOV) tvals_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;day_time_interaction&quot;, &quot;day_time_interaction_randomise_fov_fwhm3_tstat1.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) # threshold t-values for plotting thresh &lt;- 0.01 t_nii[pfov_nii &lt; 1-thresh] &lt;- NA # get data frame for the t-values t_df &lt;- getBrainFrame(brains=t_nii, mar=c(1,1,3), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;x&quot;,&quot;z&quot;),mni_coords), mar_ind=coords, mask=NULL, center_coords=TRUE) sfigc_srchlght &lt;- ggTemplate + geom_tile(data=t_df, aes(x=row,y=col,fill=value))+ facet_wrap(~col_ind, nrow=3, ncol=1, strip.position = &quot;bottom&quot;, scales = &quot;free&quot;) + scico::scale_fill_scico(palette = &quot;buda&quot;, begin = 0, end=1, name = element_text(&quot;t&quot;), limits = round(range(t_df$value), digits=2), breaks = round(range(t_df$value), digits=2)) + ggtitle(label = &quot;\\nsame vs.\\ndifferent&quot;) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), plot.title = element_text(hjust = 0.5), text = element_text(size=10), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title = element_text(size=8), legend.key.width = unit(0.02, &quot;npc&quot;), legend.key.height = unit(0.015, &quot;npc&quot;), legend.position = &quot;bottom&quot;, legend.justification = c(0.5, 0), aspect.ratio = 1)+ guides(fill = guide_colorbar( direction = &quot;horizontal&quot;, title.position = &quot;top&quot;, title.hjust = 0.5, label.position = &quot;bottom&quot;))+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=0, y=c(-94,-86.85,-88.66), aes(label = label), size = 8/.pt, family=font2use, vjust=1) + # y coords found via command below (to keep text aligned across panels): # ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range[1]-diff(ggplot_build(f6g)$layout$panel_scales_y[[2]]$range$range)/100*3 theme_no_ticks() print(sfigc_srchlght) Overview Figure layout=&quot; ABC ABC ABC&quot; sfig_srchlght &lt;- sfiga_srchlght + sfigb_srchlght + sfigc_srchlght + plot_layout(design = layout, guides = &quot;keep&quot;) &amp; plot_annotation(theme = theme(plot.margin = margin(t=0, r=-9, b=0, l=-19, unit=&quot;pt&quot;)), tag_levels = &#39;A&#39;) &amp; theme(plot.tag = element_text(size = 10, face=&quot;bold&quot;), legend.text=element_text(size=8), legend.title=element_text(size=8), text = element_text(size=10, family=font2use), plot.title = element_text(family=font2use, size=10, face=&quot;plain&quot;)) sfig_srchlght[[1]] &lt;- sfig_srchlght[[1]] + theme(plot.tag = element_text(margin = margin(l=19))) # save and print fn &lt;- here(&quot;figures&quot;, &quot;sf09&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=sfig_srchlght, units = &quot;cm&quot;, width = 8.5, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=sfig_srchlght, units = &quot;cm&quot;, width = 8.5, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;) Supplemental Figure 9. Exploratory searchlight results. A. For same-sequence event pairs, clusters of voxels in which pattern similarity change correlated positively with temporal distances were detected in the frontal pole, frontal medial cortex and left entorhinal cortex (see Supplemental Table 12). B. Pattern similarity change correlated negatively with temporal distances between events from different sequences in the cerebellum and lingual gyrus (see Supplemental Table 14). C. The interaction effect, defined as correlations of temporal distances and pattern similarity change depending on whether pairs of events belonged to the same sequence or not, was observed in the occipital pole, lingual gyrus, frontal pole, temporal fusiform cortex and the intracalcerine sulcus (see Supplemental Table 15). A-C. Statistical images are thresholded at p&lt;0.01 uncorrected for display purposes. No clusters outside the hippocampal-entorhinal region survived corrections for multiple comparisons. 8.12 Correlation of behavioral generalization bias with searchlight effects We want to see whether the structure generalization bias in behavior relates to the RSA effects For that, we extract the t-value of the searchlight effect at the peak voxel for each participant and run Spearman correlations. We then asked whether the deviation between the average time of other events and an event’s true virtual time was systematically related to signed errors in constructed event times. A positive relationship between the relative time of other events and time construction errors indicates that, when other events at the same sequence position are relatively late, participants are biased to construct a later time for a given event than when the other events took place relatively early. In the summary statistics approach, we ran a linear regression for each participant (Figure 8B, Supplemental Figure 10A) and tested the resulting coefficients for statistical significance using the permutation-based procedures described above (Figure 8C). The regression coefficients from this approach were used to test for a relationship between the behavioral generalization bias and the hippocampal searchlight effects (see below). Correlation with across-sequence searchlight effect We start with the across-sequence searchlight effect. Note that the t-values are more negative for those subjects who show a stronger effect, so we might expect a negative correlation: Participants with stronger across-sequence generalization in the hippocampus could more strongly be biased by the abstracted structure in their specific constructions. # load the p-values from across-sequence searchlight (SVC corrected) p_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_neg_randomise_svc_fwhm3_tfce_corrp_tstat1.nii.gz&quot;) p_nii &lt;- readNIfTI2(p_fn) # load the 4D image with searchlight t-values for each participant tvals_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;,&quot;diff_day_vir_time&quot;, &quot;diff_day_vir_time_4d_smooth_fwhm3.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) # find the coordinates of peak voxel coords&lt;-which(p_nii ==max(p_nii)) # store the searchlight t-value for each subject at this voxel fit_beh_bias &lt;- fit_beh_bias %&gt;% add_column(srchlght_across_t=NA) for (i in 1:28){ curr_nii &lt;- t_nii[,,,i] fit_beh_bias$srchlght_across_t[i] &lt;- mean(curr_nii[cbind(coords)]) } # correlate the same-sequence searchlight effect with the behavioral bias stats &lt;- tidy(cor.test(fit_beh_bias$beta_rel_time_other_events, fit_beh_bias$srchlght_across_t, method=&quot;spearman&quot;)) # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuemethodalternative -0.1944.36e+030.322Spearman's rank correlation rhotwo.sided Spearman correlation of across-sequence searchlight effect in aHPC and behavioral generalization bias: r=-0.19, p=0.322 The effect is not significant, but let’s still visualize it as a scatter plot. # scatterplot f8e &lt;- ggplot(fit_beh_bias, aes(x=srchlght_across_t, y=beta_rel_time_other_events))+ geom_point(size = 1, shape = 16) + geom_smooth(method=&#39;lm&#39;, formula= y~x, colour=unname(aHPC_colors[&quot;across_main&quot;]), fill=unname(aHPC_colors[&quot;across_main&quot;]), size=1)+ scale_x_continuous(labels = c(-1.5, -1, -0.5, 0, 0.5, 1)) + xlab(&quot;diff. sequence\\nsearchlight (t)&quot;) + ylab(&quot;behavioral bias&quot;) + annotate(&quot;text&quot;, x=Inf, y=Inf, hjust=1,vjust=1, size = 6/.pt, family = font2use, label=sprintf(&quot;r=%.2f,\\np=%.3f&quot;, round(stats$estimate,2), round(stats$p.value, 3)))+ theme_cowplot() + theme(text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), legend.position = &quot;none&quot;) f8e # save source data source_dat &lt;-ggplot_build(f8e)$data[[1]] readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f8e.txt&quot;)) Correlation with same-sequence searchlight effect Next, we look for a correlation with the same-sequence searchlight effect. Again, we might expect to see a negative correlation: Participants with a stronger representation of the within-sequence relationships might be less biased in their construction by the overall structure. # load the p-values from same-sequence searchlight (SVC corrected) p_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_svc_fwhm3_tfce_corrp_tstat1.nii.gz&quot;) p_nii &lt;- readNIfTI2(p_fn) # load the 4D image with searchlight t-values for each participant tvals_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;same_day_vir_time&quot;, &quot;same_day_vir_time_4d_smooth_fwhm3.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) # find the coordinates of peak voxel coords&lt;-which(p_nii ==max(p_nii)) # store the searchlight t-value for each subject at this voxel fit_beh_bias &lt;- fit_beh_bias %&gt;% add_column(srchlght_t=NA) for (i in 1:28){ curr_nii &lt;- t_nii[,,,i] fit_beh_bias$srchlght_t[i] &lt;- mean(curr_nii[cbind(coords)]) } # correlate the same-sequence searchlight effect with the behavioral bias stats &lt;- tidy(cor.test(fit_beh_bias$beta_rel_time_other_events, fit_beh_bias$srchlght_t, method=&quot;spearman&quot;)) # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuemethodalternative -0.5265.58e+030.00456Spearman's rank correlation rhotwo.sided Spearman correlation of within-sequence searchlight effect in aHPC and behavioral generalization bias: r=-0.53, p=0.005 Indeed, we observe a significant correlation. Let’s make a plot to visualize it. # scatterplot f8f &lt;- ggplot(fit_beh_bias, aes(x=srchlght_t, y=beta_rel_time_other_events))+ geom_point(size = 1, shape = 16) + geom_smooth(method=&#39;lm&#39;, formula= y~x, colour=unname(aHPC_colors[&quot;within_main&quot;]), fill=unname(aHPC_colors[&quot;within_main&quot;]), size=1)+ scale_x_continuous(labels = c(-0.5, 0, 0.5, 1, 1.5)) + xlab(&quot;same sequence\\nsearchlight (t)&quot;) + ylab(&quot;behavioral bias&quot;) + annotate(&quot;text&quot;, x=Inf, y=Inf, hjust=1,vjust=1, size = 6/.pt, family = font2use, label=sprintf(&quot;r=%.2f,\\np=%.3f&quot;, round(stats$estimate,2), round(stats$p.value, 3))) + theme_cowplot() + theme(text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), legend.position = &quot;none&quot;) f8f # save source data source_dat &lt;-ggplot_build(f8f)$data[[1]] readr::write_tsv(source_dat %&gt;% select(x,y), file = file.path(dirs$source_dat_dir, &quot;f8f.txt&quot;)) We are now ready to compose figure 8. layout = &quot; ACDE BCDF&quot; f8 &lt;- plot_spacer() + f8b + f8c + f8d + f8e + f8f + plot_layout(design = layout, guides = &quot;keep&quot;) &amp; theme(text = element_text(size=10, family=font2use), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), legend.position = &quot;none&quot; ) &amp; plot_annotation(theme = theme(plot.margin = margin(t=0, r=0, b=0, l=-5, unit=&quot;pt&quot;)), tag_levels = list(c(&#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;, &#39;H&#39;, &#39;I&#39;, &#39;J&#39;))) # save as png and pdf and print to screen fn &lt;- here(&quot;figures&quot;, &quot;f8&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=f8, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=f8, units = &quot;cm&quot;, width = 17.4, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;) Figure 8. Structural knowledge biases construction of event times. A. The generalization bias quantifies the influence of structural knowledge on the construction of individual event times. For each event, the mean time of events at the same sequence position in the other sequences was calculated to test whether event times were biased towards the relative time of other events. B. The scatterplot illustrates the generalization bias for an example participant. Each circle corresponds to one event and the regression line highlights the relationship between the relative time of other events and the errors in constructed event times. The example participant was chosen to have a median-strength generalization bias. See Supplemental Figure 10 for the entire sample. Correlation coefficient is based on Pearson correlation. C. The relative time of events from other sequences predicted signed event time construction errors as measured in the timeline task. Positive values indicate that when other events took place late relative to a specific event, the time of that event was estimated to be later than when other events were relatively early. Circles show individual participant Z-values from participant-specific linear models (B); boxplot shows median and upper/lower quartile along with whiskers extending to most extreme data point within 1.5 interquartile ranges above/below the upper/lower quartile; black circle with error bars corresponds to mean±S.E.M.; distribution shows probability density function of data points. D. The generalization bias in event time construction through structural knowledge was replicated in an independent sample (n=46) based on Montijn et al.66. Data shown as in B. E. The behavioral generalization bias (regression coefficients from summary statistics approach) did not correlate significantly with the across-sequence generalization effect in the anterior hippocampus (searchlight peak voxel t-values). F. We observed a significant negative correlation between the same-sequence searchlight effect (peak voxel t-values) and the behavioral generalization bias (regression coefficients from summary statistics approach), suggesting that participants with strong hippocampal representations of the temporal relations between events from the same sequence were less biased by structural knowledge in their construction of event times. Statistics in E and F are based on Spearman correlation. 8.13 Mixed Model Summaries Supplemental Figure Make supplemental mixed model figure. layout = &quot; ABCDEF GHIJKL MNOPQR&quot; sfigmm_a &lt;- sfigmm_a + ggtitle(&quot;remembered times: all time metrics&quot;) sfigmm_c &lt;- sfigmm_c + ggtitle(&quot;aHPC same seq.: virtual time&quot;) sfigmm_e &lt;- sfigmm_e + ggtitle(&quot;aHPC same seq.: all time metrics&quot;) sfigmm_g &lt;- sfigmm_g + ggtitle(&quot;aHPC different seq.: virtual time&quot;) sfigmm_i &lt;- sfigmm_i + ggtitle(&quot;aHPC: interaction virtual time &amp; seq.&quot;) sfigmm_k &lt;- sfigmm_k + ggtitle(&quot;alEC all events: virtual time&quot;) sfigmm_m &lt;- sfigmm_m + ggtitle(&quot;searchlight peak different seq.: virtual time&quot;) sfigmm_o &lt;- sfigmm_o + ggtitle(&quot;generalization bias (fMRI sample)&quot;) sfigmm_q &lt;- sfigmm_q + ggtitle(&quot;generalization bias (replication sample)&quot;) sfigmm &lt;- sfigmm_a + sfigmm_b + sfigmm_c + sfigmm_d + sfigmm_e + sfigmm_f + sfigmm_g + sfigmm_h + sfigmm_i + sfigmm_j + sfigmm_k + sfigmm_l + sfigmm_m + sfigmm_n + sfigmm_o + sfigmm_p + sfigmm_q + sfigmm_r + plot_layout(design = layout, guides = &quot;collect&quot;) &amp; theme(text = element_text(size=10, family=font2use), axis.title = element_blank(), axis.text = element_text(size=8), legend.text=element_text(size=8), legend.title=element_text(size=8), legend.position = &quot;bottom&quot;, legend.justification = 0, legend.spacing.x = unit(-0.1, &#39;cm&#39;), legend.margin=margin(t = 0, r = 0.1, l = 0, b = 0, unit=&#39;cm&#39;), plot.title = element_text(size=8, family=font2use, hjust = 0, vjust = 1, margin = margin(t=0.2, r=0, b=0, l=0, unit = &quot;cm&quot;)), plot.title.position = &quot;plot&quot;, plot.tag.position = c(0, 1), plot.tag = element_text(hjust = 0, vjust = 0) ) &amp; plot_annotation(tag_levels = &quot;A&quot;) # save as png and pdf and print to screen fn &lt;- here(&quot;figures&quot;, &quot;sf04&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=sfigmm, units = &quot;cm&quot;, width = 17.4, height = 18, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=sfigmm, units = &quot;cm&quot;, width = 17.4, height = 18, dpi = &quot;retina&quot;, device = &quot;png&quot;) Supplemental Figure 4. Mixed model results. Dot plots show parameter estimates and 95% CIs for fixed effects of mixed model analyses. Line plots show estimated marginal means. A, B. Remembered times in the time line task are predicted by virtual event times with order and real time in the model (c.f. Figure 2B). C, D. Temporal distances in virtual time explain representational change in the anterior hippocampus (aHPC) for same-sequence events (c.f. Figure 4B). E, F. Temporal distances in virtual time explain representational change in the aHPC for same-sequence events when competing for variance with temporal distances based on order and real time (c.f. Figure 4D). G, H. Temporal distances in virtual time explain representational change in the aHPC for different-sequence events (c.f. Figure 5A). I, J. There was a significant interaction of virtual temporal distances and sequence membership characterized by a differential relationship between temporal distances and aHPC representational change for event pairs from the same sequence or from different sequences (c.f. Figure 5A). K, L. Virtual temporal distances explain representational change in the anterior-lateral entorhinal cortex (alEC) when collapsing across all event pairs (c.f. Figure 6B). M, N. In the aHPC peak cluster of the same-sequence searchlight analysis, virtual temporal distances were siginificantly related to representational change for events from different sequences (c.f. Figure 7B). O-R. The relative time of events from other sequences predicted signed event time construction errors as measured in the timeline task (c.f. Figure 8CD) in the main fMRI sample (O, P) and in the independent replication sample (Q, R). Supplemental Tables Write and save the word document with the supplemental tables. # make a list of the supplemental tables we want to write to file stables &lt;- list(stable_lme_memtime_time_metrics, #1 stable_lme_aHPC_virtime_same_seq, #2 stable_lme_aHPC_virtime_same_seq_first_last, #3 stable_lme_aHPC_virtime_same_seq_time_metrics, #4 stable_lme_aHPC_virtime_diff_seq, #5 stable_lme_aHPC_virtime_diff_seq_time_metrics, #6 stable_lme_aHPC_virtime_same_vs_diff_seq, #7 stable_lme_aHPC_virtime_same_vs_diff_seq_all_interactions, #8 stable_lme_alEC_virtime_all_events, #9 stable_lme_alEC_virtime_all_comps_time_metrics, #10 stable_lme_virtime_aHPC_vs_alEC, #11 stable_srchlght_same_seq, #12 stable_lme_same_seq_cluster_virtime_diff_seq, #13 stable_srchlght_diff_seq, #14 stable_srchlght_interaction, #15 stable_lme_beh_gen_bias, # 16 stable_lme_beh_gen_bias_replication) #17 # loop over tables and write to file for (i in 1:length(stables)){ # run auto-numbering to find table number and define table heading f_par &lt;- officer::fpar(officer::run_autonum(seq_id = &quot;tab&quot;, pre_label = &quot;Supplemental Table &quot;, post_label = &quot;&quot;)) # add table heading and table to the document stables_docx &lt;- stables_docx %&gt;% officer::body_add_fpar(f_par, style = &quot;heading 2&quot;, pos = &quot;after&quot;) %&gt;% # table heading flextable::body_add_flextable(stables[[i]], align = &quot;left&quot;) # table # page break if not the last table if (i&lt;length(stables)){ stables_docx &lt;- stables_docx %&gt;% officer::body_add_break(pos = &quot;after&quot;) # page break } } # save the document print(stables_docx, target = here(&quot;virtem_code&quot;, &quot;docs&quot;, &quot;supplemental_tables.docx&quot;)) 8.13.1 Source data Let’s also put all the source data into a zipped folder. zip::zip(zipfile = file.path(dirs$source_dat_dir, &quot;SourceData.zip&quot;), files = dir(dirs$source_dat_dir, full.names = TRUE), mode = &quot;cherry-pick&quot;) "],
["signal-to-noise-ratio.html", "9 Signal to noise ratio 9.1 Calculate voxel-wise temporal SNR 9.2 Average tSNR per ROI 9.3 Contrast tSNR between aHPC and alEC", " 9 Signal to noise ratio The temporal signal-to-noise ratio was quantified as the mean unsmoothed signal over time divided by its standard deviation. It was calculated for each voxel and then averaged across voxels in a region of interest. We want to contrast the signal to noise ratio in our regions of interest. We will look at both the temporal and spatial signal to noise ratio. We will used the cleaned voxel time series that we also extracted the multi-voxel patterns from for this. snr_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;SNR&quot;) invisible(lapply(file.path(snr_dir, subjects), function(x) if(!dir.exists(x)) dir.create(x, recursive = TRUE))) 9.1 Calculate voxel-wise temporal SNR The first step is to calculate the temporal signal to noise ratio for each voxel. We use fslmaths for that via the fslr-package. Specifically, the tSNR of a voxel is its temporal mean divided by its standard deviation over time. This is done for each of the ten blocks of each run of the picture viewing task. # build a tibble with the info we&#39;ll need tp build the file names (will be overwritten later) snr_df &lt;- tibble(subject = rep(subjects, each = n_runs*n_blocks), run = rep(rep(runs, each = n_blocks), n_subs), run_no = rep(rep(c(1,2), each = n_blocks), n_subs), block = rep(c(1:n_blocks), n_runs*n_subs) ) # create the file names to use filt_func_fn &lt;- file.path(dirs$samespace_dir, paste0(&quot;VIRTEM_P&quot;, snr_df$subject), sprintf(&quot;VIRTEM_P%s_RSA_%02d_Block%02d_func.nii.gz&quot;, snr_df$subject, snr_df$run_no, snr_df$block)) tMean_fn &lt;- file.path(snr_dir, snr_df$subject, sprintf(&quot;%s_RSA_%s_block%02d_tMean.nii.gz&quot;, snr_df$subject, snr_df$run, snr_df$block)) tSd_fn &lt;- file.path(snr_dir, snr_df$subject, sprintf(&quot;%s_RSA_%s_block%02d_tSd.nii.gz&quot;, snr_df$subject, snr_df$run, snr_df$block)) tSNR_fn &lt;- file.path(snr_dir, snr_df$subject, sprintf(&quot;%s_RSA_%s_block%02d_tSNR.nii.gz&quot;, snr_df$subject, snr_df$run, snr_df$block)) # define function to calculate tSNR calc_tSNR &lt;- function(fourD_fn=NULL, tMean_fn=NULL, tStd_fn=NULL, tSNR_fn=NULL){ # calculate temporal mean for each voxel fslr::fsl_maths(file = fourD_fn, opts = &quot;-Tmean&quot;, outfile = tMean_fn, verbose = FALSE, retimg = FALSE) # calculate temporal mean for each voxel fslr::fsl_maths(file = fourD_fn, opts = &quot;-Tstd&quot;, outfile = tStd_fn, verbose = FALSE, retimg = FALSE) # calculate temporal mean for each voxel fslr::fsl_div(file = tMean_fn, file2 = tStd_fn, outfile = tSNR_fn, verbose = FALSE, retimg = FALSE) } # apply to images in questions (invisible to avoid overly long output) invisible(mapply(calc_tSNR, fourD_fn=filt_func_fn, tMean_fn=tMean_fn, tStd_fn=tSd_fn, tSNR_fn=tSNR_fn)) 9.2 Average tSNR per ROI To find the average tSNR for a given ROI, we load the tSNR image and the ROI mask before calculating the mean across voxels. Here we define the function that returns the mean tSNR, given the file names of the tSNR image and the ROI masks. # function to calculate the mean tSNR for an ROI get_ROI_tSNR &lt;- function(roi_fn, tSNR_fn){ # load images tSNR_nii &lt;- neurobase::readNIfTI2(tSNR_fn) roi_nii &lt;- neurobase::readNIfTI2(roi_fn) # the tSNR values of the ROI voxels snr_dat &lt;- tSNR_nii[roi_nii==1] # return the average return(mean(snr_dat)) } Let’s apply the function and store the tSNR values for each block in each run. snr_df &lt;- tibble(subject = rep(subjects, each = n_runs*n_blocks*length(rois)), roi = rep(rep(rois, each = n_runs * n_blocks),n_subs), run = rep(rep(runs, each = n_blocks), length(rois)*n_subs), block = rep(c(1:n_blocks), n_runs*length(rois)*n_subs), tSNR = NA) for (i in 1:nrow(snr_df)){ snr_df$tSNR[i] &lt;- get_ROI_tSNR(roi_fn = here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, snr_df$roi[i], &quot;samespace&quot;, sprintf(&quot;P%s_%s_ss_fs.nii.gz&quot;, snr_df$subject[i], snr_df$roi[i])), tSNR_fn = file.path(snr_dir, snr_df$subject[i], sprintf(&quot;%s_RSA_%s_block%02d_tSNR.nii.gz&quot;, snr_df$subject[i], snr_df$run[i], snr_df$block[i]))) } Lastly, we want to write out the dataframe with the tSNR values for later analysis. # save the relevant data fn &lt;- file.path(dirs$data4analysis, &quot;snr.txt&quot;) write.table(snr_df, file = fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) 9.3 Contrast tSNR between aHPC and alEC We begin by loading the SNR data from file. # load data from CSV fn &lt;- file.path(dirs$data4analysis, &quot;snr.txt&quot;) col_types_list &lt;- cols_only( subject = col_factor(), roi = col_factor(levels = rois), run = col_factor(levels = runs), block = col_double(), tSNR = col_double() ) snr_data &lt;- as_tibble(read_csv(fn, col_types = col_types_list)) head(snr_data) subjectroirunblocktSNR 031aHPC_lrpre126.9 031aHPC_lrpre225.2 031aHPC_lrpre325.1 031aHPC_lrpre425.5 031aHPC_lrpre525.4 031aHPC_lrpre624.1 To contrast the global tSNR between the aHPC and the alEC, we need to average across all blocks and runs to obtain one value per ROI per participant. # calculate the average tSNR per ROI for each suject snr_data %&gt;% filter(roi == &quot;aHPC_lr&quot; | roi == &quot;alEC_lr&quot;) %&gt;% group_by(subject, roi) %&gt;% summarise(tSNR = mean(tSNR), .groups=&quot;drop&quot;) -&gt; snr_avg To test whether the tSNR differs between the aHPC and the alEC we run a permutation-based t-test on the within-subject differences. set.seed(56) # set seed for reproducibility stats &lt;- snr_avg %&gt;% filter(roi==&quot;aHPC_lr&quot; | roi == &quot;alEC_lr&quot;) %&gt;% group_by(subject) %&gt;% summarise(tSNR_diff = tSNR[roi==&quot;aHPC_lr&quot;]-tSNR[roi==&quot;alEC_lr&quot;],.groups = &quot;drop&quot;) %&gt;% # test difference against 0 select(tSNR_diff) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for paired samples using non-central t-distribution for CI d&lt;-cohen.d(d=(snr_avg %&gt;% filter(roi == &quot;aHPC_lr&quot;))$tSNR, f=(snr_avg %&gt;% filter(roi == &quot;alEC_lr&quot;))$tSNR, paired=TRUE, pooled=TRUE, hedges.correction=TRUE, noncentral=TRUE) ## Warning in pt(q = t, df = df, ncp = x): full precision may not have been achieved in &#39;pnt{final}&#39; ## Warning in pt(q = t, df = df, ncp = x): full precision may not have been achieved in &#39;pnt{final}&#39; stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternativeddCI_lowdCI_high 4.4412.41.09e-120.0001273.715.17One Sample t-testtwo.sided1.991.653.13 Summary Statistics: paired t-test comparing tSNR in aHPC and alEC t27=12.43, p=0.000, d=1.99, 95% CI [1.65, 3.13] Here is a plot of the results # add column with custom jitter snr_avg &lt;- snr_avg %&gt;% mutate(x_jit = as.numeric(roi) + rep(jitter(rep(0,n_subs), amount=0.05), each=2) * rep(c(1,-1),n_subs)) snr_figa &lt;- ggplot(data=snr_avg, aes(x=roi, y=tSNR, fill = roi, color = roi)) + geom_boxplot(aes(group=roi), position = position_nudge(x = 0, y = 0), width = .1, colour = &quot;black&quot;, outlier.shape = NA) + scale_fill_manual(values = unname(c(aHPC_colors[&quot;within_main&quot;], alEC_colors[&quot;main&quot;]))) + scale_color_manual(values = unname(c(aHPC_colors[&quot;within_main&quot;], alEC_colors[&quot;main&quot;])), name = &quot;roi&quot;, labels=c(&quot;aHPC&quot;, &quot;alEC&quot;)) + gghalves::geom_half_violin(data = snr_avg %&gt;% filter(roi == &quot;aHPC_lr&quot;), aes(x=roi, y=tSNR), position=position_nudge(-0.1), side = &quot;l&quot;, color = NA) + gghalves::geom_half_violin(data = snr_avg %&gt;% filter(roi == &quot;alEC_lr&quot;), aes(x=roi, y=tSNR), position=position_nudge(0.1), side = &quot;r&quot;, color = NA) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 1, shape = 16, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(c(-0.1, 0.1)), colour = &quot;black&quot;, width = 0, size = 0.5) + geom_line(aes(x = x_jit, group=subject,), color = ultimate_gray, position = position_nudge(c(0.15, -0.15))) + geom_point(aes(x=x_jit, fill = roi), position = position_nudge(c(0.15, -0.15)), shape=16, size = 1) + scale_x_discrete(labels = c(&quot;aHPC&quot;, &quot;alEC&quot;)) + ylab(&#39;tSNR&#39;) + xlab(&#39;ROI&#39;) + guides(fill= &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2))) + #annotate(geom = &quot;line&quot;,x=c(1.3, 1.7), y=25, size=0.5) + annotate(geom = &quot;text&quot;, x = c(1.5), y = Inf, label = &#39;underline(&quot; *** &quot;)&#39;, hjust = 0.5, vjust = 1, parse = TRUE) + guides(fill= &quot;none&quot;, color=guide_legend(override.aes=list(fill=NA, alpha = 1, size=2))) + theme_cowplot() + theme(text = element_text(size=10, family = font2use), axis.text = element_text(size=8), legend.position = &quot;right&quot;) Save the figure with theme to match the other figures. snr_fig &lt;- snr_figa + theme(plot.tag = element_text(size = 10, face=&quot;bold&quot;), plot.tag.position = &quot;topleft&quot;, #plot.margin=grid::unit(c(5,8,5,0), &quot;pt&quot;), text = element_text(size=10, family=font2use), legend.text=element_text(size=8), legend.title=element_blank(), legend.position = &#39;bottom&#39;, legend.spacing.x = unit(1, &#39;mm&#39;), legend.key.size = unit(3,&quot;mm&quot;), legend.margin = margin(0,0,0,1, unit=&quot;cm&quot;)) # save and print fn &lt;- here(&quot;figures&quot;, &quot;sf08&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=snr_fig, units = &quot;cm&quot;, width = 5, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=snr_fig, units = &quot;cm&quot;, width = 5, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;) Supplemental Figure 8. Temporal signal-to-noise ratio in the anterior hippocampus and the anterior-lateral entorhinal cortex. A. The temporal signal-to-noise ratio was quantified as the mean unsmoothed signal over time divided by its standard deviation. It was calculated for each voxel and then averaged across voxels in a region of interest. The temporal signal-to-noise ratio was higher in the anterior hippocampus (aHPC) than in the anterior-lateral entorhinal cortex (alEC, summary statistics: t27=12.43, p&lt;0.001, d=1.99, 95% CI [1.65, 3.13]). Circles show individual participant values; boxplot shows median and upper/lower quartile along with whiskers extending to most extreme data point within 1.5 interquartile ranges above/below the upper/lower quartile; black circle with error bars corresponds to mean±S.E.M.; distribution shows probability density function of data points. *** p&lt;0.001 "],
["rsa-in-searchlight-peak-with-different-threshold.html", "10 RSA in searchlight peak with different threshold 10.1 Prepare RSA in searchlight peak 10.2 RSA", " 10 RSA in searchlight peak with different threshold This section is to address a reviewer comment about whether the results of this analysis differ when running it with a different threshold (p&lt;0.001 rather than p&lt;0.01) to define the ROI based on the same-sequence searchlight. The code below is thus copied from the original script and modified to include the different p-threshold. To test whether within- and across-sequence representations overlap, we defined an ROI based on the within-sequence searchlight analysis. Specifically, voxels belonging to the cluster around the peak voxel, thresholded at p&lt;0.01 uncorrected within our small volume correction mask, were included. The analysis of representational change was then carried out as described for the other ROIs above. The results observed using a threshold of p&lt;0.001 were not statistically different from those obtained with a threshold of p&lt;0.01 (t27=-0.95, p=0.338; test against 0 using the ROI resulting from the p&lt;0.001 threshold: t27=-1.98, p=0.056). 10.1 Prepare RSA in searchlight peak To show that overlapping clusters of voxels drive both the within- and across sequence effects, we will run the across-sequence analysis in the cluster of voxels that show the within-day effect. Because these are independent comparisons, we can use the within-day searchlight to define a region of interest for the across-day analysis. Create the ROI mask The first steps are to prepare the ROI mask we want to use. Thus, we need to threshold the ROI from the main analysis in MNI space and move the resulting mask to each participant’s functional space for the analysis. threshs &lt;- c(&quot;uncorrp001lh&quot;) for (i_thresh in threshs){ # threshold the searchlight results if (i_thresh == &quot;corrp05&quot; | i_thresh == &quot;corrp05lh&quot;){ in_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_svc_fwhm3_tfce_corrp_tstat1.nii.gz&quot;) } else{ in_fn &lt;- file.path(dirs$data4analysis, &quot;searchlight_results&quot;, &quot;same_day_vir_time&quot;, &quot;same_day_vir_time_randomise_svc_fwhm3_tfce_p_tstat1.nii.gz&quot;) } svc_fn &lt;- file.path(dirs$mask_dir, &quot;svc&quot;, &quot;svc_mask.nii.gz&quot;) bin_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, sprintf(&quot;searchlight_same-day_svc_%s.nii.gz&quot;, i_thresh)) fsl_thresh(file = in_fn, outfile = bin_fn, thresh = 0.95, opts = sprintf(&quot;-bin -mas %s&quot;, svc_fn), verbose = FALSE, retimg = FALSE) # peak cluster is in left hemisphere, so don&#39;t include any voxels in right hemisphere if (i_thresh == &quot;uncorrp001lh&quot; | i_thresh == &quot;corrp05lh&quot;){ lh_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;mni_masks&quot;, &quot;left_hemi.nii.gz&quot;) fsl_maths(file=bin_fn, outfile = lh_fn, opts = &quot;-mul 0 -add 1 -roi 91 182 0 218 0 182 0 1&quot;, retimg = FALSE, verbose = FALSE) fsl_mul(file=bin_fn, file2=lh_fn, outfile=bin_fn, retimg = FALSE, verbose = FALSE) } # let&#39;s have a look at the mask we created mni_nii &lt;- readNIfTI2(mni_fname(&quot;1&quot;)) roi_nii &lt;- readNIfTI2(bin_fn) coords &lt;- c(statip::mfv1(which(roi_nii==1, arr.ind = TRUE)[,1]), statip::mfv1(which(roi_nii==1, arr.ind = TRUE)[,2]), statip::mfv1(which(roi_nii==1, arr.ind = TRUE)[,3])) ortho2(mni_nii, y = roi_nii, xyz = coords, add.orient = TRUE) # check the number of voxels in this ROI sum(c(roi_nii)) } The resulting ROI mask is now coregistered from MNI 1mm space to the analysis space of the wholebrain functional sequence. Finally, it is thresholded at a probability of 0.5. for (i_thresh in threshs){ samespace_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;same-day_searchlight&quot;, &quot;samespace&quot;) if (!dir.exists(samespace_dir)){dir.create(samespace_dir, recursive = TRUE)} # name of transformation matrix file to move from highres to functional space standard2func &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;wholebrain&quot;, paste0(&quot;VIRTEM_P&quot;, subjects, &quot;.feat&quot;), &quot;reg&quot;, &quot;standard2example_func.mat&quot;) # use the mean EPI of wholebrain image as a reference mean_epi &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;samespace&quot;, paste0(&quot;VIRTEM_P&quot;, subjects), paste0(&quot;VIRTEM_P&quot;, subjects, &quot;_wholebrain.nii.gz&quot;)) # define output files in samespace (ss) = analysis space based on the wholebrain EPI roi_ss &lt;- file.path(samespace_dir, sprintf(&quot;P%s_%s_%s_ss.nii.gz&quot;, subjects, &quot;same-day_searchlight&quot;, i_thresh)) # apply FSL flirt to move ROI from standard to wholebrain functional space invisible(mapply(flirt_apply, infile = bin_fn, reffile = mean_epi, initmat = standard2func, outfile = roi_ss, verbose = FALSE, retimg = FALSE)) # use fslmaths to binarize the masked ROIs using a threshold of 0.5 out &lt;- mapply(fsl_thresh, file = roi_ss, outfile = roi_ss, thresh = 0.5, opts = &quot;-bin&quot;, verbose = FALSE, retimg = FALSE) } Calculating the correlation matrices The following steps are analogous to the preparation of the functional data in the ROI and searchlight analyses. For the main searchlight analyses we already cleaned the voxel-wise time series and extracted the volumes relevant to RSA. Thus, to calculate the correlation matrices and to calculate pattern similarity changes, we fall back onto the scripts from the main ROI analyses and the searchlight. for (i_thresh in threshs){ # for all 10x10 comparisons we will be averaging all comparisons apart from the diagonal # to exclude same_block comparisons no_diag &lt;- matrix(data = TRUE, nrow=10, ncol = 10) diag(no_diag)&lt;- FALSE out_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;correlation_matrices&quot;, &quot;same-day_searchlight&quot;) if (!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} for (i_sub in subjects){ # load the ROI based on the searchlight roi_fn &lt;-file.path(samespace_dir, sprintf(&quot;P%s_%s_%s_ss.nii.gz&quot;, i_sub, &quot;same-day_searchlight&quot;, i_thresh)) roi_nii &lt;- readNIfTI(roi_fn, reorient=FALSE) # array indices of ROI voxels roi_vox &lt;- which(roi_nii == 1, arr.ind=TRUE) sprintf(&quot;%s: %d voxels\\n&quot;, i_sub, nrow(roi_vox)) for (i_run in 1:n_runs){ # load the relevant functional volumes rel_vol_fn &lt;- file.path(dirs$searchlight, &quot;rel_vols_4D&quot;, sprintf(&quot;%s_run%02d_rel_vols.nii.gz&quot;, i_sub, i_run)) func_nii &lt;- readNIfTI(rel_vol_fn, reorient=FALSE) # get the ROI voxels rel_dat &lt;- array(NA, c(n_pics*n_blocks, nrow(roi_vox))); # images in rows (ROI voxels), voxels in columns for (i in 1:nrow(roi_vox)) { # i &lt;- 1 curr_vox &lt;- func_nii[roi_vox[i,1], roi_vox[i,2], roi_vox[i,3],] if (sd(curr_vox) &gt; 0) {rel_dat[,i] &lt;- curr_vox} else { stop(&quot;zero variance voxel&quot;)} } # data is in repetition (row) by voxel (col) format, so we transpose # to get a voxel x repetition format rel_dat &lt;- t(rel_dat) # calculate correlation matrix (trial by trial) for pre and post run cor_mat_trial &lt;- cor(rel_dat, rel_dat) # initialize condition by condition correlation matrix for pre and post run corr_mat &lt;- matrix(nrow = 20, ncol = 20) # loop over all picture comparisons for(i_pic1 in 1:20){ for(i_pic2 in 1:20){ # extract the current 10x10 correlation matrix i1 &lt;- (1+(i_pic1-1)*10):(i_pic1*10) i2 &lt;- (1+(i_pic2-1)*10):(i_pic2*10) curr_mat &lt;- cor_mat_trial[i1, i2] # average the correlations while excluding diagonal (same block comparisons) corr_mat[i_pic1, i_pic2] &lt;- mean(curr_mat[no_diag]) } } # save the correlation matrix fn &lt;- file.path(out_dir, sprintf(&quot;%s_%s_%s_%s_corr_mat.txt&quot;, i_sub, &quot;same-day_searchlight&quot;, i_thresh, runs[i_run])) write.table(corr_mat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = FALSE) } } } Calculate Pattern Similarity Change In the next step, we calculate how the correlation between patterns change from the first to the second picture viewing task, i.e. through the day learning task. To do this, we load both correlation matrices and reduce them to the upper triangle, excluding the diagonal. Then, the correlations are Fisher Z-transformed and the similarity values from the pre-learning picture viewing task are subtracted from those of the post-learning picture viewing task to isolate pattern similarity change. The correlations and their changes are then saved together with information about the pictures that were compared. This part is based on the corresponding script from the main ROI analyses. in_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;correlation_matrices&quot;, &quot;same-day_searchlight&quot;) out_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;pattern_similarity_change&quot;, &quot;same-day_searchlight&quot;) if (!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} for (i_thresh in threshs){ for(i_sub in subjects){ # load pre correlation matrix fn &lt;- file.path(in_dir, sprintf(&quot;%s_%s_%s_pre_corr_mat.txt&quot;, i_sub, &quot;same-day_searchlight&quot;, i_thresh)) corr_mat_pre &lt;- read.table(fn, sep = &quot;,&quot;, dec = &quot;.&quot;) # load post correlation matrix fn &lt;- file.path(in_dir, sprintf(&quot;%s_%s_%s_post_corr_mat.txt&quot;, i_sub, &quot;same-day_searchlight&quot;, i_thresh)) corr_mat_post &lt;- read.table(fn, sep = &quot;,&quot;, dec = &quot;.&quot;) # reduce to upper triangle of correlations (without diagonal) pre_corrs &lt;- corr_mat_pre[upper.tri(corr_mat_pre, diag = FALSE)] post_corrs &lt;- corr_mat_post[upper.tri(corr_mat_post, diag = FALSE)] # create a tibble with the correlations pics &lt;- which(upper.tri(corr_mat_post), arr.ind=TRUE) corr_df &lt;- tibble(pic1 = pics[,1], pic2 = pics[,2], pre_corrs, post_corrs) # Fisher Z transform correlations and calculate pattern similarity change # by subtracting the pre-correlations from the post-correlations corr_df$ps_change &lt;- FisherZ(corr_df$post_corrs) - FisherZ(corr_df$pre_corrs) # write to file fn &lt;- file.path(out_dir, sprintf(&quot;%s_%s_%s_pattern_similarity_change.txt&quot;, i_sub, &quot;same-day_searchlight&quot;, i_thresh)) write.table(corr_df, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) } } Combine behavioral and fMRI data for RSA To create input for the RSA the next step is to combine the similarity change data with the behavioral data, so that we can do meaningful analyses. First the behavioral data is loaded and brought into the same pair-wise format as the similarity change data. Both datasets are combined for each subject and then written to disk. This part is based on the corresponding script from the main ROI analyses. in_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;pattern_similarity_change&quot;, &quot;same-day_searchlight&quot;) out_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;,&quot;rsa&quot;,&quot;data_for_rsa&quot;) if (!dir.exists(out_dir)){dir.create(out_dir, recursive = TRUE)} for (i_thresh in threshs){ for (i_sub in subjects){ # load the behavioral data fn &lt;- file.path(dirs$timeline_dat_dir, sprintf(&quot;%s_behavior_tbl_timeline.txt&quot;, i_sub)) col_types_list &lt;- cols_only(sub_id = col_character(), day = col_factor(), event = col_integer(), pic = col_integer(), virtual_time = col_double(), real_time = col_double(), memory_time = col_double(), memory_order = col_double(), sorted_day = col_integer()) beh_dat &lt;- read_csv(fn, col_types = col_types_list) # sort behavioral data according to picture identity beh_dat_ordered &lt;- beh_dat[order(beh_dat$pic),] # find the order of comparisons pairs &lt;- which(upper.tri(matrix(nrow = 20, ncol = 20)), arr.ind=TRUE) # extract the data for the first and second picture in each pair from the behavioral data pic1_dat &lt;- beh_dat_ordered[pairs[,1],] pic2_dat &lt;- beh_dat_ordered[pairs[,2],] colnames(pic1_dat) &lt;- paste0(colnames(pic1_dat), &quot;1&quot;) colnames(pic2_dat) &lt;- paste0(colnames(pic2_dat), &quot;2&quot;) # combine the two tibbles pair_dat &lt;- cbind(pic1_dat, pic2_dat) # reorder the tibble columns to make a bit more sense pair_dat &lt;- pair_dat %&gt;% select(sub_id1, day1, day2, pic1, pic2, event1, event2, virtual_time1, virtual_time2, real_time1, real_time2, memory_order1, memory_order2, memory_time1, memory_time2, sorted_day1, sorted_day2) %&gt;% rename(sub_id = sub_id1) rsa_dat &lt;- tibble() # load the pattern similarity change data for this ROI fn &lt;- file.path(in_dir, sprintf(&quot;%s_%s_%s_pattern_similarity_change.txt&quot;, i_sub, &quot;same-day_searchlight&quot;, i_thresh)) ps_change_dat &lt;- read.csv(fn) # make sure files have the same order assertthat::are_equal(c(pair_dat$pic1, pair_dat$pic2), c(ps_change_dat$pic1, ps_change_dat$pic2)) # add column with ROI name ps_change_dat &lt;- add_column(ps_change_dat, roi = paste0(&quot;same-day_searchlight_&quot;, i_thresh)) # collect the data from this ROI and merge into long data frame roi_dat &lt;- cbind(pair_dat, ps_change_dat[,3:6]) rsa_dat &lt;- rbind(rsa_dat, roi_dat) # write to file fn &lt;- file.path(dirs$rsa_dat_dir, sprintf(&quot;%s_data_for_rsa_same-day_searchlight_%s.txt&quot;, i_sub, i_thresh)) write.table(rsa_dat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) } } Finally, the datasets are combined across subjects. # set up a dataframe to collect the data rsa_dat = tibble() for (i_thresh in threshs){ for (i_sub in subjects){ # load data from CSV fn &lt;- file.path(out_dir, sprintf(&quot;%s_data_for_rsa_same-day_searchlight_%s.txt&quot;,i_sub, i_thresh)) col_types_list &lt;- cols_only( sub_id = col_character(), day1 = col_integer(), day2 = col_integer(), event1 = col_integer(), event2 = col_integer(), pic1 = col_integer(), pic2 = col_integer(), virtual_time1 = col_double(), virtual_time2 = col_double(), real_time1 = col_double(), real_time2 = col_double(), memory_time1 = col_double(), memory_time2 = col_double(), memory_order1 = col_double(), memory_order2 = col_double(), sorted_day1 = col_integer(), sorted_day2 = col_integer(), pre_corrs = col_double(), post_corrs = col_double(), ps_change = col_double(), roi = col_factor() ) sub_dat &lt;- as_tibble(read_csv(fn, col_types = col_types_list)) # append to table with data from all subjects rsa_dat &lt;- bind_rows(sub_dat, rsa_dat) } # sort the data rsa_dat &lt;- rsa_dat[with(rsa_dat, order(sub_id, day1, day2, event1, event2)),] # write to file fn &lt;- file.path(dirs$data4analysis, sprintf(&quot;rsa_data_in_same-seq_searchlight_peak_%s.txt&quot;, i_thresh)) write.table(rsa_dat, fn, append = FALSE, sep = &quot;,&quot;, dec = &quot;.&quot;, row.names = FALSE, col.names = TRUE) } 10.2 RSA We use permutation-based linear model to analyze the data with a summary statistics approach. # define function that calculates z-value for permutation lm_perm_jb2 &lt;- function(in_dat = df, lm_formula = lm_formula, nsim = 1000){ # run the model for original data and store observed t-values lm_fit &lt;- lm(formula = lm_formula, data=in_dat) obs &lt;- coef(summary(lm_fit))[,&quot;t value&quot;] # extract the dependent variable from the formula dv &lt;- str_extract(lm_formula, &quot;[^~]+&quot;) dv &lt;- str_replace_all(dv, fixed(&quot; &quot;), &quot;&quot;) if(!(dv %in% colnames(in_dat))){stop(&quot;Cannot find dependent variable in input data&quot;);} # initialize df for permutation data_perm &lt;- in_dat # set aside space for results res &lt;- matrix(nrow = nsim, ncol = length(obs)) for (i in 1:nsim) { # scramble response value perm &lt;- sample(nrow(in_dat)) dv_dat &lt;- in_dat[dv] data_perm[dv] &lt;- dv_dat[perm,] # compute linear model and store the t-value of predictor lm_fit_perm &lt;- lm(formula = lm_formula, data=data_perm) res[i,] &lt;- coef(summary(lm_fit_perm))[,&quot;t value&quot;] } # append the observed value to the list of results res &lt;- rbind(res,obs) # calculate p-value for each coefficient and transform to z p &lt;- rep(0,length(obs)) z &lt;- rep(0,length(obs)) for (i_coef in 1:length(obs)){ p[i_coef] &lt;- sum(res[,i_coef] &lt;= obs[i_coef])/nrow(res) z[i_coef] &lt;- -1*qnorm(1-p[i_coef]) } return(z) } Now run the linear model for each participant and do group-level stats. for (i_thresh in threshs){ # load the data col_types_list &lt;- cols_only( sub_id = col_character(), day1 = col_integer(), day2 = col_integer(), virtual_time1 = col_double(), virtual_time2 = col_double(), ps_change = col_double(), roi = col_factor()) fn &lt;- file.path(dirs$data4analysis, sprintf(&quot;rsa_data_in_same-seq_searchlight_peak_%s.txt&quot;, i_thresh)) rsa_dat_within_cluster &lt;- as_tibble(read_csv(fn, col_types = col_types_list)) ## PREAPARE RSA # create predictors for RSA rsa_dat_within_cluster &lt;- rsa_dat_within_cluster %&gt;% mutate( # pair of events from say or different day (dv = deviation code) same_day = day1 == day2, same_day_dv = plyr::mapvalues(same_day, from = c(FALSE, TRUE), to = c(-1, 1)), # absolute difference in time metrics vir_time_diff = abs(virtual_time1 - virtual_time2)) %&gt;% # z-score the time metric predictors (within each subject) group_by(sub_id) %&gt;% mutate_at(c(&quot;vir_time_diff&quot;), scale, scale = TRUE) %&gt;% ungroup() ##### First-level RSA {-} set.seed(179) # set seed for reproducibility # do RSA using linear model and calculate z-score for model fit based on permutations rsa_fit_within_cluster2 &lt;- rsa_dat_within_cluster %&gt;% group_by(sub_id, same_day) %&gt;% # run the linear model do(z = lm_perm_jb2(in_dat = ., lm_formula = &quot;ps_change ~ vir_time_diff&quot;, nsim = n_perm)) %&gt;% mutate(z = list(setNames(z, c(&quot;z_intercept&quot;, &quot;z_virtual_time&quot;)))) %&gt;% unnest_wider(z) # add group column used for plotting rsa_fit_within_cluster2$group &lt;- factor(1) # add a factor with character labels for within/across days and one to later color control in facets rsa_fit_within_cluster2 &lt;- rsa_fit_within_cluster2 %&gt;% mutate(same_day_char = plyr::mapvalues(same_day, from = c(0, 1), to = c(&quot;across days&quot;, &quot;within days&quot;), warn_missing = FALSE), same_day_char = factor(same_day_char, levels = c(&quot;within days&quot;, &quot;across days&quot;))) # run a group-level t-test on the different sequence RSA fits in the within-searchlight cluster stats &lt;- rsa_fit_within_cluster2 %&gt;% filter(same_day == FALSE) %&gt;% select(z_virtual_time) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # Cohen&#39;s d with Hedges&#39; correction for one sample using non-central t-distribution for CI d&lt;-cohen.d(d=(rsa_fit_within_cluster2 %&gt;% filter(same_day == FALSE))$z_virtual_time, f=NA, paired=TRUE, hedges.correction=TRUE, noncentral=TRUE) stats$d &lt;- d$estimate stats$dCI_low &lt;- d$conf.int[[1]] stats$dCI_high &lt;- d$conf.int[[2]] # print results huxtable(stats) %&gt;% theme_article() } Summary Statistics: t-test against 0 for virtual time across sequences in within-sequence searchlight peak (thresholded at p&lt;0.001) t27=-1.98, p=0.056, d=-0.36, 95% CI [-0.77, 0.01] Compare the results using the two different thresholds with a permutation-based t-test on the difference. # add column for cluster rsa_fit_within_cluster2 &lt;- rsa_fit_within_cluster2 %&gt;% mutate(thresh = &quot;v2&quot;) rsa_fit_within_cluster &lt;- rsa_fit_within_cluster %&gt;% mutate(thresh = &quot;v1&quot;) # merge data frames and select across-sequence values rsa_fit_within_cluster_comp &lt;- rbind(rsa_fit_within_cluster, rsa_fit_within_cluster2) %&gt;% filter(same_day == FALSE) # run permutation-based t-test on the difference between the two sets of results stats &lt;- rsa_fit_within_cluster_comp %&gt;% select(-z_intercept) %&gt;% pivot_wider(names_from = thresh, values_from = z_virtual_time) %&gt;% mutate(thresh_diff = v1 - v2) %&gt;% select(thresh_diff) %&gt;% paired_t_perm_jb (., n_perm = n_perm) # print results huxtable(stats) %&gt;% theme_article() estimatestatisticp.valuep_permparameterconf.lowconf.highmethodalternative -0.088-0.9470.3520.33827-0.2790.103One Sample t-testtwo.sided Summary Statistics: t-test between RSA results in searchlight cluster defined on different thresholds (p&lt;0.01 vs. p&lt;0.001) t27=-0.95, p=0.338 "],
["univariate-glm-relative-time.html", "11 Univariate GLM: Relative time 11.1 Copy FEAT-folder from Preprocessing to First-Level GLM directory 11.2 Prepare GLM 11.3 Run 1st-level GLM on HTCondor 11.4 Group-level", " 11 Univariate GLM: Relative time We want to run a GLM with the relative time of other events as a parametric modulator for activity evoked by picture presentations in the picture viewing task. We will use the already-preprocessed data and use some FSL tools to implement the GLM 11.1 Copy FEAT-folder from Preprocessing to First-Level GLM directory First, set up the folder we are working in. Make sure it’s empty when we start and then copy the preprocessed data there because we don’t need to repeat the preprocessing. dirs$level1glm_dir &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;processed&quot;, &quot;glm_rel_time&quot; ) # if the directory is not empty delete everything for a fresh start if(is_empty(dir(path = dirs$level1glm_dir)) | !length(dir(path = dirs$level1glm_dir)) == 0) { # delete the entire directory and create it again # unlink(dirs$level1glm_dir, recursive = TRUE) dir.create(dirs$level1glm_dir) } # copy preprocessing FEATs to directory (~45 mins) if(length(dir(path = dirs$level1glm_dir)) == 0) { start_time &lt;- Sys.time() print(&#39;Copying preprocessed data to first level folder&#39;) for (i_sub in subjects){ # copy the preprocessed data dir.create(file.path(dirs$level1glm_dir, sprintf(&quot;VIRTEM_P%s&quot;, i_sub)), recursive = TRUE) to_copy &lt;- list.dirs(file.path(dirs$feat_dir, sprintf(&quot;VIRTEM_P%s&quot;, i_sub)), recursive = FALSE) to_copy &lt;- grep(&quot;RSA_02&quot;, to_copy, value = TRUE) file.copy(from = to_copy, to = file.path(dirs$level1glm_dir, sprintf(&quot;VIRTEM_P%s&quot;, i_sub)), recursive = TRUE) } end_time &lt;- Sys.time() end_time-start_time } 11.2 Prepare GLM Compute relative time First, we need to calculate, for each event, the mean relative time of events at the same sequence position in the other sequences. As a difference to the main behavioral analyses, we use the absolute relative time of other events. # load data from CSV fname &lt;- file.path(dirs$data4analysis, &quot;behavioral_data.txt&quot;) col_types_list &lt;- cols_only( sub_id = col_factor(), day = col_factor(), event = col_factor(), pic = col_integer(), virtual_time = col_double(), real_time = col_double(), memory_time = col_double(), memory_order = col_double(), sorted_day = col_integer() ) beh_data &lt;- as_tibble(read_csv(fname, col_types = col_types_list)) head(beh_data) # quantify the deviation in virtual time for each event relative to other events # at the same sequence position beh_data$rel_time_other_events&lt;-NA for (i_day in 1:n_days){ for (i_event in 1:n_events_day){ # find the events at this sequence position from all four sequences curr_events &lt;- beh_data[beh_data$event == i_event,] # find the average virtual time of the other events at this sequence position avg_vir_time_other_events &lt;- curr_events %&gt;% filter(day != i_day) %&gt;% summarise(avg_vir_time = mean(virtual_time)) %&gt;% select(avg_vir_time) %&gt;% as.numeric(.) # for the given event, store the relative time of the other events, # as the absolute difference in virtual time (positive values mean other events happen later) event_idx &lt;- beh_data$day==i_day &amp; beh_data$event==i_event beh_data$rel_time_other_events[event_idx] &lt;- abs(avg_vir_time_other_events - beh_data$virtual_time[event_idx]) } } # center the relative time beh_data %&gt;% mutate(rel_time_other_events = scale(rel_time_other_events, center = TRUE, scale = FALSE)) Write custom EV files for FSL FEAT As a first step, let’s write the EVs to file. For that we load the log from the post-learning picture viewing, split it into the ten blocks of each of those runs, and reference the onset times of the event image presentations to the first volume of the scanning runs. Then we write out the custom EV files: picture presentations relative time parametric modulator for picture presentations catch trial picture presentations #Write custom EV files for FSL FEAT for (i_sub in subjects){ # define the directory we want to use feat_in_dir &lt;- file.path(dirs$level1glm_dir, &quot;feat_evs&quot;, i_sub) if (!dir.exists(feat_in_dir)){dir.create(feat_in_dir, recursive = TRUE)} for (i_run in 2:n_runs){ # load the logfile for this run (pre or post) log_fn &lt;- file.path(dirs$pvt_log_dir, sprintf(&#39;P%s_%svirtem.txt&#39;, i_sub, runs[i_run])) log &lt;- read.table(log_fn) colnames(log) &lt;- c(&quot;pic&quot;, &quot;fix_start&quot;, &quot;pic_start&quot;, &quot;volume&quot;, &quot;response&quot;, &quot;RT&quot;, &quot;trial_end&quot;) # split the log file into the 10 blocks log_split &lt;- split(log, rep(1:10, each = 21)) # reference volume numbers to the first volume of that block vol_block &lt;- lapply(log_split, function(x){x$volume - x$volume[1]}) log_split &lt;- mapply(cbind, log_split, &quot;vol_block&quot;=vol_block, SIMPLIFY=FALSE) # reference picture starts to 0 for each block pic_start_block &lt;- lapply(log_split, function(x){((x$vol_block)*2270)/1000}) log_split &lt;- mapply(cbind, log_split, &quot;pic_start_block&quot;=pic_start_block, SIMPLIFY=FALSE) for (i_block in 1:10){ # write picture onset EV file fn &lt;- file.path(feat_in_dir, sprintf(&quot;%s_%s_%02d_%s.txt&quot;, i_sub, runs[i_run], i_block, &quot;pic_on&quot;)) log_split[[i_block]] %&gt;% filter(pic != 21) %&gt;% select(pic_start_block) %&gt;% mutate(duration = 2.5, weight = 1) %&gt;% write.table(., file = fn, row.names = FALSE, col.names = FALSE, sep = &quot;\\t&quot;) # write parametric regressor for picture onsets to EV file based on relative time of other events # get the relative times for pictures (sorted 1-20) rel_times &lt;- beh_data %&gt;% filter(sub_id == as.numeric(i_sub)) %&gt;% arrange(pic) %&gt;% pull(rel_time_other_events) fn &lt;- file.path(feat_in_dir, sprintf(&quot;%s_%s_%02d_%s.txt&quot;, i_sub, runs[i_run], i_block, &quot;rel_time&quot;)) log_split[[i_block]] %&gt;% filter(pic != 21) %&gt;% arrange(pic) %&gt;% mutate(rel_time = rel_times) %&gt;% arrange(pic_start_block) %&gt;% select(pic_start_block, rel_time) %&gt;% mutate(duration = 2.5) %&gt;% relocate(duration, .before = &quot;rel_time&quot;) %&gt;% write.table(., file = fn, row.names = FALSE, col.names = FALSE, sep = &quot;\\t&quot;) # write catch picture onset EV file fn &lt;- file.path(feat_in_dir, sprintf(&quot;%s_%s_%02d_%s.txt&quot;, i_sub, runs[i_run], i_block, &quot;catch_pic_on&quot;)) log_split[[i_block]] %&gt;% filter(pic == 21) %&gt;% select(pic_start_block) %&gt;% mutate(duration = 2.5, weight = 1) %&gt;% write.table(., file = fn, row.names = FALSE, col.names = FALSE, sep = &quot;\\t&quot;) # write button press EV file fn &lt;- file.path(feat_in_dir, sprintf(&quot;%s_%s_%02d_%s.txt&quot;, i_sub, runs[i_run], i_block, &quot;button_press&quot;)) log_split[[i_block]] %&gt;% filter(response == 1) %&gt;% mutate(press = pic_start_block + RT/1000, duration = 0, weight = 1) %&gt;% select(press, duration, weight) %&gt;% write.table(., file = fn, row.names = FALSE, col.names = FALSE, sep = &quot;\\t&quot;) } } } GLM Design files Now that we have the relevant EV files, let’s set up the GLM using the tools from FSL FEAT. We create a configuration file for each block of each participant. dsgn &lt;- &#39; # FEAT version number set fmri(version) 6.00 # Are we in MELODIC? set fmri(inmelodic) 0 # Analysis level # 1 : First-level analysis # 2 : Higher-level analysis set fmri(level) 1 # Which stages to run # 0 : No first-level analysis (registration and/or group stats only) # 7 : Full first-level analysis # 1 : Pre-processing # 2 : Statistics set fmri(analysis) 2 # Use relative filenames set fmri(relative_yn) 0 # Balloon help set fmri(help_yn) 1 # Run Featwatcher set fmri(featwatcher_yn) 0 # Cleanup first-level standard-space images set fmri(sscleanup_yn) 0 # Output directory set fmri(outputdir) &quot;&quot; # TR(s) set fmri(tr) 2.270000 # Total volumes set fmri(npts) @n_vols@ # Delete volumes set fmri(ndelete) 0 # Perfusion tag/control order set fmri(tagfirst) 1 # Number of first-level analyses set fmri(multiple) 1 # Higher-level input type # 1 : Inputs are lower-level FEAT directories # 2 : Inputs are cope images from FEAT directories set fmri(inputtype) 1 # Carry out pre-stats processing? set fmri(filtering_yn) 0 # Brain/background threshold, % set fmri(brain_thresh) 10 # Critical z for design efficiency calculation set fmri(critical_z) 5.3 # Noise level set fmri(noise) 0.66 # Noise AR(1) set fmri(noisear) 0.34 # Motion correction # 0 : None # 1 : MCFLIRT set fmri(mc) 1 # Spin-history (currently obsolete) set fmri(sh_yn) 0 # B0 fieldmap unwarping? set fmri(regunwarp_yn) 0 # EPI dwell time (ms) set fmri(dwell) 0.235 # EPI TE (ms) set fmri(te) 24 # % Signal loss threshold set fmri(signallossthresh) 10 # Unwarp direction set fmri(unwarp_dir) y- # Slice timing correction # 0 : None # 1 : Regular up (0, 1, 2, 3, ...) # 2 : Regular down # 3 : Use slice order file # 4 : Use slice timings file # 5 : Interleaved (0, 2, 4 ... 1, 3, 5 ... ) set fmri(st) 0 # Slice timings file set fmri(st_file) &quot;&quot; # BET brain extraction set fmri(bet_yn) 1 # Spatial smoothing FWHM (mm) set fmri(smooth) 0 # Intensity normalization set fmri(norm_yn) 0 # Perfusion subtraction set fmri(perfsub_yn) 0 # Highpass temporal filtering set fmri(temphp_yn) 1 # Lowpass temporal filtering set fmri(templp_yn) 0 # MELODIC ICA data exploration set fmri(melodic_yn) 0 # Carry out main stats? set fmri(stats_yn) 1 # Carry out prewhitening? set fmri(prewhiten_yn) 1 # Add motion parameters to model # 0 : No # 1 : Yes set fmri(motionevs) 0 set fmri(motionevsbeta) &quot;&quot; set fmri(scriptevsbeta) &quot;&quot; # Robust outlier detection in FLAME? set fmri(robust_yn) 0 # Higher-level modelling # 3 : Fixed effects # 0 : Mixed Effects: Simple OLS # 2 : Mixed Effects: FLAME 1 # 1 : Mixed Effects: FLAME 1+2 set fmri(mixed_yn) 2 # Number of EVs set fmri(evs_orig) 3 set fmri(evs_real) 6 set fmri(evs_vox) 0 # Number of contrasts set fmri(ncon_orig) 1 set fmri(ncon_real) 1 # Number of F-tests set fmri(nftests_orig) 0 set fmri(nftests_real) 0 # Add constant column to design matrix? (obsolete) set fmri(constcol) 0 # Carry out post-stats steps? set fmri(poststats_yn) 0 # Pre-threshold masking? set fmri(threshmask) &quot;&quot; # Thresholding # 0 : None # 1 : Uncorrected # 2 : Voxel # 3 : Cluster set fmri(thresh) 3 # P threshold set fmri(prob_thresh) 0.05 # Z threshold set fmri(z_thresh) 2.3 # Z min/max for colour rendering # 0 : Use actual Z min/max # 1 : Use preset Z min/max set fmri(zdisplay) 0 # Z min in colour rendering set fmri(zmin) 2 # Z max in colour rendering set fmri(zmax) 8 # Colour rendering type # 0 : Solid blobs # 1 : Transparent blobs set fmri(rendertype) 1 # Background image for higher-level stats overlays # 1 : Mean highres # 2 : First highres # 3 : Mean functional # 4 : First functional # 5 : Standard space template set fmri(bgimage) 1 # Create time series plots set fmri(tsplot_yn) 1 # Registration to initial structural set fmri(reginitial_highres_yn) 0 # Search space for registration to initial structural # 0 : No search # 90 : Normal search # 180 : Full search set fmri(reginitial_highres_search) 90 # Degrees of Freedom for registration to initial structural set fmri(reginitial_highres_dof) 6 # Registration to main structural set fmri(reghighres_yn) 0 # Search space for registration to main structural # 0 : No search # 90 : Normal search # 180 : Full search set fmri(reghighres_search) 90 # Degrees of Freedom for registration to main structural set fmri(reghighres_dof) BBR # Registration to standard image? set fmri(regstandard_yn) 1 # Use alternate reference images? set fmri(alternateReference_yn) 0 # Standard image set fmri(regstandard) &quot;/afs/cbs/software/fsl/6.0.3/ubuntu-bionic-amd64/data/standard/MNI152_T1_1mm_brain&quot; # Search space for registration to standard space # 0 : No search # 90 : Normal search # 180 : Full search set fmri(regstandard_search) 90 # Degrees of Freedom for registration to standard space set fmri(regstandard_dof) 12 # Do nonlinear registration from structural to standard space? set fmri(regstandard_nonlinear_yn) 0 # Control nonlinear warp field resolution set fmri(regstandard_nonlinear_warpres) 10 # High pass filter cutoff set fmri(paradigm_hp) 100 # Total voxels set fmri(totalVoxels) 366912000 # Number of lower-level copes feeding into higher-level analysis set fmri(ncopeinputs) 0 # 4D AVW data or FEAT directory (1) set feat_files(1) &quot;@level1glm_dir@/VIRTEM_P@sub_ID@/RSA_@run_no@_Block@block_no@.feat&quot; # Add confound EVs text file set fmri(confoundevs) 1 # Confound EVs text file for analysis 1 set confoundev_files(1) &quot;@level1glm_dir@/VIRTEM_P@sub_ID@/RSA_@run_no@_Block@block_no@.feat/mc/prefiltered_func_data_mcf.par&quot; # Subjects structural image for analysis 1 set highres_files(1) &quot;/home/memspa/lordeu/Projects/virtem/mri/raw/Nifti/VIRTEM_P@sub_ID@/VIRTEM_P@sub_ID@_t1_co_brain&quot; # EV 1 title set fmri(evtitle1) &quot;pic_onset&quot; # Basic waveform shape (EV 1) # 0 : Square # 1 : Sinusoid # 2 : Custom (1 entry per volume) # 3 : Custom (3 column format) # 4 : Interaction # 10 : Empty (all zeros) set fmri(shape1) 3 # Convolution (EV 1) # 0 : None # 1 : Gaussian # 2 : Gamma # 3 : Double-Gamma HRF # 4 : Gamma basis functions # 5 : Sine basis functions # 6 : FIR basis functions set fmri(convolve1) 2 # Convolve phase (EV 1) set fmri(convolve_phase1) 0 # Apply temporal filtering (EV 1) set fmri(tempfilt_yn1) 1 # Add temporal derivative (EV 1) set fmri(deriv_yn1) 1 # Custom EV file (EV 1) set fmri(custom1) &quot;@level1glm_dir@/feat_evs/@sub_ID@/@sub_ID@_@run_name@_@block_no@_pic_on.txt&quot; # Gamma sigma (EV 1) set fmri(gammasigma1) 3 # Gamma delay (EV 1) set fmri(gammadelay1) 6 # Orthogonalise EV 1 wrt EV 0 set fmri(ortho1.0) 0 # Orthogonalise EV 1 wrt EV 1 set fmri(ortho1.1) 0 # Orthogonalise EV 1 wrt EV 2 set fmri(ortho1.2) 0 # Orthogonalise EV 1 wrt EV 3 set fmri(ortho1.3) 0 # EV 2 title set fmri(evtitle2) &quot;rel_time_other&quot; # Basic waveform shape (EV 2) # 0 : Square # 1 : Sinusoid # 2 : Custom (1 entry per volume) # 3 : Custom (3 column format) # 4 : Interaction # 10 : Empty (all zeros) set fmri(shape2) 3 # Convolution (EV 2) # 0 : None # 1 : Gaussian # 2 : Gamma # 3 : Double-Gamma HRF # 4 : Gamma basis functions # 5 : Sine basis functions # 6 : FIR basis functions set fmri(convolve2) 2 # Convolve phase (EV 2) set fmri(convolve_phase2) 0 # Apply temporal filtering (EV 2) set fmri(tempfilt_yn2) 1 # Add temporal derivative (EV 2) set fmri(deriv_yn2) 1 # Custom EV file (EV 2) set fmri(custom2) &quot;@level1glm_dir@/feat_evs/@sub_ID@/@sub_ID@_@run_name@_@block_no@_rel_time.txt&quot; # Gamma sigma (EV 2) set fmri(gammasigma2) 3 # Gamma delay (EV 2) set fmri(gammadelay2) 6 # Orthogonalise EV 2 wrt EV 0 set fmri(ortho2.0) 0 # Orthogonalise EV 2 wrt EV 1 set fmri(ortho2.1) 0 # Orthogonalise EV 2 wrt EV 2 set fmri(ortho2.2) 0 # Orthogonalise EV 2 wrt EV 3 set fmri(ortho2.3) 0 # EV 3 title set fmri(evtitle3) &quot;catch_pic_on&quot; # Basic waveform shape (EV 3) # 0 : Square # 1 : Sinusoid # 2 : Custom (1 entry per volume) # 3 : Custom (3 column format) # 4 : Interaction # 10 : Empty (all zeros) set fmri(shape3) 3 # Convolution (EV 3) # 0 : None # 1 : Gaussian # 2 : Gamma # 3 : Double-Gamma HRF # 4 : Gamma basis functions # 5 : Sine basis functions # 6 : FIR basis functions set fmri(convolve3) 2 # Convolve phase (EV 3) set fmri(convolve_phase3) 0 # Apply temporal filtering (EV 3) set fmri(tempfilt_yn3) 1 # Add temporal derivative (EV 3) set fmri(deriv_yn3) 1 # Custom EV file (EV 3) set fmri(custom3) &quot;@level1glm_dir@/feat_evs/@sub_ID@/@sub_ID@_@run_name@_@block_no@_catch_pic_on.txt&quot; # Gamma sigma (EV 3) set fmri(gammasigma3) 3 # Gamma delay (EV 3) set fmri(gammadelay3) 6 # Orthogonalise EV 3 wrt EV 0 set fmri(ortho3.0) 0 # Orthogonalise EV 3 wrt EV 1 set fmri(ortho3.1) 0 # Orthogonalise EV 3 wrt EV 2 set fmri(ortho3.2) 0 # Orthogonalise EV 3 wrt EV 3 set fmri(ortho3.3) 0 # Contrast &amp; F-tests mode # real : control real EVs # orig : control original EVs set fmri(con_mode_old) orig set fmri(con_mode) orig # Display images for contrast_real 1 set fmri(conpic_real.1) 1 # Title for contrast_real 1 set fmri(conname_real.1) &quot;&quot; # Real contrast_real vector 1 element 1 set fmri(con_real1.1) 0.0 # Real contrast_real vector 1 element 2 set fmri(con_real1.2) 0 # Real contrast_real vector 1 element 3 set fmri(con_real1.3) 1.0 # Real contrast_real vector 1 element 4 set fmri(con_real1.4) 0 # Real contrast_real vector 1 element 5 set fmri(con_real1.5) 0 # Real contrast_real vector 1 element 6 set fmri(con_real1.6) 0 # Display images for contrast_orig 1 set fmri(conpic_orig.1) 1 # Title for contrast_orig 1 set fmri(conname_orig.1) &quot;&quot; # Real contrast_orig vector 1 element 1 set fmri(con_orig1.1) 0.0 # Real contrast_orig vector 1 element 2 set fmri(con_orig1.2) 1.0 # Real contrast_orig vector 1 element 3 set fmri(con_orig1.3) 0 # Contrast masking - use &gt;0 instead of thresholding? set fmri(conmask_zerothresh_yn) 0 # Do contrast masking at all? set fmri(conmask1_1) 0 ########################################################## # Now options that do not appear in the GUI # Alternative (to BETting) mask image set fmri(alternative_mask) &quot;&quot; # Initial structural space registration initialisation transform set fmri(init_initial_highres) &quot;&quot; # Structural space registration initialisation transform set fmri(init_highres) &quot;&quot; # Standard space registration initialisation transform set fmri(init_standard) &quot;&quot; # For full FEAT analysis: overwrite existing .feat output dir? set fmri(overwrite_yn) 0 &#39; # folder for fsf files and copy template there fsf_dir &lt;- file.path(dirs$level1glm_dir, &quot;fsf&quot;) if(!dir.exists(fsf_dir)){dir.create(fsf_dir)} #file.copy(from = here(&quot;virtem_code&quot;, &quot;unused_do_not_delete&quot;, &quot;feat_level1glm_relTimeOther.fsf&quot;), # to = file.path(fsf_dir, &quot;feat_level1glm_template.fsf&quot;)) # set up data frame df &lt;- tibble(sub_id = subjects %&gt;% rep(each = n_blocks)) df$run_name &lt;- &quot;post&quot; #runs %&gt;% rep(each = 10) %&gt;% rep(n_subs) df$run_no &lt;- 2 #c(1,2) %&gt;% rep(each = 10) %&gt;% rep(n_subs) df$block &lt;- c(1:10) %&gt;% rep(n_subs) # c(1:10) %&gt;% rep(n_runs) %&gt;% rep(n_subs) df$fn &lt;- file.path(fsf_dir, sprintf(&quot;%s_%s_%02d.fsf&quot;, df$sub_id, df$run_name, df$block)) df$in_dir &lt;- dirs$level1glm_dir df$filtered_func &lt;- file.path(dirs$level1glm_dir, sprintf(&quot;VIRTEM_P%s&quot;, df$sub_id), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, df$run_no, df$block), &quot;filtered_func_data.nii.gz&quot;) df$fsf_dir &lt;- fsf_dir # define function to substitute placeholders replace_fsf &lt;- function(df = df[1,], fsf_lines = dsgn){ # read in the template file #fn &lt;- file.path(df$fsf_dir, &quot;feat_level1glm_template.fsf&quot;) #fsf_lines &lt;- read_delim(fn, delim = &quot;\\r&quot;, # col_names = FALSE, col_types = cols(col_character())) # replace the path fsf_lines &lt;- mgsub(pattern = &quot;@level1glm_dir@&quot;, replacement = df$in_dir, string = fsf_lines) # replace the subject ID fsf_lines &lt;- mgsub(pattern = &quot;@sub_ID@&quot;, replacement = df$sub_id, string = fsf_lines) # replace character run name fsf_lines &lt;- mgsub(pattern = &quot;@run_name@&quot;, replacement = df$run_name, string = fsf_lines) fsf_lines &lt;- mgsub(pattern = &quot;@run_no@&quot;, replacement = str_pad(as.character(df$run_no), width = 2, side = &quot;left&quot;, pad = &quot;0&quot;), string = fsf_lines) # replace block number fsf_lines &lt;- mgsub(pattern = &quot;@block_no@&quot;, replacement = str_pad(as.character(df$block), width = 2, side = &quot;left&quot;, pad = &quot;0&quot;), string = fsf_lines) # replace the number of volumes in 4D-nifti based on Header info # min throws a warning because min is called with no data, but much faster nii &lt;- readNIfTI2(df$filtered_func, read_data = FALSE) fsf_lines &lt;- mgsub(pattern = &quot;@n_vols@&quot;, replacement = nii@dim_[5], string = fsf_lines) # write to file con &lt;- file(df$fn) writeLines(fsf_lines, con) close(con) } # create FSF info for each row of the data frame (i.e. each block of each subject) for (i in 1:nrow(df)){replace_fsf(df = df[i,], fsf_lines = dsgn)} Prepare confound EV We want to include the motion parameters from FEAT as confound regressors. Thus, we copy them from the mc subfolder of the FEAT directory and save them using just one space as a delimiter so they can be used with feat_model. for (i in 1:nrow(df)){ mcpar_fn &lt;- file.path(df$in_dir[i], sprintf(&quot;VIRTEM_P%s&quot;,df$sub_id[i]), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, df$run_no[i], df$block[i]), &quot;mc&quot;, &quot;prefiltered_func_data_mcf.par&quot;) conf_ev_fn &lt;- file.path(df$in_dir[i], sprintf(&quot;VIRTEM_P%s&quot;,df$sub_id[i]), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, df$run_no[i], df$block[i]), &quot;mcf.txt&quot;) # paste to the correct file using only one space as delimiter system(sprintf(&quot;paste -d &#39; &#39; %s &gt; %s&quot;, mcpar_fn, conf_ev_fn)) } Create design matrix Next, we call feat_model to create the design matrix and contrast file needed to run the GLM from the fsf-configuration. for (i in 1:nrow(df)){ fsf_fn &lt;- file.path(df$in_dir[i], sprintf(&quot;VIRTEM_P%s&quot;,df$sub_id[i]), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, df$run_no[i], df$block[i]), &quot;design.fsf&quot;) conf_ev_fn &lt;- file.path(df$in_dir[i], sprintf(&quot;VIRTEM_P%s&quot;,df$sub_id[i]), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, df$run_no[i], df$block[i]), &quot;mcf.txt&quot;) # copy fsf to Feat folder file.copy(from = df$fn[i], to = file.path(df$in_dir[i], sprintf(&quot;VIRTEM_P%s&quot;,df$sub_id[i]), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, df$run_no[i], df$block[i]), &quot;design.fsf&quot;), overwrite = TRUE) # run feat_model design to create files for film_gls system(sprintf(&quot;feat_model %s %s&quot;, tools::file_path_sans_ext(fsf_fn), conf_ev_fn)) } 11.3 Run 1st-level GLM on HTCondor Finally, we can run the GLM. For that, we create a small shell script that has the film_gls command and that deletes the residual file after finishing. This is used as the executable for a task list that specifies the specific files to use for each block of each participant. # folder for condor output htc_dir &lt;- here(&quot;htc_logs&quot;, &quot;rel_time_glm_level1&quot;) if(!exists(htc_dir)){dir.create(htc_dir, recursive = TRUE)} # write shell script that runs the GLM and removes the residuals afterwards fn_sh &lt;- file.path(htc_dir, &quot;rel_time_glm_level1&quot;) con &lt;- file(fn_sh) open(con, &quot;w&quot;) writeLines(&quot;#!/bin/bash /usr/share/fsl/5.0/bin/film_gls --in=$1 --rn=$2 --pd=$3 --thr=1000.0 --sa --ms=5 --con=$4 rm $5 &quot;, con) close(con) system(sprintf(&quot;chmod +x %s&quot;,fn_sh)) # write the submit file fn &lt;- file.path(htc_dir, &quot;rel_time_glm_level1_tasklist.txt&quot;) con &lt;- file(fn) open(con, &quot;w&quot;) writeLines(c( &quot;universe = vanilla&quot;, sprintf(&quot;executable = %s&quot;, fn_sh), &quot;request_memory = 5000&quot;, &quot;request_cpus = 1&quot;, &quot;getenv = True&quot;, &quot;notification = error&quot; ),con) c=0 for (i in 1:nrow(df)){ stat_pth &lt;- file.path(df$in_dir[i], sprintf(&quot;VIRTEM_P%s&quot;,df$sub_id[i]), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, df$run_no[i], df$block[i]), &quot;stats&quot;) chk_fn &lt;- file.path(stat_pth, &quot;zstat1.nii.gz&quot;) if (!file.exists(chk_fn)){ if(dir.exists(stat_pth)){unlink(stat_pth, recursive = TRUE)} writeLines(c( sprintf(&quot;arguments = %s %s %s %s %s&quot;, df$filtered_func[i], stat_pth, file.path(df$in_dir[i], sprintf(&quot;VIRTEM_P%s&quot;,df$sub_id[i]), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, df$run_no[i], df$block[i]), &quot;design.mat&quot;), file.path(df$in_dir[i], sprintf(&quot;VIRTEM_P%s&quot;,df$sub_id[i]), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, df$run_no[i], df$block[i]), &quot;design.con&quot;), file.path(stat_pth, &quot;res4d.nii.gz&quot;) ), sprintf(&quot;output = %s&quot;, file.path(htc_dir, sprintf(&quot;%s_%s_block%d.out&quot;, df$sub_id[i], df$run_name[i], df$block[i]))), sprintf(&quot;error = %s&quot;, file.path(htc_dir, sprintf(&quot;%s_%s_block%d.err&quot;, df$sub_id[i], df$run_name[i], df$block[i]))), sprintf(&quot;log = %s&quot;, file.path(htc_dir, sprintf(&quot;%s_%s_block%d.log&quot;, df$sub_id[i], df$run_name[i], df$block[i]))), sprintf(&quot;queue&quot;) ),con) c=c+1 } } close(con) # submit to cluster batch_id &lt;- system(paste(&quot;condor_submit&quot;, fn), intern = TRUE) # find batch ID and wait batch_id &lt;- regmatches(batch_id[2], gregexpr(&quot;[[:digit:]]+&quot;, batch_id[2]))[[1]][2] sprintf(&quot;submitted jobs (ID = %s) for 1st level GLM for parametric modulation by relative time. Time to wait...&quot;, batch_id) pause_until_batch_done(batch_id = batch_id, wait_interval = 300) 11.4 Group-level Move first-level images to MNI space The first level output of each block is in each participant’s common functional space. For group-level stats we need to register the outputs to MNI (1mm) space. # name of transformation matrix file to move from functional space to MNI 1mm func2standard &lt;- file.path(dirs$level1glm_dir, sprintf(&quot;VIRTEM_P%s&quot;, rep(subjects, each = n_blocks)), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, 2, rep(1:n_blocks, n_subs)), &quot;reg&quot;, &quot;example_func2standard.mat&quot;) # parametric modulation result in whole-brain functional space in_nii_fn &lt;- file.path(df$in_dir[i], sprintf(&quot;VIRTEM_P%s&quot;, rep(subjects, each = n_blocks)), sprintf(&quot;RSA_%02d_Block%02d.feat&quot;, 2, rep(1:n_blocks, n_subs)), &quot;stats&quot;, &quot;zstat1.nii.gz&quot;) # output folder &amp; file name in MNI space invisible(lapply(file.path(dirs$level1glm_dir, &quot;mni&quot;, subjects), function(x) if(!dir.exists(x)) dir.create(x, recursive=TRUE))) out_nii_fn &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, rep(subjects, each = n_blocks), sprintf(&quot;%s_post_block%02d_rel_time_parametric_modulation.nii.gz&quot;, rep(subjects, each = n_blocks), rep(1:n_blocks, n_subs))) # apply FSL flirt to move from whole-brain functional space to 1mm MNI space invisible(mapply(flirt_apply, infile = in_nii_fn, reffile = mni_fname(&quot;1&quot;), initmat = func2standard, outfile = out_nii_fn, verbose = FALSE, retimg = FALSE)) Merge first-level images For each subject, we merge the results files from each block. for (i_sub in subjects){ # input files in_files &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, i_sub, sprintf(&quot;%s_post_block%02d_rel_time_parametric_modulation.nii.gz&quot;, i_sub, 1:n_blocks)) # output file name fn_4d &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, i_sub, sprintf(&quot;%s_post_rel_time_parametric_modulation_merged.nii.gz&quot;, i_sub)) # concatenate the images fslmerge(infiles = in_files, direction = &quot;t&quot;, outfile = fn_4d, retimg = FALSE, verbose = FALSE) } Smooth first-level images We smooth the first level images for each participant. # what smoothing to apply FWHM = 3 # in mm sigma = FWHM/2.3548 # fslmaths needs sigma not FWHM of kernel # field of view mask fov_mask &lt;- file.path(dirs$mask_dir, &quot;fov&quot;, &quot;fov_mask_mni.nii.gz&quot;) # open the task list fn &lt;- file.path(htc_dir, &quot;smooth_rel_time_tasklist.txt&quot;) con &lt;- file(fn) open(con, &quot;w&quot;) # file name of 4D output file fn_4d &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, subjects, sprintf(&quot;%s_post_rel_time_parametric_modulation_merged.nii.gz&quot;, subjects)) # name of smoothed output file fn_4d_smooth &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, subjects, sprintf(&quot;%s_post_rel_time_parametric_modulation_merged_smoothed.nii.gz&quot;, subjects)) # write smoothing command to file writeLines(sprintf(&quot;fslmaths %s -s %f -mas %s %s&quot;, fn_4d, sigma, fov_mask, fn_4d_smooth), con) close(con) # submit to cluster cmd &lt;- sprintf(&quot;fsl_sub -T 30 -t %s -l %s -M bellmund@cbs.mpg.de -N smooth_rel_time&quot;, fn, htc_dir) batch_id &lt;- system(cmd, intern = TRUE) pause_until_batch_done(batch_id = batch_id, wait_interval = 20) Average first-level images for (i_sub in subjects){ fslmaths(file = file.path(dirs$level1glm_dir, &quot;mni&quot;, i_sub, sprintf(&quot;%s_post_rel_time_parametric_modulation_merged_smoothed.nii.gz&quot;, i_sub)), outfile = file.path(dirs$level1glm_dir, &quot;mni&quot;, i_sub, sprintf(&quot;%s_post_rel_time_parametric_modulation_merged_smoothed_mean.nii.gz&quot;, i_sub)), retimg = FALSE, opts = &quot;-Tmean&quot;, verbose = FALSE ) } Now that we have the mean parametric modulation effect for each participant, we concatenate the images across participants. # file names of files to merge in_files &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, subjects, sprintf(&quot;%s_post_rel_time_parametric_modulation_merged_smoothed_mean.nii.gz&quot;, subjects)) # file name of 4D output file fn_4d &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, &quot;4D_rel_time_parametric_modulation.nii.gz&quot;) # concatenate the images fslmerge(infiles = in_files, direction = &quot;t&quot;, outfile = fn_4d, retimg = FALSE, verbose = FALSE) aHPC ROI analysis # load 4D file with data of each participant in_fn &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, &quot;4D_rel_time_parametric_modulation.nii.gz&quot;) in_nii &lt;- readNIfTI2(in_fn) # load ROI mask and binarize it and linearize aHPC_fn &lt;- aHPC_fn &lt;- here(&quot;data&quot;, &quot;mri&quot;, &quot;rois&quot;, &quot;aHPC_lr&quot;, sprintf(&quot;%s_group_prob_mni1mm.nii.gz&quot;, &quot;aHPC_lr&quot;)) aHPC_nii &lt;- readNIfTI2(aHPC_fn) aHPC_nii[aHPC_nii &gt;0.99] &lt;- 1 aHPC_nii[aHPC_nii !=1 ] &lt;- 0 aHPC_lin &lt;- as.logical(c(aHPC_nii)) aHPC_rel_time &lt;- tibble(pmod = rep(0,n_subs)) for (i_sub in 1:n_subs){ # get this subject&#39;s data from aHPC curr_nii &lt;- in_nii[,,,i_sub] curr_dat &lt;- c(curr_nii) curr_dat &lt;- curr_dat[aHPC_lin] # store mean across aHPC voxels aHPC_rel_time$pmod[i_sub] &lt;- mean(curr_dat) } # across subject statistics stats &lt;- paired_t_perm_jb (aHPC_rel_time$pmod, n_perm = n_perm) Run FSL Randomise Now we are ready to run FSL Randomise. # open the task list fn &lt;- file.path(htc_dir, &quot;glm_rel_time_randomise_tasklist.txt&quot;) con &lt;- file(fn) open(con, &quot;w&quot;) # masks to use gm_mask_fn &lt;- file.path(dirs$mask_dir, &quot;gray_matter&quot;, &quot;gray_matter_mask.nii.gz&quot;) svc_mask_fn &lt;- file.path(dirs$mask_dir, &quot;svc&quot;, &quot;svc_mask.nii.gz&quot;) # 4D input image to run randomise on in_fn &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, &quot;4D_rel_time_parametric_modulation.nii.gz&quot;) # output file name for FOV out_fn &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, &quot;randomise_rel_time_parametric_modulation_fov&quot;) # define randomise command for full FOV and write to file writeLines(sprintf(&quot;randomise -i %s -o %s -1 -T --uncorrp -m %s -n 5000&quot;, in_fn, out_fn, gm_mask_fn),con) # output file name for SVC out_fn &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, &quot;randomise_rel_time_parametric_modulation_svc&quot;) # define randomise command for small-volume correction and and write to file writeLines(sprintf(&quot;randomise -i %s -o %s -1 -T --uncorrp -m %s -n 5000&quot;, in_fn, out_fn, svc_mask_fn),con) close(con) # submit to cluster cmd &lt;- sprintf(&quot;fsl_sub -T 300 -t %s -l %s -M bellmund@cbs.mpg.de -N randomise_rel_time&quot;, fn, htc_dir) batch_id &lt;- system(cmd, intern = TRUE) pause_until_batch_done(batch_id = batch_id, wait_interval = 300) Results in aHPC and alEC Let’s first have a look at the results in our a priori regions of interest, the aHPC and the alEC. For this we have run randomise using the small volume correction mask that includes these areas. We create a figure of the results at a liberal threshold. # 1 mm MNI template as background image mni_fn &lt;- file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;, &quot;MNI152_T1_1mm_brain.nii.gz&quot;) mni_nii &lt;- readNIfTI2(mni_fn) # load FOV mask and binarize it fov_fn &lt;- file.path(dirs$data4analysis, &quot;mni1mm_masks&quot;, &quot;fov_mask_mni.nii.gz&quot;) fov_nii &lt;- readNIfTI2(fov_fn) fov_nii[fov_nii &gt;0] &lt;- 1 fov_nii[fov_nii !=1 ] &lt;- 0 # make a mask for the brain area outside our FOV out_fov &lt;- (fov_nii == 0) &amp; (mni_nii&gt;0) mni_nii[out_fov] &lt;- scales::rescale(mni_nii[out_fov], from=range(mni_nii[out_fov]), to=c(6000, 8207)) mni_nii[mni_nii == 0] &lt;- NA # load image with uncorrected p-values in SVC uncorrpsvc_fn &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, &quot;randomise_rel_time_parametric_modulation_svc_tfce_p_tstat1.nii.gz&quot;) uncorrpsvc_nii &lt;- readNIfTI2(uncorrpsvc_fn) # load image with corrected p-values in SVC corrpsvc_fn &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, &quot;randomise_rel_time_parametric_modulation_svc_tfce_corrp_tstat1.nii.gz&quot;) corrpsvc_nii &lt;- readNIfTI2(corrpsvc_fn) # t-values to plot (SVC) tvals_fn &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, &quot;randomise_rel_time_parametric_modulation_svc_tstat1.nii.gz&quot;) t_nii &lt;- readNIfTI2(tvals_fn) # find coordinates of voxel with minimum p-value (image is 1-p) and get a data frame for labeling the plots coords &lt;- which(t_nii == max(t_nii), arr.ind = TRUE) #coords &lt;- mni2vox(coords) # print results sprintf(&quot;peak voxel of relative time parametric modulation in SVC at MNI x=%d, y=%d, z=%d, t=%.2f, puncorr=%.3f, pcorr=%.3f&quot;, vox2mni(coords)[1], vox2mni(coords)[2], vox2mni(coords)[3], t_nii[coords], 1-uncorrpsvc_nii[coords], 1-corrpsvc_nii[coords]) ## [1] &quot;peak voxel of relative time parametric modulation in SVC at MNI x=22, y=-13, z=-16, t=3.93, puncorr=0.001, pcorr=0.173&quot; label_df &lt;- data.frame(col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;), vox2mni(coords)), label = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;), vox2mni(coords))) # get ggplot object for template as background ggTemplate&lt;-getggTemplate(col_template=rev(RColorBrewer::brewer.pal(8, &#39;Greys&#39;)),brains=mni_nii, all_brain_and_time_inds_one=TRUE, mar=c(1,2,3), mar_ind=coords, row_ind = c(1,1,1), col_ind=sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), center_coords=TRUE) # threshold t-values for plotting thresh &lt;- 0.05 t_nii[uncorrpsvc_nii &lt; 1-thresh] &lt;- NA # get data frame for the t-values t_df &lt;- getBrainFrame(brains=t_nii, mar=c(1,2,3), col_ind = sprintf(&quot;%s=%d&quot;,c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),vox2mni(coords)), mar_ind=coords, mask=NULL, center_coords=TRUE) fig_rel_time &lt;- ggTemplate + geom_tile(data=t_df, aes(x=row,y=col,fill=value))+ facet_wrap(~col_ind, nrow=1, ncol=3, strip.position = &quot;top&quot;, scales = &quot;free&quot;) + scico::scale_fill_scico(palette = &quot;lajolla&quot;, begin = 0.3, end=0.9, direction = -1, name = element_text(&quot;t&quot;), limits = round(range(t_df$value), digits=2), breaks = round(range(t_df$value), digits=2)) + #ggtitle(label = &quot;\\ndifferent\\nsequence&quot;) + theme_cowplot(line_size = NA) + theme(strip.background = element_blank(), strip.text.x = element_blank(), plot.title = element_text(hjust = 0.5), text = element_text(size=10, family=font2use), axis.text = element_text(size=8, family=font2use), legend.text=element_text(size=8, family=font2use), legend.title = element_text(size=8, family=font2use), legend.key.width = unit(0.015, &quot;npc&quot;), legend.key.height = unit(0.03, &quot;npc&quot;), legend.position = &quot;right&quot;, legend.justification = c(0.5, 0), aspect.ratio = 1)+ guides(fill = guide_colorbar( direction = &quot;vertical&quot;, title.position = &quot;top&quot;, title.hjust = 0, label.position = &quot;right&quot;))+ xlab(element_blank()) + ylab(element_blank()) + coord_cartesian(clip = &quot;off&quot;) + geom_text(data = label_df, x=0, y=-Inf, aes(label = label), size = 8/.pt, family=font2use, vjust=1) + theme_no_ticks() fn &lt;- here(&quot;figures&quot;, &quot;letter_glm_rel_time_aHPC_svc&quot;) ggsave(paste0(fn, &quot;.pdf&quot;), plot=fig_rel_time, units = &quot;cm&quot;, width = 17.4, height = 6, dpi = &quot;retina&quot;, device = cairo_pdf) ggsave(paste0(fn, &quot;.png&quot;), plot=fig_rel_time, units = &quot;cm&quot;, width = 17.4, height = 6, dpi = &quot;retina&quot;, device = &quot;png&quot;) print(fig_rel_time) FOV results We check how many voxels survive at an uncorrected p-threshold of 0.001 in our FOV. No clusters are visible at an uncorrected threshold of p&lt;0.001. # load image with uncorrected p-values in FOV uncorrpfov_fn &lt;- file.path(dirs$level1glm_dir, &quot;mni&quot;, &quot;randomise_rel_time_parametric_modulation_fov_tfce_p_tstat1.nii.gz&quot;) uncorrpfov_nii &lt;- readNIfTI2(uncorrpfov_fn) thresh &lt;- 0.001 if(sum(c(uncorrpfov_nii)&gt;(1-thresh)) == 0){ # print to screen cat(&quot;no significant voxels in parametric modulation analysis at p&lt;&quot;, sprintf(&quot;%.3f (uncorrected)&quot;,thresh)) } ## no significant voxels in parametric modulation analysis at p&lt; 0.001 (uncorrected) "],
["credit.html", "12 Credit 12.1 List of packages 12.2 References 12.3 Session Info", " 12 Credit 12.1 List of packages Here is a list of packages used in the analysis and references to them. It is created using the grateful package. Unfortunately, not all packages provide all information, so there are a few warning messages. The code below generates a markdown file with references that we read in and print below. invisible(grateful::cite_packages(all.pkg = FALSE, include.rmd=FALSE, style = &quot;cell&quot;, out.format = &quot;md&quot;)) ## Warning in FUN(X[[i]], ...): no date field in DESCRIPTION file of package &#39;extrafont&#39; ## Warning in FUN(X[[i]], ...): no date field in DESCRIPTION file of package &#39;ggBrain&#39; ## ## ## processing file: refs.Rmd ## output file: refs.knit.md ## ## Output created: citations.md used_pkgs &lt;-readLines(here(&quot;virtem_code&quot;, &quot;citations.md&quot;)) ref_line &lt;- which(used_pkgs==&quot;References&quot;) used_pkgs[ref_line] &lt;- &quot;### References {-}&quot; used_pkgs[ref_line+1] &lt;- &quot;&quot; paste(used_pkgs, collapse = &#39;\\n&#39;) %&gt;% cat() base (R Core Team, 2019) extrafont (Chang, 2020) ggBrain (Fisher, 2020) grateful (Rodríguez-Sánchez and Hutchins, 2020) effsize (Torchiano, 2020) patchwork (Pedersen, 2020a) scico (Pedersen and Crameri, 2020) ggcorrplot (Kassambara, 2019) umap (Konopka, 2020) smacof (de Leeuw and Mair, 2009) e1071 (Meyer et al., 2019) colorspace (Stauffer et al., 2009) plotrix (J, 2006) permuco (Frossard and Renaud, 2019) afex (Singmann et al., 2020) sjmisc (Lüdecke, 2018a) gghalves (Tiedemann, 2020) huxtable (Hugh-Jones, 2020) tidystats (Sleegers, 2020) flextable (Gohel, 2020a) matrixStats (Bengtsson, 2020) emmeans (Lenth, 2020) ggeffects (Lüdecke, 2018b) broom.mixed (Bolker and Robinson, 2020) lmerTest (Kuznetsova et al., 2017) car (Fox and Weisberg, 2019) carData (Fox et al., 2020) texreg (Leifeld, 2013) officer (Gohel, 2020b) lme4 (Bates et al., 2015) Matrix (Bates and Maechler, 2019) DescTools (al., 2020) corrplot (corrplot?) tictoc (Izrailev, 2014) freesurfer (freesurfer?) fslr (fslr?) neurobase (Muschelli, 2020) oro.nifti (Whitcher et al., 2011) Hmisc (Harrell Jr et al., 2020) Formula (Zeileis and Croissant, 2010) survival (survival?) lattice (Sarkar, 2008) smooth (Svetunkov, 2020a) greybox (Svetunkov, 2020b) lavaan (Rosseel, 2012) cowplot (Wilke, 2019) ggforce (Pedersen, 2020b) statip (Poncet and The R Core Team, 2019) broom (Robinson et al., 2021) here (Müller, 2017) wesanderson (Ram and Wickham, 2018) bookdown (Xie, 2016) assertr (Fischetti, 2020) mgsub (Ewing, 2019) tinytex (Xie, 2019) combinat (Chasalow, 2012) devtools (Wickham et al., 2020a) usethis (Wickham and Bryan, 2020) forcats (Wickham, 2020) stringr (Wickham, 2019) dplyr (Wickham et al., 2020b) purrr (Henry and Wickham, 2020) readr (Wickham and Hester, 2021) tidyr (Wickham and Henry, 2020) tibble (Müller and Wickham, 2021) ggplot2 (Wickham, 2016) tidyverse (Wickham et al., 2019) R.matlab (Bengtsson, 2018) 12.2 References al., A.S. et mult. (2020). DescTools: Tools for descriptive statistics. Bates, D., and Maechler, M. (2019). Matrix: Sparse and dense matrix classes and methods. Bates, D., Mächler, M., Bolker, B., and Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67, 1–48. Bengtsson, H. (2018). R.matlab: Read and write MAT files and call MATLAB from within r. Bengtsson, H. (2020). matrixStats: Functions that apply to rows and columns of matrices (and to vectors). Bolker, B., and Robinson, D. (2020). Broom.mixed: Tidying methods for mixed models. Chang, W. (2020). Extrafont: Tools for using fonts. Chasalow, S. (2012). Combinat: Combinatorics utilities. de Leeuw, J., and Mair, P. (2009). Multidimensional scaling using majorization: SMACOF in R. Journal of Statistical Software 31, 1–30. Ewing, M. (2019). Mgsub: Safe, multiple, simultaneous string substitution. Fischetti, T. (2020). Assertr: Assertive programming for r analysis pipelines. Fisher, A. (2020). ggBrain: Ggplot brain images. Fox, J., and Weisberg, S. (2019). An R companion to applied regression (Thousand Oaks CA: Sage). Fox, J., Weisberg, S., and Price, B. (2020). carData: Companion to applied regression data sets. Frossard, J., and Renaud, O. (2019). Permuco: Permutation tests for regression, (repeated measures) ANOVA/ANCOVA and comparison of signals. Gohel, D. (2020a). Flextable: Functions for tabular reporting. Gohel, D. (2020b). Officer: Manipulation of microsoft word and PowerPoint documents. Harrell Jr, F.E., Charles Dupont, with contributions from, and others., many (2020). Hmisc: Harrell miscellaneous. Henry, L., and Wickham, H. (2020). Purrr: Functional programming tools. Hugh-Jones, D. (2020). Huxtable: Easily create and style tables for LaTeX, HTML and other formats. Izrailev, S. (2014). Tictoc: Functions for timing r scripts, as well as implementations of stack and list structures. J, L. (2006). Plotrix: A package in the red light district of r. R-News 6, 8–12. Kassambara, A. (2019). Ggcorrplot: Visualization of a correlation matrix using ’ggplot2’. Konopka, T. (2020). Umap: Uniform manifold approximation and projection. Kuznetsova, A., Brockhoff, P.B., and Christensen, R.H.B. (2017). lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software 82, 1–26. Leifeld, P. (2013). texreg: Conversion of statistical model output in R to LaTeX and HTML tables. Journal of Statistical Software 55, 1–24. Lenth, R. (2020). Emmeans: Estimated marginal means, aka least-squares means. Lüdecke, D. (2018a). Sjmisc: Data and variable transformation functions. Journal of Open Source Software 3, 754. Lüdecke, D. (2018b). Ggeffects: Tidy data frames of marginal effects from regression models. Journal of Open Source Software 3, 772. Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., and Leisch, F. (2019). e1071: Misc functions of the department of statistics, probability theory group (formerly: E1071), TU wien. Müller, K. (2017). Here: A simpler way to find your files. Müller, K., and Wickham, H. (2021). Tibble: Simple data frames. Muschelli, J. (2020). Neurobase: ’Neuroconductor’ base package with helper functions for ’nifti’ objects. Pedersen, T.L. (2020a). Patchwork: The composer of plots. Pedersen, T.L. (2020b). Ggforce: Accelerating ’ggplot2’. Pedersen, T.L., and Crameri, F. (2020). Scico: Colour palettes based on the scientific colour-maps. Poncet, P., and The R Core Team (2019). Statip: Statistical functions for probability distributions and regression. R Core Team (2019). R: A language and environment for statistical computing (Vienna, Austria: R Foundation for Statistical Computing). Ram, K., and Wickham, H. (2018). Wesanderson: A wes anderson palette generator. Robinson, D., Hayes, A., and Couch, S. (2021). Broom: Convert statistical objects into tidy tibbles. Rodríguez-Sánchez, F., and Hutchins, S.D. (2020). Grateful: Facilitate citation of r packages. Rosseel, Y. (2012). lavaan: An R package for structural equation modeling. Journal of Statistical Software 48, 1–36. Sarkar, D. (2008). Lattice: Multivariate data visualization with r (New York: Springer). Singmann, H., Bolker, B., Westfall, J., Aust, F., and Ben-Shachar, M.S. (2020). Afex: Analysis of factorial experiments. Sleegers, W. (2020). Tidystats: Save output of statistical tests. Stauffer, R., Mayr, G.J., Dabernig, M., and Zeileis, A. (2009). Somewhere over the rainbow: How to make effective use of colors in meteorological visualizations. Bulletin of the American Meteorological Society 96, 203–216. Svetunkov, I. (2020a). Smooth: Forecasting using state space models. Svetunkov, I. (2020b). Greybox: Toolbox for model building and forecasting. Tiedemann, F. (2020). Gghalves: Compose half-half plots using your favourite geoms. Torchiano, M. (2020). Effsize: Efficient effect size computation. Whitcher, B., Schmid, V.J., and Thornton, A. (2011). Working with the DICOM and NIfTI data standards in R. Journal of Statistical Software 44, 1–28. Wickham, H. (2016). ggplot2: Elegant graphics for data analysis (Springer-Verlag New York). Wickham, H. (2019). Stringr: Simple, consistent wrappers for common string operations. Wickham, H. (2020). Forcats: Tools for working with categorical variables (factors). Wickham, H., and Bryan, J. (2020). Usethis: Automate package and project setup. Wickham, H., and Henry, L. (2020). Tidyr: Tidy messy data. Wickham, H., and Hester, J. (2021). Readr: Read rectangular text data. Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., et al. (2019). Welcome to the tidyverse. Journal of Open Source Software 4, 1686. Wickham, H., Hester, J., and Chang, W. (2020a). Devtools: Tools to make developing r packages easier. Wickham, H., François, R., Henry, L., and Müller, K. (2020b). Dplyr: A grammar of data manipulation. Wilke, C.O. (2019). Cowplot: Streamlined plot theme and plot annotations for ’ggplot2’. Xie, Y. (2016). Bookdown: Authoring books and technical documents with R markdown (Boca Raton, Florida: Chapman; Hall/CRC). Xie, Y. (2019). TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX live. TUGboat 30–32. Zeileis, A., and Croissant, Y. (2010). Extended model formulas in R: Multiple parts and multiple responses. Journal of Statistical Software 34, 1–13. 12.3 Session Info Lastly, we run session info, using the version from the devtools package. devtools::session_info() ## ─ Session info ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## setting value ## version R version 3.6.1 (2019-07-05) ## os Ubuntu 18.04.6 LTS ## system x86_64, linux-gnu ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Europe/Berlin ## date 2022-02-03 ## ## ─ Packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## ! package * version date lib source ## abind 1.4-5 2016-07-21 [1] CRAN (R 3.6.1) ## acepack 1.4.1 2016-10-29 [1] CRAN (R 3.6.1) ## afex * 0.27-2 2020-03-28 [1] CRAN (R 3.6.1) ## askpass 1.1 2019-01-13 [1] CRAN (R 3.6.1) ## assertr * 2.7 2020-02-05 [1] CRAN (R 3.6.1) ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.1) ## backports 1.1.8 2020-06-17 [1] CRAN (R 3.6.1) ## base64enc 0.1-3 2015-07-28 [1] CRAN (R 3.6.1) ## bit 4.0.4 2020-08-04 [2] CRAN (R 3.6.1) ## bit64 4.0.5 2020-08-30 [2] CRAN (R 3.6.1) ## bitops 1.0-6 2013-08-17 [1] CRAN (R 3.6.1) ## blob 1.2.1 2020-01-20 [1] CRAN (R 3.6.1) ## bookdown * 0.19 2020-05-15 [1] CRAN (R 3.6.1) ## boot 1.3-22 2019-04-02 [3] CRAN (R 3.6.1) ## broom * 0.7.9 2021-07-27 [1] CRAN (R 3.6.1) ## broom.mixed * 0.2.6 2020-05-17 [1] CRAN (R 3.6.1) ## callr 3.4.3 2020-03-28 [1] CRAN (R 3.6.1) ## candisc 0.8-3 2020-04-22 [1] CRAN (R 3.6.1) ## car * 3.0-8 2020-05-21 [1] CRAN (R 3.6.1) ## carData * 3.0-4 2020-05-22 [1] CRAN (R 3.6.1) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 3.6.1) ## checkmate 2.0.0 2020-02-06 [1] CRAN (R 3.6.1) ## checkpoint 0.4.10 2020-10-29 [1] CRAN (R 3.6.1) ## class 7.3-15 2019-01-01 [3] CRAN (R 3.6.1) ## cli 2.2.0 2020-11-20 [1] CRAN (R 3.6.1) ## clue 0.3-57 2019-02-25 [1] CRAN (R 3.6.1) ## cluster 2.1.0 2019-06-19 [3] CRAN (R 3.6.1) ## coda 0.19-3 2019-07-05 [1] CRAN (R 3.6.1) ## codetools 0.2-16 2018-12-24 [3] CRAN (R 3.6.1) ## P colorspace * 2.0-2 2021-06-24 [2] CRAN (R 3.6.1) ## combinat * 0.0-8 2012-10-29 [1] CRAN (R 3.6.1) ## commonmark 1.7 2018-12-01 [1] CRAN (R 3.6.1) ## corrplot * 0.84 2017-10-16 [1] CRAN (R 3.6.1) ## cowplot * 1.0.0 2019-07-11 [1] CRAN (R 3.6.1) ## VP crayon 1.4.1 2017-09-16 [2] CRAN (R 3.6.1) ## curl 4.3 2019-12-02 [1] CRAN (R 3.6.1) ## data.table 1.13.4 2020-12-08 [1] CRAN (R 3.6.1) ## DBI 1.1.0 2019-12-15 [1] CRAN (R 3.6.1) ## dbplyr 1.4.4 2020-05-27 [1] CRAN (R 3.6.1) ## desc 1.2.0 2018-05-01 [1] CRAN (R 3.6.1) ## DescTools * 0.99.36 2020-05-23 [1] CRAN (R 3.6.1) ## devtools * 2.3.0 2020-04-10 [1] CRAN (R 3.6.1) ## VP digest 0.6.27 2021-09-23 [2] CRAN (R 3.6.1) ## doParallel 1.0.15 2019-08-02 [1] CRAN (R 3.6.1) ## dplyr * 1.0.0 2020-05-29 [1] CRAN (R 3.6.1) ## e1071 * 1.7-3 2019-11-26 [1] CRAN (R 3.6.1) ## effsize * 0.8.1 2020-10-05 [1] CRAN (R 3.6.1) ## ellipse 0.4.2 2020-05-27 [1] CRAN (R 3.6.1) ## VP ellipsis 0.3.2 2020-05-15 [2] CRAN (R 3.6.1) ## emmeans * 1.4.7 2020-05-25 [1] CRAN (R 3.6.1) ## estimability 1.3 2018-02-11 [1] CRAN (R 3.6.1) ## P evaluate 0.14 2019-05-28 [2] CRAN (R 3.6.1) ## expm 0.999-4 2019-03-21 [1] CRAN (R 3.6.1) ## extrafont * 0.17.0.9000 2020-12-22 [1] Github (wch/extrafont@cfa3269) ## extrafontdb 1.0 2012-06-11 [1] CRAN (R 3.6.1) ## VP fansi 0.5.0 2020-01-08 [2] CRAN (R 3.6.1) ## farver 2.0.3 2020-01-16 [1] CRAN (R 3.6.1) ## flextable * 0.6.1 2020-12-08 [1] CRAN (R 3.6.1) ## forcats * 0.5.0 2020-03-01 [1] CRAN (R 3.6.1) ## foreach 1.5.0 2020-03-30 [1] CRAN (R 3.6.1) ## forecast 8.12 2020-03-31 [1] CRAN (R 3.6.1) ## foreign 0.8-71 2018-07-20 [3] CRAN (R 3.6.1) ## Formula * 1.2-3 2018-05-03 [1] CRAN (R 3.6.1) ## fracdiff 1.5-1 2020-01-24 [1] CRAN (R 3.6.1) ## freesurfer * 1.6.7 2020-03-30 [1] CRAN (R 3.6.1) ## fs 1.4.1 2020-04-04 [1] CRAN (R 3.6.1) ## fslr * 2.24.1 2019-08-05 [1] CRAN (R 3.6.1) ## gdata 2.18.0 2017-06-06 [1] CRAN (R 3.6.1) ## gdtools 0.2.2 2020-04-03 [1] CRAN (R 3.6.1) ## generics 0.0.2 2018-11-29 [1] CRAN (R 3.6.1) ## ggBrain * 0.1.2 2020-11-18 [1] Github (neuroconductor/ggBrain@cae74e5) ## ggcorrplot * 0.1.3 2019-05-19 [1] CRAN (R 3.6.1) ## ggeffects * 0.15.0 2020-06-16 [1] CRAN (R 3.6.1) ## ggforce * 0.3.2 2020-06-23 [1] CRAN (R 3.6.1) ## gghalves * 0.1.0 2020-03-28 [1] CRAN (R 3.6.1) ## ggplot2 * 3.3.2 2020-06-19 [1] CRAN (R 3.6.1) ## glue 1.4.1 2020-05-13 [1] CRAN (R 3.6.1) ## grateful * 0.0.2 2020-12-18 [1] Github (Pakillo/grateful@552b4bd) ## greybox * 0.6.0 2020-05-20 [1] CRAN (R 3.6.1) ## gridExtra 2.3 2017-09-09 [1] CRAN (R 3.6.1) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 3.6.1) ## gtools 3.8.2 2020-03-31 [1] CRAN (R 3.6.1) ## haven 2.3.1 2020-06-01 [1] CRAN (R 3.6.1) ## heplots 1.3-5 2018-04-03 [1] CRAN (R 3.6.1) ## here * 0.1 2017-05-28 [1] CRAN (R 3.6.1) ## highr 0.8 2019-03-20 [1] CRAN (R 3.6.1) ## Hmisc * 4.4-0 2020-03-23 [1] CRAN (R 3.6.1) ## VP hms 1.1.0 2020-01-08 [2] CRAN (R 3.6.1) ## htmlTable 2.0.0 2020-06-21 [1] CRAN (R 3.6.1) ## VP htmltools 0.5.1.1 2020-06-16 [2] CRAN (R 3.6.1) ## htmlwidgets 1.5.1 2019-10-08 [1] CRAN (R 3.6.1) ## httr 1.4.1 2019-08-05 [1] CRAN (R 3.6.1) ## huxtable * 5.0.0 2020-06-15 [1] CRAN (R 3.6.1) ## insight 0.8.5 2020-06-08 [1] CRAN (R 3.6.1) ## iterators 1.0.12 2019-07-26 [1] CRAN (R 3.6.1) ## jpeg 0.1-8.1 2019-10-24 [1] CRAN (R 3.6.1) ## jsonlite 1.6.1 2020-02-02 [1] CRAN (R 3.6.1) ## VP knitr 1.33 2020-02-06 [2] CRAN (R 3.6.1) ## labeling 0.3 2014-08-23 [1] CRAN (R 3.6.1) ## lamW 1.3.2 2020-05-25 [1] CRAN (R 3.6.1) ## lattice * 0.20-38 2018-11-04 [3] CRAN (R 3.6.1) ## latticeExtra 0.6-29 2019-12-19 [1] CRAN (R 3.6.1) ## lavaan * 0.6-6 2020-05-13 [1] CRAN (R 3.6.1) ## VP lifecycle 1.0.0 2020-03-06 [2] CRAN (R 3.6.1) ## lme4 * 1.1-23 2020-04-07 [1] CRAN (R 3.6.1) ## lmerTest * 3.1-2 2020-04-08 [1] CRAN (R 3.6.1) ## lmtest 0.9-37 2019-04-30 [1] CRAN (R 3.6.1) ## lubridate 1.7.9 2020-06-08 [1] CRAN (R 3.6.1) ## VP magrittr 2.0.1 2014-11-22 [2] CRAN (R 3.6.1) ## MASS 7.3-51.4 2019-03-31 [3] CRAN (R 3.6.1) ## Matrix * 1.2-17 2019-03-22 [3] CRAN (R 3.6.1) ## matrixStats * 0.56.0 2020-03-13 [1] CRAN (R 3.6.1) ## memoise 1.1.0 2017-04-21 [1] CRAN (R 3.6.1) ## mgcv 1.8-28 2019-03-21 [3] CRAN (R 3.6.1) ## mgsub * 1.7.1 2019-03-13 [1] CRAN (R 3.6.1) ## mice 3.11.0 2020-08-05 [1] CRAN (R 3.6.1) ## minqa 1.2.4 2014-10-09 [1] CRAN (R 3.6.1) ## mnormt 1.5-6 2020-02-03 [1] CRAN (R 3.6.1) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 3.6.1) ## P munsell 0.5.0 2018-06-12 [2] CRAN (R 3.6.1) ## mvtnorm 1.1-1 2020-06-09 [1] CRAN (R 3.6.1) ## neurobase * 1.29.0 2020-01-14 [1] CRAN (R 3.6.1) ## nlme 3.1-140 2019-05-12 [3] CRAN (R 3.6.1) ## nloptr 1.2.2.1 2020-03-11 [1] CRAN (R 3.6.1) ## nnet 7.3-12 2016-02-02 [3] CRAN (R 3.6.1) ## nnls 1.4 2012-03-19 [1] CRAN (R 3.6.1) ## numDeriv 2016.8-1.1 2019-06-06 [1] CRAN (R 3.6.1) ## officer * 0.3.15 2020-11-01 [1] CRAN (R 3.6.1) ## openssl 1.4.1 2019-07-18 [1] CRAN (R 3.6.1) ## openxlsx 4.1.5 2020-05-06 [1] CRAN (R 3.6.1) ## oro.nifti * 0.10.3 2020-06-08 [1] CRAN (R 3.6.1) ## patchwork * 1.1.1 2020-12-17 [1] CRAN (R 3.6.1) ## pbivnorm 0.6.0 2015-01-23 [1] CRAN (R 3.6.1) ## permuco * 1.1.0 2019-12-19 [1] CRAN (R 3.6.1) ## permute 0.9-5 2019-03-12 [1] CRAN (R 3.6.1) ## VP pillar 1.6.2 2020-05-05 [2] CRAN (R 3.6.1) ## pkgbuild 1.0.8 2020-05-07 [1] CRAN (R 3.6.1) ## P pkgconfig 2.0.3 2019-09-22 [2] CRAN (R 3.6.1) ## pkgload 1.1.0 2020-05-29 [1] CRAN (R 3.6.1) ## plotrix * 3.7-8 2020-04-16 [1] CRAN (R 3.6.1) ## P plyr 1.8.6 2020-03-03 [2] CRAN (R 3.6.1) ## png 0.1-7 2013-12-03 [1] CRAN (R 3.6.1) ## polyclip 1.10-0 2019-03-14 [1] CRAN (R 3.6.1) ## polynom 1.4-0 2019-03-22 [1] CRAN (R 3.6.1) ## pracma 2.2.9 2019-12-15 [1] CRAN (R 3.6.1) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 3.6.1) ## processx 3.4.2 2020-02-09 [1] CRAN (R 3.6.1) ## ps 1.3.3 2020-05-08 [1] CRAN (R 3.6.1) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 3.6.1) ## quadprog 1.5-8 2019-11-20 [1] CRAN (R 3.6.1) ## quantmod 0.4.17 2020-03-31 [1] CRAN (R 3.6.1) ## R.matlab * 3.6.2 2018-09-27 [1] CRAN (R 3.6.1) ## R.methodsS3 1.8.0 2020-02-14 [1] CRAN (R 3.6.1) ## R.oo 1.23.0 2019-11-03 [1] CRAN (R 3.6.1) ## R.utils 2.9.2 2019-12-08 [1] CRAN (R 3.6.1) ## VP R6 2.5.0 2019-11-12 [2] CRAN (R 3.6.1) ## P RColorBrewer 1.1-2 2014-12-07 [2] CRAN (R 3.6.1) ## VP Rcpp 1.0.7 2020-04-09 [2] CRAN (R 3.6.1) ## RcppParallel 5.0.1 2020-05-06 [1] CRAN (R 3.6.1) ## VP readr * 2.0.0 2018-12-21 [2] CRAN (R 3.6.1) ## readxl 1.3.1 2019-03-13 [1] CRAN (R 3.6.1) ## VP remotes 2.4.0 2020-02-15 [2] CRAN (R 3.6.1) ## reprex 0.3.0 2019-05-16 [1] CRAN (R 3.6.1) ## P reshape2 1.4.4 2020-04-09 [2] CRAN (R 3.6.1) ## reticulate 1.20 2021-05-03 [1] CRAN (R 3.6.1) ## rio 0.5.16 2018-11-26 [1] CRAN (R 3.6.1) ## VP rlang 0.4.11 2020-05-02 [2] CRAN (R 3.6.1) ## VP rmarkdown 2.9 2021-09-14 [2] CRAN (R 3.6.1) ## RNifti 1.1.0 2020-01-31 [1] CRAN (R 3.6.1) ## rpart 4.1-15 2019-04-12 [3] CRAN (R 3.6.1) ## rprojroot 1.3-2 2018-01-03 [1] CRAN (R 3.6.1) ## RSpectra 0.16-0 2019-12-01 [1] CRAN (R 3.6.1) ## rstudioapi 0.11 2020-02-07 [1] CRAN (R 3.6.1) ## Rttf2pt1 1.3.8 2020-12-22 [1] Github (wch/Rttf2pt1@000efae) ## rvest 0.3.5 2019-11-08 [1] CRAN (R 3.6.1) ## P scales 1.1.1 2020-05-11 [2] CRAN (R 3.6.1) ## P scico * 1.2.0 2020-06-08 [2] CRAN (R 3.6.1) ## sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.6.1) ## sjlabelled 1.1.5 2020-05-25 [1] CRAN (R 3.6.1) ## sjmisc * 2.8.5 2020-05-28 [1] CRAN (R 3.6.1) ## smacof * 2.1-1 2020-06-29 [1] CRAN (R 3.6.1) ## smooth * 2.6.0 2020-06-17 [1] CRAN (R 3.6.1) ## statip * 0.2.3 2019-11-17 [1] CRAN (R 3.6.1) ## statmod 1.4.34 2020-02-17 [1] CRAN (R 3.6.1) ## VP stringi 1.7.3 2020-02-17 [2] CRAN (R 3.6.1) ## P stringr * 1.4.0 2019-02-10 [2] CRAN (R 3.6.1) ## survival * 3.2-3 2020-06-13 [1] CRAN (R 3.6.1) ## systemfonts 0.3.2 2020-09-29 [1] CRAN (R 3.6.1) ## testthat 2.3.2 2020-03-02 [1] CRAN (R 3.6.1) ## texreg * 1.37.5 2020-06-18 [1] CRAN (R 3.6.1) ## VP tibble * 3.1.3 2020-04-20 [2] CRAN (R 3.6.1) ## tictoc * 1.0 2014-06-17 [1] CRAN (R 3.6.1) ## tidyr * 1.1.0 2020-05-20 [1] CRAN (R 3.6.1) ## tidyselect 1.1.0 2020-05-11 [1] CRAN (R 3.6.1) ## tidystats * 0.4.1 2020-06-15 [1] CRAN (R 3.6.1) ## tidyverse * 1.3.0 2019-11-21 [1] CRAN (R 3.6.1) ## timeDate 3043.102 2018-02-21 [1] CRAN (R 3.6.1) ## tinytex * 0.24 2020-06-20 [1] CRAN (R 3.6.1) ## TMB 1.7.16 2020-01-15 [1] CRAN (R 3.6.1) ## tseries 0.10-47 2019-06-05 [1] CRAN (R 3.6.1) ## TTR 0.23-6 2019-12-15 [1] CRAN (R 3.6.1) ## tweenr 1.0.1 2018-12-14 [1] CRAN (R 3.6.1) ## tzdb 0.1.2 2021-07-20 [2] CRAN (R 3.6.1) ## umap * 0.2.7.0 2020-11-04 [1] CRAN (R 3.6.1) ## urca 1.3-0 2016-09-06 [1] CRAN (R 3.6.1) ## usethis * 1.6.1 2020-04-29 [1] CRAN (R 3.6.1) ## VP utf8 1.2.2 2018-05-24 [2] CRAN (R 3.6.1) ## uuid 0.1-4 2020-02-26 [1] CRAN (R 3.6.1) ## VP vctrs 0.3.8 2020-06-05 [2] CRAN (R 3.6.1) ## vroom 1.5.3 2021-07-14 [2] CRAN (R 3.6.1) ## weights 1.0.1 2020-02-12 [1] CRAN (R 3.6.1) ## wesanderson * 0.3.6 2018-04-20 [1] CRAN (R 3.6.1) ## withr 2.2.0 2020-04-20 [1] CRAN (R 3.6.1) ## wordcloud 2.6 2018-08-24 [1] CRAN (R 3.6.1) ## VP xfun 0.24 2020-06-21 [2] CRAN (R 3.6.1) ## xml2 1.3.2 2020-04-23 [1] CRAN (R 3.6.1) ## xtable 1.8-4 2019-04-21 [1] CRAN (R 3.6.1) ## xts 0.12-0 2020-01-19 [1] CRAN (R 3.6.1) ## P yaml 2.2.1 2020-02-01 [2] CRAN (R 3.6.1) ## zip 2.1.1 2020-08-27 [1] CRAN (R 3.6.1) ## zoo 1.8-8 2020-05-02 [1] CRAN (R 3.6.1) ## ## [1] /data/pt_02261/virtem/virtem_code/R3.6.1/library/Linux ## [2] /data/hu_bellmund/R/x86_64-pc-linux-gnu-library/3.6 ## [3] /afs/cbs.mpg.de/software/r/3.6.1/ubuntu-bionic-amd64/lib/R/library ## ## V ── Loaded and on-disk version mismatch. ## P ── Loaded and on-disk path mismatch. "]
]
